{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-07T02:25:45.939892Z",
     "start_time": "2017-09-07T02:25:45.465392Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-07T02:25:45.968964Z",
     "start_time": "2017-09-07T02:25:45.941180Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parser = {\n",
    "    'data_path': '../data/wttsf/',\n",
    "    'train_file': 'train_1.csv',\n",
    "    'key_file': 'key_1.csv',\n",
    "    'intermediate_path': '../intermediate/ensemble/',\n",
    "    'future': 73,\n",
    "    'batch_size': 64,\n",
    "    'hidden_size': 256,\n",
    "    'read_from_file': True,\n",
    "    'model2_name': 'model_20170905_epoch6_loss26.4678.pth',\n",
    "    'model3_name': 'model_20170906_epoch6_loss26.4670.pth',\n",
    "    'forecast_start': '2017-01-01',\n",
    "    'forecast_end': '2017-03-01',\n",
    "    'cuda': True,\n",
    "}\n",
    "args = argparse.Namespace(**parser)\n",
    "\n",
    "args.cuda = args.cuda and torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-07T02:25:46.055670Z",
     "start_time": "2017-09-07T02:25:45.969987Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DenseLSTMForecast(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(DenseLSTMForecast, self).__init__()\n",
    "        self.lstm1 = nn.LSTMCell(15, hidden_size)\n",
    "        self.lstm2 = nn.LSTMCell(hidden_size+15, hidden_size)\n",
    "        self.lstm3 = nn.LSTMCell(2*hidden_size+15, hidden_size)\n",
    "        self.linear = nn.Linear(3*hidden_size+15, 1)\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    def forward(self, x, feature, future=1):\n",
    "        o = []\n",
    "        tt = torch.cuda if args.cuda else torch\n",
    "        h1_t = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())\n",
    "        c1_t = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())\n",
    "        h2_t = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())\n",
    "        c2_t = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())\n",
    "        h3_t = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())\n",
    "        c3_t = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())\n",
    "        \n",
    "        for x_t in x.chunk(x.size(1), dim=1):\n",
    "            x_t = x_t.squeeze(dim=1)\n",
    "            xd_t = torch.cat([x_t, feature], dim=1)\n",
    "            h1_t, c1_t = self.lstm1(xd_t, (h1_t, c1_t))\n",
    "            h1d_t = torch.cat([xd_t, h1_t], dim=1)\n",
    "            h2_t, c2_t = self.lstm2(h1d_t, (h2_t, c2_t))\n",
    "            h2d_t = torch.cat([xd_t, h1_t, h2_t], dim=1)\n",
    "            h3_t, c3_t = self.lstm3(h2d_t, (h3_t, c3_t))\n",
    "            h3d_t = torch.cat([xd_t, h1_t, h2_t, h3_t], dim=1)\n",
    "            o_t = self.linear(h3d_t)\n",
    "            o.append(o_t)\n",
    "            \n",
    "        for i in range(future-1):\n",
    "            od_t = torch.cat([o_t, feature], dim=1)\n",
    "            h1_t, c1_t = self.lstm1(od_t, (h1_t, c1_t))\n",
    "            h1d_t = torch.cat([od_t, h1_t], dim=1)\n",
    "            h2_t, c2_t = self.lstm2(h1d_t, (h2_t, c2_t))\n",
    "            h2d_t = torch.cat([od_t, h1_t, h2_t], dim=1)\n",
    "            h3_t, c3_t = self.lstm3(h2d_t, (h3_t, c3_t))\n",
    "            h3d_t = torch.cat([od_t, h1_t, h2_t, h3_t], dim=1)\n",
    "            o_t = self.linear(h3d_t)\n",
    "            o.append(o_t)\n",
    "\n",
    "        return torch.stack(o, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-07T02:25:46.116812Z",
     "start_time": "2017-09-07T02:25:46.056790Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    scaled_data_file = os.path.join(args.intermediate_path,\n",
    "                                    'scaled_data.pkl')\n",
    "    scaler_file = os.path.join(args.intermediate_path, 'scaler.pkl')\n",
    "    features_file = os.path.join(args.intermediate_path, 'features.pkl')\n",
    "    \n",
    "    if not args.read_from_file:\n",
    "        data_df = pd.read_csv(os.path.join(args.data_path, args.train_file),\n",
    "                              index_col='Page')\n",
    "        data_df = data_df.fillna(method='ffill', axis=1).fillna(\n",
    "            method='bfill', axis=1)\n",
    "        data_df[\"agent\"] = data_df.index.str.rsplit('_').str.get(-1)\n",
    "        data_df[\"access\"] = data_df.index.str.rsplit('_').str.get(-2)\n",
    "        data_df[\"project\"] = data_df.index.str.rsplit('_').str.get(-3)\n",
    "        features = pd.get_dummies(data_df[[\"agent\", \"access\", \"project\"]],\n",
    "            columns=[\"agent\", \"access\", \"project\"]).values.astype('float32')\n",
    "        raw_data = np.nan_to_num(\n",
    "            data_df.iloc[:,:-3].values.astype('float32'))\n",
    "        data = np.log1p(raw_data)\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(np.swapaxes(data, 0, 1))\n",
    "#        scaler.fit(np.swapaxes(data[:, :-args.future], 0, 1))\n",
    "        scaled_data = scaler.transform(np.swapaxes(data, 0, 1))\n",
    "        scaled_data = np.swapaxes(scaled_data, 0, 1)\n",
    "        \n",
    "        with open(scaled_data_file, 'wb') as f:\n",
    "            pickle.dump(scaled_data, f)\n",
    "        with open(scaler_file, 'wb') as f:\n",
    "            pickle.dump(scaler, f)\n",
    "        with open(features_file, 'wb') as f:\n",
    "            pickle.dump(features, f)\n",
    "    else:\n",
    "        with open(scaled_data_file, 'rb') as f:\n",
    "            scaled_data = pickle.load(f)\n",
    "        with open(scaler_file, 'rb') as f:\n",
    "            scaler = pickle.load(f)\n",
    "        with open(features_file, 'rb') as f:\n",
    "            features = pickle.load(f)\n",
    "    return scaled_data, scaler, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-07T02:25:46.175910Z",
     "start_time": "2017-09-07T02:25:46.118012Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forecast(scaled_data, scaler, features, model):\n",
    "    input_tensor = torch.from_numpy(scaled_data).unsqueeze(2)\n",
    "    target_tensor = torch.zeros(input_tensor.size(0))\n",
    "    features_tensor = torch.from_numpy(features)\n",
    "    dataset = torch.utils.data.TensorDataset(input_tensor, target_tensor)\n",
    "    data_loader = DataLoader(dataset, 256)\n",
    "    \n",
    "    output_list = []\n",
    "    for i, (inputt, _) in enumerate(data_loader):\n",
    "        feature = features_tensor[i*args.batch_size:(i*args.batch_size\n",
    "                                                     +inputt.size(0))]\n",
    "        if args.cuda:\n",
    "            inputt = inputt.cuda()\n",
    "            feature = feature.cuda()\n",
    "        inputt = Variable(inputt)\n",
    "        feature = Variable(feature)\n",
    "        output = model(inputt, feature, args.future)\n",
    "        output_list.append(output.data.squeeze(2).cpu().numpy()\n",
    "                           [:, -args.future:])\n",
    "        \n",
    "    output_all = np.concatenate(output_list, axis=0)\n",
    "    prediction = np.swapaxes(scaler.inverse_transform(\n",
    "            np.swapaxes(output_all, 0, 1)), 0, 1)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-07T02:25:46.327998Z",
     "start_time": "2017-09-07T02:25:46.177089Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaled_data, scaler, features = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-07T02:25:47.488737Z",
     "start_time": "2017-09-07T02:25:46.329281Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = DenseLSTMForecast(args.hidden_size)\n",
    "if args.cuda:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-07T02:25:47.492743Z",
     "start_time": "2017-09-07T02:25:47.490015Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prediction1_file = os.path.join(args.intermediate_path, 'prediction1.pkl')\n",
    "# if 1 == 0:\n",
    "#     model1_file = os.path.join(args.intermediate_path, args.model1_name)\n",
    "#     model.load_state_dict(torch.load(model1_file))\n",
    "\n",
    "#     prediction1 = forecast(scaled_data, scaler, features, model)\n",
    "\n",
    "#     with open(prediction1_file, 'wb') as f:\n",
    "#         pickle.dump(prediction1, f)\n",
    "# else:\n",
    "#     with open(prediction1_file, 'rb') as f:\n",
    "#         prediction1 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-07T02:25:47.627290Z",
     "start_time": "2017-09-07T02:25:47.493671Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction2_file = os.path.join(args.intermediate_path, 'prediction2.pkl')\n",
    "if 1 == 0:\n",
    "    model2_file = os.path.join(args.intermediate_path, args.model2_name)\n",
    "    model.load_state_dict(torch.load(model2_file))\n",
    "\n",
    "    prediction2 = forecast(scaled_data, scaler, features, model)\n",
    "\n",
    "    with open(prediction2_file, 'wb') as f:\n",
    "        pickle.dump(prediction2, f)\n",
    "else:\n",
    "    with open(prediction2_file, 'rb') as f:\n",
    "        prediction2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-07T02:25:47.666413Z",
     "start_time": "2017-09-07T02:25:47.629499Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction3_file = os.path.join(args.intermediate_path, 'prediction3.pkl')\n",
    "if 1 == 0:\n",
    "    model3_file = os.path.join(args.intermediate_path, args.model3_name)\n",
    "    model.load_state_dict(torch.load(model3_file))\n",
    "\n",
    "    prediction3 = forecast(scaled_data, scaler, features, model)\n",
    "\n",
    "    with open(prediction3_file, 'wb') as f:\n",
    "        pickle.dump(prediction3, f)\n",
    "else:\n",
    "    with open(prediction3_file, 'rb') as f:\n",
    "        prediction3 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-07T02:25:47.838186Z",
     "start_time": "2017-09-07T02:25:47.667771Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction = (prediction2 + prediction3) / 2\n",
    "prediction = np.clip(np.exp(prediction) - 1, 0, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-07T02:32:59.237331Z",
     "start_time": "2017-09-07T02:32:39.394765Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_df = pd.read_csv(os.path.join(args.data_path,\n",
    "                                   args.train_file)).fillna(0)\n",
    "data_df.index = data_df['Page']\n",
    "key_df = pd.read_csv(os.path.join(args.data_path, args.key_file))\n",
    "key_df['Date'] = key_df['Page'].apply(lambda a: a[-10:]).astype(\n",
    "    'datetime64[ns]')\n",
    "key_df['Page'] = key_df['Page'].apply(lambda a: a[:-11])\n",
    "key_df['Weekend']= key_df['Date'].dt.dayofweek >= 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-07T02:37:34.179786Z",
     "start_time": "2017-09-07T02:32:59.238646Z"
    }
   },
   "outputs": [],
   "source": [
    "windows = [6, 12, 18, 30, 48, 78, 126, 203, 329]\n",
    "for i in windows:\n",
    "    val = 'MW'+str(i)\n",
    "    tmp = pd.melt(data_df[list(data_df.columns[-i:])+['Page']], \n",
    "                  id_vars='Page', var_name='Date', value_name=val)\n",
    "    tmp['Date'] = tmp['Date'].astype('datetime64[ns]')\n",
    "    tmp['Weekend']= tmp['Date'].dt.dayofweek >= 5\n",
    "    tmp = tmp.groupby(['Page','Weekend']).median().reset_index()\n",
    "    key_df = key_df.merge(tmp, how='left')\n",
    "    \n",
    "key_df['Visits_MW']= key_df.iloc[:, 4:].median(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-07T02:37:37.143037Z",
     "start_time": "2017-09-07T02:37:34.180775Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "future_start = (pd.Timestamp(args.forecast_start)\n",
    "                - pd.Timestamp(data_df.columns[-1])).days - 1\n",
    "future_end = (pd.Timestamp(args.forecast_end)\n",
    "              - pd.Timestamp(data_df.columns[-1])).days\n",
    "future_period = future_end - future_start\n",
    "\n",
    "visits = np.zeros(key_df.shape[0])\n",
    "for i in range(0, len(visits), future_period):\n",
    "    page = key_df['Page'][i]\n",
    "    page_index = data_df.index.get_loc(page)\n",
    "    visits[i:(i+future_period)] = prediction[page_index,\n",
    "                                             future_start:future_end]\n",
    "\n",
    "key_df['Visits_RNN'] = visits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-07T02:39:26.715385Z",
     "start_time": "2017-09-07T02:39:17.769738Z"
    }
   },
   "outputs": [],
   "source": [
    "key_df['Visits'] = (key_df['Visits_MW'] + key_df['Visits_RNN']) / 2\n",
    "key_df['Visits'] = key_df['Visits'].round().astype(int)\n",
    "submission_file = os.path.join(args.intermediate_path, 'submission5.csv')\n",
    "key_df[['Id', 'Visits']].to_csv(submission_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-07T02:40:19.936175Z",
     "start_time": "2017-09-07T02:40:19.918733Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Page</th>\n",
       "      <th>Id</th>\n",
       "      <th>Date</th>\n",
       "      <th>Weekend</th>\n",
       "      <th>MW6</th>\n",
       "      <th>MW12</th>\n",
       "      <th>MW18</th>\n",
       "      <th>MW30</th>\n",
       "      <th>MW48</th>\n",
       "      <th>MW78</th>\n",
       "      <th>MW126</th>\n",
       "      <th>MW203</th>\n",
       "      <th>MW329</th>\n",
       "      <th>Visits_MW</th>\n",
       "      <th>Visits_RNN</th>\n",
       "      <th>Visits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>!vote_en.wikipedia.org_all-access_all-agents</td>\n",
       "      <td>bf4edcf969af</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.436407</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>!vote_en.wikipedia.org_all-access_all-agents</td>\n",
       "      <td>929ed2bf52b9</td>\n",
       "      <td>2017-01-02</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.558730</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>!vote_en.wikipedia.org_all-access_all-agents</td>\n",
       "      <td>ff29d0f51d5c</td>\n",
       "      <td>2017-01-03</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.467755</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>!vote_en.wikipedia.org_all-access_all-agents</td>\n",
       "      <td>e98873359be6</td>\n",
       "      <td>2017-01-04</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.470027</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>!vote_en.wikipedia.org_all-access_all-agents</td>\n",
       "      <td>fa012434263a</td>\n",
       "      <td>2017-01-05</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.525051</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Page            Id       Date  \\\n",
       "0  !vote_en.wikipedia.org_all-access_all-agents  bf4edcf969af 2017-01-01   \n",
       "1  !vote_en.wikipedia.org_all-access_all-agents  929ed2bf52b9 2017-01-02   \n",
       "2  !vote_en.wikipedia.org_all-access_all-agents  ff29d0f51d5c 2017-01-03   \n",
       "3  !vote_en.wikipedia.org_all-access_all-agents  e98873359be6 2017-01-04   \n",
       "4  !vote_en.wikipedia.org_all-access_all-agents  fa012434263a 2017-01-05   \n",
       "\n",
       "   Weekend  MW6  MW12  MW18  MW30  MW48  MW78  MW126  MW203  MW329  Visits_MW  \\\n",
       "0     True  1.0   3.0   3.0   2.0   3.0   3.0    3.0    3.0    3.0        3.0   \n",
       "1    False  1.0   2.0   2.0   2.0   2.0   3.0    3.0    3.0    3.0        2.0   \n",
       "2    False  1.0   2.0   2.0   2.0   2.0   3.0    3.0    3.0    3.0        2.0   \n",
       "3    False  1.0   2.0   2.0   2.0   2.0   3.0    3.0    3.0    3.0        2.0   \n",
       "4    False  1.0   2.0   2.0   2.0   2.0   3.0    3.0    3.0    3.0        2.0   \n",
       "\n",
       "   Visits_RNN  Visits  \n",
       "0    2.436407       3  \n",
       "1    2.558730       2  \n",
       "2    2.467755       2  \n",
       "3    2.470027       2  \n",
       "4    2.525051       2  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pydata)",
   "language": "python",
   "name": "pydata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
