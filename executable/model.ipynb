{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-07T11:46:52.882567Z",
     "start_time": "2017-09-07T11:46:52.873597Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import numpy as np; np.seterr(invalid='ignore')\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-07T11:46:53.599037Z",
     "start_time": "2017-09-07T11:46:53.579705Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parser = {\n",
    "    'data_path': '../data/wttsf/',\n",
    "    'train_file': 'train_2.csv',\n",
    "    'key_file': 'key_2.csv',\n",
    "    'intermediate_path': '../intermediate/',\n",
    "    'n_epoch': 6,\n",
    "    'future': 73,\n",
    "    'batch_size': 128,\n",
    "    'hidden_size': 256,\n",
    "    'log_every': 10,\n",
    "    'read_from_file': False,\n",
    "    'train': True,\n",
    "    'model_name': '',\n",
    "    'forecast': False,\n",
    "    'forecast_start': '2017-01-01',\n",
    "    'forecast_end': '2017-03-01',\n",
    "    'cuda': True,\n",
    "    'seed': 20170907,\n",
    "}\n",
    "args = argparse.Namespace(**parser)\n",
    "\n",
    "args.cuda = args.cuda and torch.cuda.is_available()\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "args.intermediate_path = os.path.join(args.intermediate_path, str(args.seed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-07T11:46:54.559777Z",
     "start_time": "2017-09-07T11:46:54.475103Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DenseLSTMForecast(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(DenseLSTMForecast, self).__init__()\n",
    "        self.lstm1 = nn.LSTMCell(15, hidden_size)\n",
    "        self.lstm2 = nn.LSTMCell(hidden_size+15, hidden_size)\n",
    "        self.lstm3 = nn.LSTMCell(2*hidden_size+15, hidden_size)\n",
    "        self.linear = nn.Linear(3*hidden_size+15, 1)\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    def forward(self, x, feature, future=1):\n",
    "        o = []\n",
    "        tt = torch.cuda if args.cuda else torch\n",
    "        h1_t = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())\n",
    "        c1_t = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())\n",
    "        h2_t = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())\n",
    "        c2_t = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())\n",
    "        h3_t = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())\n",
    "        c3_t = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())\n",
    "        \n",
    "        for x_t in x.chunk(x.size(1), dim=1):\n",
    "            x_t = x_t.squeeze(dim=1)\n",
    "            xd_t = torch.cat([x_t, feature], dim=1)\n",
    "            h1_t, c1_t = self.lstm1(xd_t, (h1_t, c1_t))\n",
    "            h1d_t = torch.cat([xd_t, h1_t], dim=1)\n",
    "            h2_t, c2_t = self.lstm2(h1d_t, (h2_t, c2_t))\n",
    "            h2d_t = torch.cat([xd_t, h1_t, h2_t], dim=1)\n",
    "            h3_t, c3_t = self.lstm3(h2d_t, (h3_t, c3_t))\n",
    "            h3d_t = torch.cat([xd_t, h1_t, h2_t, h3_t], dim=1)\n",
    "            o_t = self.linear(h3d_t)\n",
    "            o.append(o_t)\n",
    "            \n",
    "        for i in range(future-1):\n",
    "            od_t = torch.cat([o_t, feature], dim=1)\n",
    "            h1_t, c1_t = self.lstm1(od_t, (h1_t, c1_t))\n",
    "            h1d_t = torch.cat([od_t, h1_t], dim=1)\n",
    "            h2_t, c2_t = self.lstm2(h1d_t, (h2_t, c2_t))\n",
    "            h2d_t = torch.cat([od_t, h1_t, h2_t], dim=1)\n",
    "            h3_t, c3_t = self.lstm3(h2d_t, (h3_t, c3_t))\n",
    "            h3d_t = torch.cat([od_t, h1_t, h2_t, h3_t], dim=1)\n",
    "            o_t = self.linear(h3d_t)\n",
    "            o.append(o_t)\n",
    "\n",
    "        return torch.stack(o, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-07T11:46:55.915248Z",
     "start_time": "2017-09-07T11:46:55.908425Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def smape(y_pred, y_true):\n",
    "    y_pred = np.around(np.clip(np.exp(y_pred)-1, 0, None))\n",
    "    y_true = np.around(np.exp(y_true) - 1)\n",
    "    raw_smape = np.abs(y_true - y_pred) / (np.abs(y_true) + np.abs(y_pred))\n",
    "    kaggle_smape = np.nan_to_num(raw_smape)\n",
    "    return np.mean(kaggle_smape) * 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-07T11:46:56.552108Z",
     "start_time": "2017-09-07T11:46:56.499183Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    scaled_data_file = os.path.join(args.intermediate_path,\n",
    "                                    'scaled_data.pkl')\n",
    "    scaler_file = os.path.join(args.intermediate_path, 'scaler.pkl')\n",
    "    features_file = os.path.join(args.intermediate_path, 'features.pkl')\n",
    "    \n",
    "    if not args.read_from_file:\n",
    "        data_df = pd.read_csv(os.path.join(args.data_path, args.train_file),\n",
    "                              index_col='Page')\n",
    "        data_df = data_df.fillna(method='ffill', axis=1).fillna(\n",
    "            method='bfill', axis=1)\n",
    "        data_df[\"agent\"] = data_df.index.str.rsplit('_').str.get(-1)\n",
    "        data_df[\"access\"] = data_df.index.str.rsplit('_').str.get(-2)\n",
    "        data_df[\"project\"] = data_df.index.str.rsplit('_').str.get(-3)\n",
    "        features = pd.get_dummies(data_df[[\"agent\", \"access\", \"project\"]],\n",
    "            columns=[\"agent\", \"access\", \"project\"]).values.astype('float32')\n",
    "        raw_data = np.nan_to_num(\n",
    "            data_df.iloc[:,:-3].values.astype('float32'))\n",
    "        data = np.log1p(raw_data)\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(np.swapaxes(data, 0, 1))\n",
    "#        scaler.fit(np.swapaxes(data[:, :-args.future], 0, 1))\n",
    "        scaled_data = scaler.transform(np.swapaxes(data, 0, 1))\n",
    "        scaled_data = np.swapaxes(scaled_data, 0, 1)\n",
    "        \n",
    "        with open(scaled_data_file, 'wb') as f:\n",
    "            pickle.dump(scaled_data, f)\n",
    "        with open(scaler_file, 'wb') as f:\n",
    "            pickle.dump(scaler, f)\n",
    "        with open(features_file, 'wb') as f:\n",
    "            pickle.dump(features, f)\n",
    "    else:\n",
    "        with open(scaled_data_file, 'rb') as f:\n",
    "            scaled_data = pickle.load(f)\n",
    "        with open(scaler_file, 'rb') as f:\n",
    "            scaler = pickle.load(f)\n",
    "        with open(features_file, 'rb') as f:\n",
    "            features = pickle.load(f)\n",
    "    return scaled_data, scaler, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-07T11:48:25.782500Z",
     "start_time": "2017-09-07T11:48:25.778824Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(145063, 793)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-07T11:47:04.842991Z",
     "start_time": "2017-09-07T11:47:04.757826Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(scaled_data, scaler, features, model, criterion, optimizer):\n",
    "    p = np.random.permutation(scaled_data.shape[0])\n",
    "    inverse_p = np.argsort(p)\n",
    "    \n",
    "    input_tensor = torch.from_numpy(scaled_data[p, :-1]).unsqueeze(2)\n",
    "    target_tensor = torch.from_numpy(scaled_data[p, 1:]).unsqueeze(2)\n",
    "    features_tensor = torch.from_numpy(features[p, :])\n",
    "    dataset = TensorDataset(input_tensor, target_tensor)\n",
    "    data_loader = DataLoader(dataset, args.batch_size)\n",
    "    \n",
    "    train_loss = 0\n",
    "    val_output_list = []\n",
    "    init_time = time.time()\n",
    "    for i, (inputt, target) in enumerate(data_loader):\n",
    "        feature = features_tensor[i*args.batch_size:(i*args.batch_size\n",
    "                                                     +inputt.size(0))]\n",
    "        if args.cuda:\n",
    "            inputt = inputt.cuda()\n",
    "            target = target.cuda()\n",
    "            feature = feature.cuda()\n",
    "        inputt = Variable(inputt)\n",
    "        target = Variable(target)\n",
    "        feature = Variable(feature)\n",
    "        \n",
    "        output = model(inputt, feature)\n",
    "        pos = np.random.randint(args.future,\n",
    "                                554-args.future+1)\n",
    "        loss = criterion(output[:, pos:pos+args.future],\n",
    "                         target[:, pos:pos+args.future])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.data[0] * inputt.size(0)\n",
    "        val_output_list.append(output[:, -args.future:]\n",
    "                               .data.squeeze(2).cpu().numpy())\n",
    "        \n",
    "        if i % args.log_every == 0:\n",
    "            print(\"   % Time: {:4.0f}s | Batch: {:4} | \"\n",
    "                  \"Train loss: {:.4f}\".format(\n",
    "                      time.time()-init_time, i+1, loss.data[0]))\n",
    "        \n",
    "    val_output_all = np.concatenate(val_output_list, axis=0)[inverse_p]\n",
    "    prediction = np.swapaxes(scaler.inverse_transform(\n",
    "            np.swapaxes(val_output_all, 0, 1)), 0, 1)\n",
    "    var_target = np.swapaxes(scaler.inverse_transform(\n",
    "            np.swapaxes(scaled_data[:, -args.future:], 0, 1)), 0, 1)\n",
    "    \n",
    "    train_loss /= scaled_data.shape[0]\n",
    "    val_loss = smape(prediction, var_target)\n",
    "    print(\"=\"*10)\n",
    "    print(\"   % Epoch: {} | Time: {:4.0f}s | \"\n",
    "          \"Train loss: {:.4f} | Val loss: {:.4f}\"\n",
    "          .format(epoch, time.time()-init_time, train_loss , val_loss))\n",
    "    print(\"=\"*10)\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-07T11:47:05.367816Z",
     "start_time": "2017-09-07T11:47:05.343812Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forecast(scaled_data, scaler, features, model):\n",
    "    input_tensor = torch.from_numpy(scaled_data).unsqueeze(2)\n",
    "    target_tensor = torch.zeros(input_tensor.size(0))\n",
    "    features_tensor = torch.from_numpy(features)\n",
    "    dataset = torch.utils.data.TensorDataset(input_tensor, target_tensor)\n",
    "    data_loader = DataLoader(dataset, 256)\n",
    "    \n",
    "    output_list = []\n",
    "    for i, (inputt, _) in enumerate(data_loader):\n",
    "        feature = features_tensor[i*args.batch_size:(i*args.batch_size\n",
    "                                                     +inputt.size(0))]\n",
    "        if args.cuda:\n",
    "            inputt = inputt.cuda()\n",
    "            feature = feature.cuda()\n",
    "        inputt = Variable(inputt)\n",
    "        feature = Variable(feature)\n",
    "        output = model(inputt, feature, args.future)\n",
    "        output_list.append(output.data.squeeze(2).cpu().numpy()\n",
    "                           [:, -args.future:])\n",
    "        \n",
    "    output_all = np.concatenate(output_list, axis=0)\n",
    "    prediction = np.swapaxes(scaler.inverse_transform(\n",
    "            np.swapaxes(output_all, 0, 1)), 0, 1)\n",
    "    prediction = np.around(np.clip(np.exp(prediction) - 1, 0, None))\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-07T11:47:06.234648Z",
     "start_time": "2017-09-07T11:47:06.229255Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_model(model, epoch, loss):\n",
    "    model_file = os.path.join(args.intermediate_path,\n",
    "                              \"model_{}_epoch{}_loss{:.4f}.pth\"\n",
    "                              .format(args.seed, epoch, loss))\n",
    "    torch.save(model.state_dict(), os.path.join(model_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-07T11:47:21.009041Z",
     "start_time": "2017-09-07T11:47:07.584851Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaled_data, scaler, features = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-06T07:01:14.992807Z",
     "start_time": "2017-09-06T07:01:13.899382Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = DenseLSTMForecast(args.hidden_size)\n",
    "if args.cuda:\n",
    "    model.cuda()\n",
    "criterion = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-06T07:01:14.996531Z",
     "start_time": "2017-09-06T07:01:14.993944Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "scheduler = MultiStepLR(optimizer, milestones=[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-06T09:57:02.242146Z",
     "start_time": "2017-09-06T07:01:14.998115Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> EPOCH 1 with lr 0.001\n",
      "   % Time:    1s | Batch:    1 | Train loss: 0.7267\n",
      "   % Time:    9s | Batch:   11 | Train loss: 0.5070\n",
      "   % Time:   17s | Batch:   21 | Train loss: 0.4808\n",
      "   % Time:   26s | Batch:   31 | Train loss: 0.4506\n",
      "   % Time:   34s | Batch:   41 | Train loss: 0.4256\n",
      "   % Time:   42s | Batch:   51 | Train loss: 0.4560\n",
      "   % Time:   50s | Batch:   61 | Train loss: 0.4716\n",
      "   % Time:   58s | Batch:   71 | Train loss: 0.4322\n",
      "   % Time:   66s | Batch:   81 | Train loss: 0.4597\n",
      "   % Time:   75s | Batch:   91 | Train loss: 0.4703\n",
      "   % Time:   83s | Batch:  101 | Train loss: 0.5155\n",
      "   % Time:   91s | Batch:  111 | Train loss: 0.4576\n",
      "   % Time:   99s | Batch:  121 | Train loss: 0.4472\n",
      "   % Time:  107s | Batch:  131 | Train loss: 0.4761\n",
      "   % Time:  115s | Batch:  141 | Train loss: 0.4563\n",
      "   % Time:  124s | Batch:  151 | Train loss: 0.4509\n",
      "   % Time:  132s | Batch:  161 | Train loss: 0.4419\n",
      "   % Time:  140s | Batch:  171 | Train loss: 0.4044\n",
      "   % Time:  148s | Batch:  181 | Train loss: 0.3641\n",
      "   % Time:  156s | Batch:  191 | Train loss: 0.4446\n",
      "   % Time:  164s | Batch:  201 | Train loss: 0.4115\n",
      "   % Time:  172s | Batch:  211 | Train loss: 0.3950\n",
      "   % Time:  180s | Batch:  221 | Train loss: 0.4089\n",
      "   % Time:  188s | Batch:  231 | Train loss: 0.4110\n",
      "   % Time:  196s | Batch:  241 | Train loss: 0.4611\n",
      "   % Time:  204s | Batch:  251 | Train loss: 0.3723\n",
      "   % Time:  212s | Batch:  261 | Train loss: 0.3873\n",
      "   % Time:  220s | Batch:  271 | Train loss: 0.4182\n",
      "   % Time:  228s | Batch:  281 | Train loss: 0.4083\n",
      "   % Time:  237s | Batch:  291 | Train loss: 0.4253\n",
      "   % Time:  245s | Batch:  301 | Train loss: 0.4282\n",
      "   % Time:  253s | Batch:  311 | Train loss: 0.3751\n",
      "   % Time:  261s | Batch:  321 | Train loss: 0.4016\n",
      "   % Time:  269s | Batch:  331 | Train loss: 0.3816\n",
      "   % Time:  277s | Batch:  341 | Train loss: 0.4077\n",
      "   % Time:  285s | Batch:  351 | Train loss: 0.4200\n",
      "   % Time:  293s | Batch:  361 | Train loss: 0.3749\n",
      "   % Time:  301s | Batch:  371 | Train loss: 0.4099\n",
      "   % Time:  309s | Batch:  381 | Train loss: 0.4054\n",
      "   % Time:  317s | Batch:  391 | Train loss: 0.3973\n",
      "   % Time:  325s | Batch:  401 | Train loss: 0.3919\n",
      "   % Time:  333s | Batch:  411 | Train loss: 0.4069\n",
      "   % Time:  342s | Batch:  421 | Train loss: 0.4066\n",
      "   % Time:  350s | Batch:  431 | Train loss: 0.3713\n",
      "   % Time:  358s | Batch:  441 | Train loss: 0.3792\n",
      "   % Time:  366s | Batch:  451 | Train loss: 0.3792\n",
      "   % Time:  374s | Batch:  461 | Train loss: 0.4142\n",
      "   % Time:  381s | Batch:  471 | Train loss: 0.4211\n",
      "   % Time:  389s | Batch:  481 | Train loss: 0.3823\n",
      "   % Time:  397s | Batch:  491 | Train loss: 0.4125\n",
      "   % Time:  405s | Batch:  501 | Train loss: 0.3653\n",
      "   % Time:  413s | Batch:  511 | Train loss: 0.4139\n",
      "   % Time:  421s | Batch:  521 | Train loss: 0.3893\n",
      "   % Time:  429s | Batch:  531 | Train loss: 0.3804\n",
      "   % Time:  437s | Batch:  541 | Train loss: 0.4295\n",
      "   % Time:  445s | Batch:  551 | Train loss: 0.3896\n",
      "   % Time:  453s | Batch:  561 | Train loss: 0.4198\n",
      "   % Time:  462s | Batch:  571 | Train loss: 0.3456\n",
      "   % Time:  470s | Batch:  581 | Train loss: 0.3725\n",
      "   % Time:  478s | Batch:  591 | Train loss: 0.4147\n",
      "   % Time:  486s | Batch:  601 | Train loss: 0.3734\n",
      "   % Time:  494s | Batch:  611 | Train loss: 0.3866\n",
      "   % Time:  502s | Batch:  621 | Train loss: 0.3757\n",
      "   % Time:  510s | Batch:  631 | Train loss: 0.4117\n",
      "   % Time:  518s | Batch:  641 | Train loss: 0.3804\n",
      "   % Time:  525s | Batch:  651 | Train loss: 0.4012\n",
      "   % Time:  533s | Batch:  661 | Train loss: 0.4590\n",
      "   % Time:  541s | Batch:  671 | Train loss: 0.4377\n",
      "   % Time:  549s | Batch:  681 | Train loss: 0.4115\n",
      "   % Time:  557s | Batch:  691 | Train loss: 0.4040\n",
      "   % Time:  565s | Batch:  701 | Train loss: 0.3817\n",
      "   % Time:  573s | Batch:  711 | Train loss: 0.3499\n",
      "   % Time:  581s | Batch:  721 | Train loss: 0.4232\n",
      "   % Time:  589s | Batch:  731 | Train loss: 0.4006\n",
      "   % Time:  596s | Batch:  741 | Train loss: 0.3982\n",
      "   % Time:  604s | Batch:  751 | Train loss: 0.4224\n",
      "   % Time:  612s | Batch:  761 | Train loss: 0.3578\n",
      "   % Time:  620s | Batch:  771 | Train loss: 0.4017\n",
      "   % Time:  628s | Batch:  781 | Train loss: 0.3544\n",
      "   % Time:  636s | Batch:  791 | Train loss: 0.3860\n",
      "   % Time:  644s | Batch:  801 | Train loss: 0.3626\n",
      "   % Time:  652s | Batch:  811 | Train loss: 0.3248\n",
      "   % Time:  660s | Batch:  821 | Train loss: 0.4100\n",
      "   % Time:  668s | Batch:  831 | Train loss: 0.4057\n",
      "   % Time:  676s | Batch:  841 | Train loss: 0.4018\n",
      "   % Time:  684s | Batch:  851 | Train loss: 0.3678\n",
      "   % Time:  691s | Batch:  861 | Train loss: 0.3922\n",
      "   % Time:  699s | Batch:  871 | Train loss: 0.3509\n",
      "   % Time:  707s | Batch:  881 | Train loss: 0.3503\n",
      "   % Time:  715s | Batch:  891 | Train loss: 0.3963\n",
      "   % Time:  723s | Batch:  901 | Train loss: 0.3953\n",
      "   % Time:  731s | Batch:  911 | Train loss: 0.3558\n",
      "   % Time:  738s | Batch:  921 | Train loss: 0.3603\n",
      "   % Time:  746s | Batch:  931 | Train loss: 0.3958\n",
      "   % Time:  754s | Batch:  941 | Train loss: 0.3663\n",
      "   % Time:  762s | Batch:  951 | Train loss: 0.4244\n",
      "   % Time:  770s | Batch:  961 | Train loss: 0.3668\n",
      "   % Time:  778s | Batch:  971 | Train loss: 0.3502\n",
      "   % Time:  786s | Batch:  981 | Train loss: 0.3385\n",
      "   % Time:  793s | Batch:  991 | Train loss: 0.3349\n",
      "   % Time:  801s | Batch: 1001 | Train loss: 0.3381\n",
      "   % Time:  809s | Batch: 1011 | Train loss: 0.4082\n",
      "   % Time:  818s | Batch: 1021 | Train loss: 0.4213\n",
      "   % Time:  826s | Batch: 1031 | Train loss: 0.4178\n",
      "   % Time:  834s | Batch: 1041 | Train loss: 0.4073\n",
      "   % Time:  842s | Batch: 1051 | Train loss: 0.3790\n",
      "   % Time:  850s | Batch: 1061 | Train loss: 0.4262\n",
      "   % Time:  858s | Batch: 1071 | Train loss: 0.3949\n",
      "   % Time:  866s | Batch: 1081 | Train loss: 0.3775\n",
      "   % Time:  874s | Batch: 1091 | Train loss: 0.3999\n",
      "   % Time:  882s | Batch: 1101 | Train loss: 0.3538\n",
      "   % Time:  890s | Batch: 1111 | Train loss: 0.4318\n",
      "   % Time:  898s | Batch: 1121 | Train loss: 0.3953\n",
      "   % Time:  905s | Batch: 1131 | Train loss: 0.4003\n",
      "   % Time:  913s | Batch: 1141 | Train loss: 0.4017\n",
      "   % Time:  921s | Batch: 1151 | Train loss: 0.3712\n",
      "   % Time:  929s | Batch: 1161 | Train loss: 0.3497\n",
      "   % Time:  937s | Batch: 1171 | Train loss: 0.3464\n",
      "   % Time:  944s | Batch: 1181 | Train loss: 0.3920\n",
      "   % Time:  952s | Batch: 1191 | Train loss: 0.4451\n",
      "   % Time:  960s | Batch: 1201 | Train loss: 0.4088\n",
      "   % Time:  968s | Batch: 1211 | Train loss: 0.3451\n",
      "   % Time:  976s | Batch: 1221 | Train loss: 0.3692\n",
      "   % Time:  984s | Batch: 1231 | Train loss: 0.3702\n",
      "   % Time:  992s | Batch: 1241 | Train loss: 0.4054\n",
      "   % Time: 1000s | Batch: 1251 | Train loss: 0.3785\n",
      "   % Time: 1008s | Batch: 1261 | Train loss: 0.3422\n",
      "   % Time: 1015s | Batch: 1271 | Train loss: 0.3718\n",
      "   % Time: 1023s | Batch: 1281 | Train loss: 0.4168\n",
      "   % Time: 1031s | Batch: 1291 | Train loss: 0.4106\n",
      "   % Time: 1039s | Batch: 1301 | Train loss: 0.3905\n",
      "   % Time: 1047s | Batch: 1311 | Train loss: 0.4025\n",
      "   % Time: 1055s | Batch: 1321 | Train loss: 0.3767\n",
      "   % Time: 1063s | Batch: 1331 | Train loss: 0.3911\n",
      "   % Time: 1071s | Batch: 1341 | Train loss: 0.3827\n",
      "   % Time: 1079s | Batch: 1351 | Train loss: 0.3963\n",
      "   % Time: 1086s | Batch: 1361 | Train loss: 0.3891\n",
      "   % Time: 1094s | Batch: 1371 | Train loss: 0.3762\n",
      "   % Time: 1102s | Batch: 1381 | Train loss: 0.3236\n",
      "   % Time: 1110s | Batch: 1391 | Train loss: 0.4260\n",
      "   % Time: 1118s | Batch: 1401 | Train loss: 0.3529\n",
      "   % Time: 1126s | Batch: 1411 | Train loss: 0.3676\n",
      "   % Time: 1133s | Batch: 1421 | Train loss: 0.3818\n",
      "   % Time: 1141s | Batch: 1431 | Train loss: 0.3705\n",
      "   % Time: 1149s | Batch: 1441 | Train loss: 0.4013\n",
      "   % Time: 1157s | Batch: 1451 | Train loss: 0.3915\n",
      "   % Time: 1164s | Batch: 1461 | Train loss: 0.4092\n",
      "   % Time: 1172s | Batch: 1471 | Train loss: 0.3969\n",
      "   % Time: 1179s | Batch: 1481 | Train loss: 0.4054\n",
      "   % Time: 1187s | Batch: 1491 | Train loss: 0.3955\n",
      "   % Time: 1195s | Batch: 1501 | Train loss: 0.3914\n",
      "   % Time: 1202s | Batch: 1511 | Train loss: 0.4235\n",
      "   % Time: 1210s | Batch: 1521 | Train loss: 0.3948\n",
      "   % Time: 1218s | Batch: 1531 | Train loss: 0.3300\n",
      "   % Time: 1226s | Batch: 1541 | Train loss: 0.3830\n",
      "   % Time: 1233s | Batch: 1551 | Train loss: 0.3334\n",
      "   % Time: 1241s | Batch: 1561 | Train loss: 0.3730\n",
      "   % Time: 1249s | Batch: 1571 | Train loss: 0.3821\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   % Time: 1257s | Batch: 1581 | Train loss: 0.3595\n",
      "   % Time: 1264s | Batch: 1591 | Train loss: 0.3596\n",
      "   % Time: 1272s | Batch: 1601 | Train loss: 0.3673\n",
      "   % Time: 1280s | Batch: 1611 | Train loss: 0.4112\n",
      "   % Time: 1288s | Batch: 1621 | Train loss: 0.3853\n",
      "   % Time: 1296s | Batch: 1631 | Train loss: 0.3468\n",
      "   % Time: 1304s | Batch: 1641 | Train loss: 0.4241\n",
      "   % Time: 1312s | Batch: 1651 | Train loss: 0.3835\n",
      "   % Time: 1320s | Batch: 1661 | Train loss: 0.4319\n",
      "   % Time: 1328s | Batch: 1671 | Train loss: 0.4027\n",
      "   % Time: 1336s | Batch: 1681 | Train loss: 0.3714\n",
      "   % Time: 1343s | Batch: 1691 | Train loss: 0.3410\n",
      "   % Time: 1351s | Batch: 1701 | Train loss: 0.3537\n",
      "   % Time: 1359s | Batch: 1711 | Train loss: 0.3742\n",
      "   % Time: 1367s | Batch: 1721 | Train loss: 0.3649\n",
      "   % Time: 1375s | Batch: 1731 | Train loss: 0.3861\n",
      "   % Time: 1383s | Batch: 1741 | Train loss: 0.4092\n",
      "   % Time: 1391s | Batch: 1751 | Train loss: 0.3785\n",
      "   % Time: 1399s | Batch: 1761 | Train loss: 0.4136\n",
      "   % Time: 1407s | Batch: 1771 | Train loss: 0.3788\n",
      "   % Time: 1415s | Batch: 1781 | Train loss: 0.4081\n",
      "   % Time: 1424s | Batch: 1791 | Train loss: 0.3861\n",
      "   % Time: 1432s | Batch: 1801 | Train loss: 0.3776\n",
      "   % Time: 1440s | Batch: 1811 | Train loss: 0.3952\n",
      "   % Time: 1448s | Batch: 1821 | Train loss: 0.3514\n",
      "   % Time: 1456s | Batch: 1831 | Train loss: 0.4035\n",
      "   % Time: 1463s | Batch: 1841 | Train loss: 0.3779\n",
      "   % Time: 1471s | Batch: 1851 | Train loss: 0.3889\n",
      "   % Time: 1479s | Batch: 1861 | Train loss: 0.3744\n",
      "   % Time: 1487s | Batch: 1871 | Train loss: 0.4198\n",
      "   % Time: 1495s | Batch: 1881 | Train loss: 0.3487\n",
      "   % Time: 1503s | Batch: 1891 | Train loss: 0.3390\n",
      "   % Time: 1511s | Batch: 1901 | Train loss: 0.3572\n",
      "   % Time: 1519s | Batch: 1911 | Train loss: 0.4214\n",
      "   % Time: 1527s | Batch: 1921 | Train loss: 0.3641\n",
      "   % Time: 1535s | Batch: 1931 | Train loss: 0.3416\n",
      "   % Time: 1542s | Batch: 1941 | Train loss: 0.3457\n",
      "   % Time: 1550s | Batch: 1951 | Train loss: 0.3898\n",
      "   % Time: 1558s | Batch: 1961 | Train loss: 0.3546\n",
      "   % Time: 1566s | Batch: 1971 | Train loss: 0.3881\n",
      "   % Time: 1573s | Batch: 1981 | Train loss: 0.3683\n",
      "   % Time: 1581s | Batch: 1991 | Train loss: 0.3995\n",
      "   % Time: 1589s | Batch: 2001 | Train loss: 0.3743\n",
      "   % Time: 1596s | Batch: 2011 | Train loss: 0.3992\n",
      "   % Time: 1604s | Batch: 2021 | Train loss: 0.4372\n",
      "   % Time: 1612s | Batch: 2031 | Train loss: 0.3529\n",
      "   % Time: 1620s | Batch: 2041 | Train loss: 0.3642\n",
      "   % Time: 1627s | Batch: 2051 | Train loss: 0.3908\n",
      "   % Time: 1635s | Batch: 2061 | Train loss: 0.4098\n",
      "   % Time: 1643s | Batch: 2071 | Train loss: 0.3573\n",
      "   % Time: 1651s | Batch: 2081 | Train loss: 0.4095\n",
      "   % Time: 1658s | Batch: 2091 | Train loss: 0.4204\n",
      "   % Time: 1666s | Batch: 2101 | Train loss: 0.3658\n",
      "   % Time: 1674s | Batch: 2111 | Train loss: 0.3353\n",
      "   % Time: 1681s | Batch: 2121 | Train loss: 0.4006\n",
      "   % Time: 1689s | Batch: 2131 | Train loss: 0.3432\n",
      "   % Time: 1697s | Batch: 2141 | Train loss: 0.3479\n",
      "   % Time: 1705s | Batch: 2151 | Train loss: 0.4308\n",
      "   % Time: 1713s | Batch: 2161 | Train loss: 0.3918\n",
      "   % Time: 1720s | Batch: 2171 | Train loss: 0.3549\n",
      "   % Time: 1729s | Batch: 2181 | Train loss: 0.3923\n",
      "   % Time: 1736s | Batch: 2191 | Train loss: 0.3348\n",
      "   % Time: 1744s | Batch: 2201 | Train loss: 0.3670\n",
      "   % Time: 1752s | Batch: 2211 | Train loss: 0.3853\n",
      "   % Time: 1760s | Batch: 2221 | Train loss: 0.3973\n",
      "   % Time: 1768s | Batch: 2231 | Train loss: 0.3933\n",
      "   % Time: 1776s | Batch: 2241 | Train loss: 0.3509\n",
      "   % Time: 1784s | Batch: 2251 | Train loss: 0.3944\n",
      "   % Time: 1792s | Batch: 2261 | Train loss: 0.3184\n",
      "==========\n",
      "   % Epoch: 1 | Time: 1797s | Train loss: 0.3937 | Val loss: 28.0237\n",
      "==========\n",
      "=> EPOCH 2 with lr 0.001\n",
      "   % Time:    1s | Batch:    1 | Train loss: 0.3952\n",
      "   % Time:    9s | Batch:   11 | Train loss: 0.3489\n",
      "   % Time:   16s | Batch:   21 | Train loss: 0.4045\n",
      "   % Time:   24s | Batch:   31 | Train loss: 0.4308\n",
      "   % Time:   32s | Batch:   41 | Train loss: 0.4042\n",
      "   % Time:   40s | Batch:   51 | Train loss: 0.4163\n",
      "   % Time:   48s | Batch:   61 | Train loss: 0.4430\n",
      "   % Time:   56s | Batch:   71 | Train loss: 0.4163\n",
      "   % Time:   64s | Batch:   81 | Train loss: 0.3866\n",
      "   % Time:   71s | Batch:   91 | Train loss: 0.4005\n",
      "   % Time:   80s | Batch:  101 | Train loss: 0.4714\n",
      "   % Time:   88s | Batch:  111 | Train loss: 0.3525\n",
      "   % Time:   96s | Batch:  121 | Train loss: 0.4010\n",
      "   % Time:  103s | Batch:  131 | Train loss: 0.4183\n",
      "   % Time:  111s | Batch:  141 | Train loss: 0.3769\n",
      "   % Time:  119s | Batch:  151 | Train loss: 0.4134\n",
      "   % Time:  128s | Batch:  161 | Train loss: 0.4044\n",
      "   % Time:  136s | Batch:  171 | Train loss: 0.4099\n",
      "   % Time:  144s | Batch:  181 | Train loss: 0.3388\n",
      "   % Time:  151s | Batch:  191 | Train loss: 0.3721\n",
      "   % Time:  159s | Batch:  201 | Train loss: 0.3946\n",
      "   % Time:  167s | Batch:  211 | Train loss: 0.4277\n",
      "   % Time:  174s | Batch:  221 | Train loss: 0.3652\n",
      "   % Time:  182s | Batch:  231 | Train loss: 0.3914\n",
      "   % Time:  189s | Batch:  241 | Train loss: 0.4383\n",
      "   % Time:  197s | Batch:  251 | Train loss: 0.3960\n",
      "   % Time:  205s | Batch:  261 | Train loss: 0.4160\n",
      "   % Time:  212s | Batch:  271 | Train loss: 0.3908\n",
      "   % Time:  220s | Batch:  281 | Train loss: 0.3891\n",
      "   % Time:  228s | Batch:  291 | Train loss: 0.3673\n",
      "   % Time:  235s | Batch:  301 | Train loss: 0.4230\n",
      "   % Time:  243s | Batch:  311 | Train loss: 0.4014\n",
      "   % Time:  250s | Batch:  321 | Train loss: 0.3736\n",
      "   % Time:  258s | Batch:  331 | Train loss: 0.3923\n",
      "   % Time:  265s | Batch:  341 | Train loss: 0.3791\n",
      "   % Time:  273s | Batch:  351 | Train loss: 0.4199\n",
      "   % Time:  281s | Batch:  361 | Train loss: 0.4277\n",
      "   % Time:  288s | Batch:  371 | Train loss: 0.4083\n",
      "   % Time:  296s | Batch:  381 | Train loss: 0.4098\n",
      "   % Time:  303s | Batch:  391 | Train loss: 0.4027\n",
      "   % Time:  311s | Batch:  401 | Train loss: 0.4038\n",
      "   % Time:  319s | Batch:  411 | Train loss: 0.3986\n",
      "   % Time:  326s | Batch:  421 | Train loss: 0.3854\n",
      "   % Time:  334s | Batch:  431 | Train loss: 0.3742\n",
      "   % Time:  341s | Batch:  441 | Train loss: 0.4276\n",
      "   % Time:  349s | Batch:  451 | Train loss: 0.3823\n",
      "   % Time:  357s | Batch:  461 | Train loss: 0.4481\n",
      "   % Time:  364s | Batch:  471 | Train loss: 0.3993\n",
      "   % Time:  372s | Batch:  481 | Train loss: 0.4066\n",
      "   % Time:  379s | Batch:  491 | Train loss: 0.3673\n",
      "   % Time:  387s | Batch:  501 | Train loss: 0.4200\n",
      "   % Time:  395s | Batch:  511 | Train loss: 0.3966\n",
      "   % Time:  402s | Batch:  521 | Train loss: 0.3995\n",
      "   % Time:  410s | Batch:  531 | Train loss: 0.3887\n",
      "   % Time:  418s | Batch:  541 | Train loss: 0.4173\n",
      "   % Time:  425s | Batch:  551 | Train loss: 0.4585\n",
      "   % Time:  433s | Batch:  561 | Train loss: 0.3885\n",
      "   % Time:  441s | Batch:  571 | Train loss: 0.3743\n",
      "   % Time:  448s | Batch:  581 | Train loss: 0.3673\n",
      "   % Time:  456s | Batch:  591 | Train loss: 0.3323\n",
      "   % Time:  464s | Batch:  601 | Train loss: 0.3796\n",
      "   % Time:  471s | Batch:  611 | Train loss: 0.3588\n",
      "   % Time:  479s | Batch:  621 | Train loss: 0.3923\n",
      "   % Time:  487s | Batch:  631 | Train loss: 0.3984\n",
      "   % Time:  495s | Batch:  641 | Train loss: 0.2972\n",
      "   % Time:  503s | Batch:  651 | Train loss: 0.3467\n",
      "   % Time:  510s | Batch:  661 | Train loss: 0.4218\n",
      "   % Time:  518s | Batch:  671 | Train loss: 0.3815\n",
      "   % Time:  526s | Batch:  681 | Train loss: 0.3131\n",
      "   % Time:  534s | Batch:  691 | Train loss: 0.4285\n",
      "   % Time:  541s | Batch:  701 | Train loss: 0.3725\n",
      "   % Time:  549s | Batch:  711 | Train loss: 0.3995\n",
      "   % Time:  557s | Batch:  721 | Train loss: 0.3973\n",
      "   % Time:  565s | Batch:  731 | Train loss: 0.4390\n",
      "   % Time:  573s | Batch:  741 | Train loss: 0.4043\n",
      "   % Time:  581s | Batch:  751 | Train loss: 0.4223\n",
      "   % Time:  589s | Batch:  761 | Train loss: 0.3878\n",
      "   % Time:  597s | Batch:  771 | Train loss: 0.3791\n",
      "   % Time:  604s | Batch:  781 | Train loss: 0.3984\n",
      "   % Time:  612s | Batch:  791 | Train loss: 0.3708\n",
      "   % Time:  620s | Batch:  801 | Train loss: 0.3806\n",
      "   % Time:  628s | Batch:  811 | Train loss: 0.3741\n",
      "   % Time:  635s | Batch:  821 | Train loss: 0.4194\n",
      "   % Time:  643s | Batch:  831 | Train loss: 0.3983\n",
      "   % Time:  651s | Batch:  841 | Train loss: 0.3780\n",
      "   % Time:  659s | Batch:  851 | Train loss: 0.3652\n",
      "   % Time:  667s | Batch:  861 | Train loss: 0.3502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   % Time:  674s | Batch:  871 | Train loss: 0.3332\n",
      "   % Time:  682s | Batch:  881 | Train loss: 0.3441\n",
      "   % Time:  690s | Batch:  891 | Train loss: 0.3530\n",
      "   % Time:  698s | Batch:  901 | Train loss: 0.3685\n",
      "   % Time:  706s | Batch:  911 | Train loss: 0.3685\n",
      "   % Time:  713s | Batch:  921 | Train loss: 0.3949\n",
      "   % Time:  721s | Batch:  931 | Train loss: 0.3914\n",
      "   % Time:  728s | Batch:  941 | Train loss: 0.4190\n",
      "   % Time:  736s | Batch:  951 | Train loss: 0.4196\n",
      "   % Time:  744s | Batch:  961 | Train loss: 0.3509\n",
      "   % Time:  752s | Batch:  971 | Train loss: 0.3704\n",
      "   % Time:  759s | Batch:  981 | Train loss: 0.3621\n",
      "   % Time:  767s | Batch:  991 | Train loss: 0.3967\n",
      "   % Time:  775s | Batch: 1001 | Train loss: 0.3586\n",
      "   % Time:  782s | Batch: 1011 | Train loss: 0.3905\n",
      "   % Time:  790s | Batch: 1021 | Train loss: 0.3417\n",
      "   % Time:  798s | Batch: 1031 | Train loss: 0.3836\n",
      "   % Time:  805s | Batch: 1041 | Train loss: 0.3825\n",
      "   % Time:  813s | Batch: 1051 | Train loss: 0.3794\n",
      "   % Time:  821s | Batch: 1061 | Train loss: 0.4040\n",
      "   % Time:  829s | Batch: 1071 | Train loss: 0.3963\n",
      "   % Time:  837s | Batch: 1081 | Train loss: 0.4017\n",
      "   % Time:  844s | Batch: 1091 | Train loss: 0.4080\n",
      "   % Time:  852s | Batch: 1101 | Train loss: 0.3961\n",
      "   % Time:  860s | Batch: 1111 | Train loss: 0.3538\n",
      "   % Time:  868s | Batch: 1121 | Train loss: 0.3725\n",
      "   % Time:  875s | Batch: 1131 | Train loss: 0.4328\n",
      "   % Time:  883s | Batch: 1141 | Train loss: 0.3949\n",
      "   % Time:  891s | Batch: 1151 | Train loss: 0.3736\n",
      "   % Time:  898s | Batch: 1161 | Train loss: 0.3573\n",
      "   % Time:  906s | Batch: 1171 | Train loss: 0.3580\n",
      "   % Time:  914s | Batch: 1181 | Train loss: 0.3869\n",
      "   % Time:  921s | Batch: 1191 | Train loss: 0.3624\n",
      "   % Time:  929s | Batch: 1201 | Train loss: 0.3661\n",
      "   % Time:  937s | Batch: 1211 | Train loss: 0.3810\n",
      "   % Time:  944s | Batch: 1221 | Train loss: 0.3984\n",
      "   % Time:  952s | Batch: 1231 | Train loss: 0.3594\n",
      "   % Time:  960s | Batch: 1241 | Train loss: 0.3703\n",
      "   % Time:  967s | Batch: 1251 | Train loss: 0.3590\n",
      "   % Time:  975s | Batch: 1261 | Train loss: 0.3719\n",
      "   % Time:  983s | Batch: 1271 | Train loss: 0.3696\n",
      "   % Time:  990s | Batch: 1281 | Train loss: 0.3999\n",
      "   % Time:  998s | Batch: 1291 | Train loss: 0.4029\n",
      "   % Time: 1006s | Batch: 1301 | Train loss: 0.3297\n",
      "   % Time: 1014s | Batch: 1311 | Train loss: 0.3689\n",
      "   % Time: 1021s | Batch: 1321 | Train loss: 0.3961\n",
      "   % Time: 1029s | Batch: 1331 | Train loss: 0.4076\n",
      "   % Time: 1037s | Batch: 1341 | Train loss: 0.4417\n",
      "   % Time: 1045s | Batch: 1351 | Train loss: 0.3827\n",
      "   % Time: 1052s | Batch: 1361 | Train loss: 0.4386\n",
      "   % Time: 1060s | Batch: 1371 | Train loss: 0.3682\n",
      "   % Time: 1068s | Batch: 1381 | Train loss: 0.4085\n",
      "   % Time: 1076s | Batch: 1391 | Train loss: 0.4026\n",
      "   % Time: 1084s | Batch: 1401 | Train loss: 0.3737\n",
      "   % Time: 1092s | Batch: 1411 | Train loss: 0.3377\n",
      "   % Time: 1100s | Batch: 1421 | Train loss: 0.4238\n",
      "   % Time: 1108s | Batch: 1431 | Train loss: 0.3931\n",
      "   % Time: 1116s | Batch: 1441 | Train loss: 0.3846\n",
      "   % Time: 1124s | Batch: 1451 | Train loss: 0.3831\n",
      "   % Time: 1131s | Batch: 1461 | Train loss: 0.4051\n",
      "   % Time: 1139s | Batch: 1471 | Train loss: 0.3793\n",
      "   % Time: 1147s | Batch: 1481 | Train loss: 0.3832\n",
      "   % Time: 1155s | Batch: 1491 | Train loss: 0.4086\n",
      "   % Time: 1163s | Batch: 1501 | Train loss: 0.4248\n",
      "   % Time: 1170s | Batch: 1511 | Train loss: 0.4028\n",
      "   % Time: 1178s | Batch: 1521 | Train loss: 0.3529\n",
      "   % Time: 1186s | Batch: 1531 | Train loss: 0.3878\n",
      "   % Time: 1193s | Batch: 1541 | Train loss: 0.4188\n",
      "   % Time: 1201s | Batch: 1551 | Train loss: 0.3907\n",
      "   % Time: 1209s | Batch: 1561 | Train loss: 0.3894\n",
      "   % Time: 1216s | Batch: 1571 | Train loss: 0.3641\n",
      "   % Time: 1224s | Batch: 1581 | Train loss: 0.3833\n",
      "   % Time: 1232s | Batch: 1591 | Train loss: 0.3613\n",
      "   % Time: 1240s | Batch: 1601 | Train loss: 0.4636\n",
      "   % Time: 1247s | Batch: 1611 | Train loss: 0.4480\n",
      "   % Time: 1255s | Batch: 1621 | Train loss: 0.3770\n",
      "   % Time: 1263s | Batch: 1631 | Train loss: 0.3819\n",
      "   % Time: 1271s | Batch: 1641 | Train loss: 0.3895\n",
      "   % Time: 1279s | Batch: 1651 | Train loss: 0.3796\n",
      "   % Time: 1287s | Batch: 1661 | Train loss: 0.4062\n",
      "   % Time: 1295s | Batch: 1671 | Train loss: 0.4064\n",
      "   % Time: 1303s | Batch: 1681 | Train loss: 0.3798\n",
      "   % Time: 1311s | Batch: 1691 | Train loss: 0.3874\n",
      "   % Time: 1318s | Batch: 1701 | Train loss: 0.4201\n",
      "   % Time: 1327s | Batch: 1711 | Train loss: 0.3838\n",
      "   % Time: 1335s | Batch: 1721 | Train loss: 0.3938\n",
      "   % Time: 1342s | Batch: 1731 | Train loss: 0.3604\n",
      "   % Time: 1350s | Batch: 1741 | Train loss: 0.3690\n",
      "   % Time: 1358s | Batch: 1751 | Train loss: 0.4050\n",
      "   % Time: 1366s | Batch: 1761 | Train loss: 0.4437\n",
      "   % Time: 1374s | Batch: 1771 | Train loss: 0.3648\n",
      "   % Time: 1381s | Batch: 1781 | Train loss: 0.4130\n",
      "   % Time: 1389s | Batch: 1791 | Train loss: 0.4118\n",
      "   % Time: 1397s | Batch: 1801 | Train loss: 0.3575\n",
      "   % Time: 1405s | Batch: 1811 | Train loss: 0.3980\n",
      "   % Time: 1412s | Batch: 1821 | Train loss: 0.4402\n",
      "   % Time: 1420s | Batch: 1831 | Train loss: 0.3520\n",
      "   % Time: 1428s | Batch: 1841 | Train loss: 0.3805\n",
      "   % Time: 1435s | Batch: 1851 | Train loss: 0.3866\n",
      "   % Time: 1443s | Batch: 1861 | Train loss: 0.3410\n",
      "   % Time: 1451s | Batch: 1871 | Train loss: 0.3321\n",
      "   % Time: 1459s | Batch: 1881 | Train loss: 0.3925\n",
      "   % Time: 1466s | Batch: 1891 | Train loss: 0.4240\n",
      "   % Time: 1474s | Batch: 1901 | Train loss: 0.3116\n",
      "   % Time: 1482s | Batch: 1911 | Train loss: 0.3660\n",
      "   % Time: 1490s | Batch: 1921 | Train loss: 0.4011\n",
      "   % Time: 1498s | Batch: 1931 | Train loss: 0.4376\n",
      "   % Time: 1505s | Batch: 1941 | Train loss: 0.3938\n",
      "   % Time: 1513s | Batch: 1951 | Train loss: 0.4317\n",
      "   % Time: 1521s | Batch: 1961 | Train loss: 0.3921\n",
      "   % Time: 1529s | Batch: 1971 | Train loss: 0.3853\n",
      "   % Time: 1537s | Batch: 1981 | Train loss: 0.4588\n",
      "   % Time: 1545s | Batch: 1991 | Train loss: 0.4002\n",
      "   % Time: 1553s | Batch: 2001 | Train loss: 0.3729\n",
      "   % Time: 1561s | Batch: 2011 | Train loss: 0.3710\n",
      "   % Time: 1569s | Batch: 2021 | Train loss: 0.3363\n",
      "   % Time: 1577s | Batch: 2031 | Train loss: 0.3077\n",
      "   % Time: 1585s | Batch: 2041 | Train loss: 0.3623\n",
      "   % Time: 1593s | Batch: 2051 | Train loss: 0.4259\n",
      "   % Time: 1601s | Batch: 2061 | Train loss: 0.3842\n",
      "   % Time: 1608s | Batch: 2071 | Train loss: 0.3510\n",
      "   % Time: 1616s | Batch: 2081 | Train loss: 0.3202\n",
      "   % Time: 1624s | Batch: 2091 | Train loss: 0.3461\n",
      "   % Time: 1632s | Batch: 2101 | Train loss: 0.3739\n",
      "   % Time: 1640s | Batch: 2111 | Train loss: 0.4062\n",
      "   % Time: 1648s | Batch: 2121 | Train loss: 0.3698\n",
      "   % Time: 1656s | Batch: 2131 | Train loss: 0.4173\n",
      "   % Time: 1664s | Batch: 2141 | Train loss: 0.3322\n",
      "   % Time: 1672s | Batch: 2151 | Train loss: 0.3510\n",
      "   % Time: 1680s | Batch: 2161 | Train loss: 0.3993\n",
      "   % Time: 1688s | Batch: 2171 | Train loss: 0.3821\n",
      "   % Time: 1696s | Batch: 2181 | Train loss: 0.3518\n",
      "   % Time: 1704s | Batch: 2191 | Train loss: 0.3482\n",
      "   % Time: 1711s | Batch: 2201 | Train loss: 0.3603\n",
      "   % Time: 1719s | Batch: 2211 | Train loss: 0.3862\n",
      "   % Time: 1727s | Batch: 2221 | Train loss: 0.3442\n",
      "   % Time: 1735s | Batch: 2231 | Train loss: 0.3679\n",
      "   % Time: 1743s | Batch: 2241 | Train loss: 0.3680\n",
      "   % Time: 1751s | Batch: 2251 | Train loss: 0.3361\n",
      "   % Time: 1758s | Batch: 2261 | Train loss: 0.3582\n",
      "==========\n",
      "   % Epoch: 2 | Time: 1763s | Train loss: 0.3861 | Val loss: 27.4106\n",
      "==========\n",
      "=> EPOCH 3 with lr 0.001\n",
      "   % Time:    1s | Batch:    1 | Train loss: 0.3587\n",
      "   % Time:    8s | Batch:   11 | Train loss: 0.4260\n",
      "   % Time:   16s | Batch:   21 | Train loss: 0.4582\n",
      "   % Time:   24s | Batch:   31 | Train loss: 0.3863\n",
      "   % Time:   32s | Batch:   41 | Train loss: 0.4069\n",
      "   % Time:   40s | Batch:   51 | Train loss: 0.4029\n",
      "   % Time:   48s | Batch:   61 | Train loss: 0.4413\n",
      "   % Time:   55s | Batch:   71 | Train loss: 0.3180\n",
      "   % Time:   63s | Batch:   81 | Train loss: 0.3888\n",
      "   % Time:   71s | Batch:   91 | Train loss: 0.3790\n",
      "   % Time:   79s | Batch:  101 | Train loss: 0.4143\n",
      "   % Time:   87s | Batch:  111 | Train loss: 0.3904\n",
      "   % Time:   95s | Batch:  121 | Train loss: 0.3972\n",
      "   % Time:  103s | Batch:  131 | Train loss: 0.3514\n",
      "   % Time:  110s | Batch:  141 | Train loss: 0.3308\n",
      "   % Time:  118s | Batch:  151 | Train loss: 0.4182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   % Time:  126s | Batch:  161 | Train loss: 0.4184\n",
      "   % Time:  134s | Batch:  171 | Train loss: 0.3839\n",
      "   % Time:  142s | Batch:  181 | Train loss: 0.3754\n",
      "   % Time:  150s | Batch:  191 | Train loss: 0.3884\n",
      "   % Time:  157s | Batch:  201 | Train loss: 0.3900\n",
      "   % Time:  165s | Batch:  211 | Train loss: 0.4048\n",
      "   % Time:  173s | Batch:  221 | Train loss: 0.3143\n",
      "   % Time:  180s | Batch:  231 | Train loss: 0.4186\n",
      "   % Time:  188s | Batch:  241 | Train loss: 0.4004\n",
      "   % Time:  196s | Batch:  251 | Train loss: 0.3735\n",
      "   % Time:  204s | Batch:  261 | Train loss: 0.3468\n",
      "   % Time:  212s | Batch:  271 | Train loss: 0.3527\n",
      "   % Time:  220s | Batch:  281 | Train loss: 0.4036\n",
      "   % Time:  227s | Batch:  291 | Train loss: 0.3786\n",
      "   % Time:  235s | Batch:  301 | Train loss: 0.4017\n",
      "   % Time:  243s | Batch:  311 | Train loss: 0.3926\n",
      "   % Time:  251s | Batch:  321 | Train loss: 0.3614\n",
      "   % Time:  259s | Batch:  331 | Train loss: 0.4085\n",
      "   % Time:  267s | Batch:  341 | Train loss: 0.3879\n",
      "   % Time:  275s | Batch:  351 | Train loss: 0.3618\n",
      "   % Time:  282s | Batch:  361 | Train loss: 0.4118\n",
      "   % Time:  290s | Batch:  371 | Train loss: 0.3827\n",
      "   % Time:  298s | Batch:  381 | Train loss: 0.3968\n",
      "   % Time:  305s | Batch:  391 | Train loss: 0.3658\n",
      "   % Time:  313s | Batch:  401 | Train loss: 0.3511\n",
      "   % Time:  321s | Batch:  411 | Train loss: 0.3445\n",
      "   % Time:  329s | Batch:  421 | Train loss: 0.4224\n",
      "   % Time:  336s | Batch:  431 | Train loss: 0.3976\n",
      "   % Time:  344s | Batch:  441 | Train loss: 0.3519\n",
      "   % Time:  352s | Batch:  451 | Train loss: 0.3883\n",
      "   % Time:  359s | Batch:  461 | Train loss: 0.4637\n",
      "   % Time:  367s | Batch:  471 | Train loss: 0.3378\n",
      "   % Time:  374s | Batch:  481 | Train loss: 0.4033\n",
      "   % Time:  382s | Batch:  491 | Train loss: 0.3723\n",
      "   % Time:  390s | Batch:  501 | Train loss: 0.3526\n",
      "   % Time:  397s | Batch:  511 | Train loss: 0.4055\n",
      "   % Time:  405s | Batch:  521 | Train loss: 0.3985\n",
      "   % Time:  412s | Batch:  531 | Train loss: 0.4121\n",
      "   % Time:  420s | Batch:  541 | Train loss: 0.3644\n",
      "   % Time:  428s | Batch:  551 | Train loss: 0.3273\n",
      "   % Time:  435s | Batch:  561 | Train loss: 0.3374\n",
      "   % Time:  443s | Batch:  571 | Train loss: 0.3901\n",
      "   % Time:  450s | Batch:  581 | Train loss: 0.3569\n",
      "   % Time:  458s | Batch:  591 | Train loss: 0.3261\n",
      "   % Time:  465s | Batch:  601 | Train loss: 0.4102\n",
      "   % Time:  473s | Batch:  611 | Train loss: 0.4241\n",
      "   % Time:  481s | Batch:  621 | Train loss: 0.3844\n",
      "   % Time:  488s | Batch:  631 | Train loss: 0.3967\n",
      "   % Time:  496s | Batch:  641 | Train loss: 0.3056\n",
      "   % Time:  503s | Batch:  651 | Train loss: 0.3379\n",
      "   % Time:  511s | Batch:  661 | Train loss: 0.3516\n",
      "   % Time:  519s | Batch:  671 | Train loss: 0.3857\n",
      "   % Time:  526s | Batch:  681 | Train loss: 0.3897\n",
      "   % Time:  534s | Batch:  691 | Train loss: 0.3381\n",
      "   % Time:  541s | Batch:  701 | Train loss: 0.3335\n",
      "   % Time:  549s | Batch:  711 | Train loss: 0.3910\n",
      "   % Time:  556s | Batch:  721 | Train loss: 0.3261\n",
      "   % Time:  564s | Batch:  731 | Train loss: 0.3688\n",
      "   % Time:  572s | Batch:  741 | Train loss: 0.3939\n",
      "   % Time:  579s | Batch:  751 | Train loss: 0.3546\n",
      "   % Time:  587s | Batch:  761 | Train loss: 0.4162\n",
      "   % Time:  594s | Batch:  771 | Train loss: 0.3533\n",
      "   % Time:  602s | Batch:  781 | Train loss: 0.3926\n",
      "   % Time:  610s | Batch:  791 | Train loss: 0.4137\n",
      "   % Time:  617s | Batch:  801 | Train loss: 0.3596\n",
      "   % Time:  625s | Batch:  811 | Train loss: 0.3724\n",
      "   % Time:  633s | Batch:  821 | Train loss: 0.3881\n",
      "   % Time:  640s | Batch:  831 | Train loss: 0.3632\n",
      "   % Time:  648s | Batch:  841 | Train loss: 0.3794\n",
      "   % Time:  655s | Batch:  851 | Train loss: 0.3788\n",
      "   % Time:  663s | Batch:  861 | Train loss: 0.3838\n",
      "   % Time:  671s | Batch:  871 | Train loss: 0.4238\n",
      "   % Time:  678s | Batch:  881 | Train loss: 0.3962\n",
      "   % Time:  686s | Batch:  891 | Train loss: 0.3638\n",
      "   % Time:  694s | Batch:  901 | Train loss: 0.4180\n",
      "   % Time:  701s | Batch:  911 | Train loss: 0.3879\n",
      "   % Time:  709s | Batch:  921 | Train loss: 0.3862\n",
      "   % Time:  717s | Batch:  931 | Train loss: 0.3581\n",
      "   % Time:  724s | Batch:  941 | Train loss: 0.3764\n",
      "   % Time:  732s | Batch:  951 | Train loss: 0.3701\n",
      "   % Time:  740s | Batch:  961 | Train loss: 0.4014\n",
      "   % Time:  747s | Batch:  971 | Train loss: 0.3746\n",
      "   % Time:  755s | Batch:  981 | Train loss: 0.3707\n",
      "   % Time:  763s | Batch:  991 | Train loss: 0.3783\n",
      "   % Time:  770s | Batch: 1001 | Train loss: 0.3752\n",
      "   % Time:  778s | Batch: 1011 | Train loss: 0.3555\n",
      "   % Time:  785s | Batch: 1021 | Train loss: 0.3771\n",
      "   % Time:  793s | Batch: 1031 | Train loss: 0.3913\n",
      "   % Time:  801s | Batch: 1041 | Train loss: 0.4031\n",
      "   % Time:  808s | Batch: 1051 | Train loss: 0.3448\n",
      "   % Time:  816s | Batch: 1061 | Train loss: 0.4194\n",
      "   % Time:  823s | Batch: 1071 | Train loss: 0.3963\n",
      "   % Time:  831s | Batch: 1081 | Train loss: 0.3397\n",
      "   % Time:  839s | Batch: 1091 | Train loss: 0.3571\n",
      "   % Time:  846s | Batch: 1101 | Train loss: 0.4092\n",
      "   % Time:  854s | Batch: 1111 | Train loss: 0.3813\n",
      "   % Time:  861s | Batch: 1121 | Train loss: 0.4199\n",
      "   % Time:  869s | Batch: 1131 | Train loss: 0.4063\n",
      "   % Time:  877s | Batch: 1141 | Train loss: 0.3908\n",
      "   % Time:  884s | Batch: 1151 | Train loss: 0.3597\n",
      "   % Time:  892s | Batch: 1161 | Train loss: 0.3486\n",
      "   % Time:  900s | Batch: 1171 | Train loss: 0.3958\n",
      "   % Time:  907s | Batch: 1181 | Train loss: 0.4371\n",
      "   % Time:  915s | Batch: 1191 | Train loss: 0.3703\n",
      "   % Time:  923s | Batch: 1201 | Train loss: 0.4006\n",
      "   % Time:  930s | Batch: 1211 | Train loss: 0.3685\n",
      "   % Time:  938s | Batch: 1221 | Train loss: 0.4023\n",
      "   % Time:  946s | Batch: 1231 | Train loss: 0.3931\n",
      "   % Time:  953s | Batch: 1241 | Train loss: 0.3587\n",
      "   % Time:  961s | Batch: 1251 | Train loss: 0.3309\n",
      "   % Time:  969s | Batch: 1261 | Train loss: 0.3386\n",
      "   % Time:  976s | Batch: 1271 | Train loss: 0.3800\n",
      "   % Time:  984s | Batch: 1281 | Train loss: 0.3199\n",
      "   % Time:  992s | Batch: 1291 | Train loss: 0.3842\n",
      "   % Time: 1000s | Batch: 1301 | Train loss: 0.3012\n",
      "   % Time: 1007s | Batch: 1311 | Train loss: 0.3739\n",
      "   % Time: 1015s | Batch: 1321 | Train loss: 0.4044\n",
      "   % Time: 1023s | Batch: 1331 | Train loss: 0.3433\n",
      "   % Time: 1030s | Batch: 1341 | Train loss: 0.3774\n",
      "   % Time: 1038s | Batch: 1351 | Train loss: 0.3926\n",
      "   % Time: 1046s | Batch: 1361 | Train loss: 0.3721\n",
      "   % Time: 1053s | Batch: 1371 | Train loss: 0.3582\n",
      "   % Time: 1061s | Batch: 1381 | Train loss: 0.3731\n",
      "   % Time: 1069s | Batch: 1391 | Train loss: 0.3675\n",
      "   % Time: 1076s | Batch: 1401 | Train loss: 0.4395\n",
      "   % Time: 1084s | Batch: 1411 | Train loss: 0.3363\n",
      "   % Time: 1092s | Batch: 1421 | Train loss: 0.3471\n",
      "   % Time: 1099s | Batch: 1431 | Train loss: 0.3410\n",
      "   % Time: 1107s | Batch: 1441 | Train loss: 0.4131\n",
      "   % Time: 1115s | Batch: 1451 | Train loss: 0.3551\n",
      "   % Time: 1123s | Batch: 1461 | Train loss: 0.4060\n",
      "   % Time: 1130s | Batch: 1471 | Train loss: 0.3692\n",
      "   % Time: 1138s | Batch: 1481 | Train loss: 0.3640\n",
      "   % Time: 1146s | Batch: 1491 | Train loss: 0.3733\n",
      "   % Time: 1153s | Batch: 1501 | Train loss: 0.4007\n",
      "   % Time: 1161s | Batch: 1511 | Train loss: 0.3818\n",
      "   % Time: 1169s | Batch: 1521 | Train loss: 0.3707\n",
      "   % Time: 1176s | Batch: 1531 | Train loss: 0.3837\n",
      "   % Time: 1184s | Batch: 1541 | Train loss: 0.3798\n",
      "   % Time: 1192s | Batch: 1551 | Train loss: 0.3476\n",
      "   % Time: 1199s | Batch: 1561 | Train loss: 0.3814\n",
      "   % Time: 1207s | Batch: 1571 | Train loss: 0.3230\n",
      "   % Time: 1215s | Batch: 1581 | Train loss: 0.4077\n",
      "   % Time: 1222s | Batch: 1591 | Train loss: 0.4361\n",
      "   % Time: 1230s | Batch: 1601 | Train loss: 0.3369\n",
      "   % Time: 1238s | Batch: 1611 | Train loss: 0.3856\n",
      "   % Time: 1245s | Batch: 1621 | Train loss: 0.3944\n",
      "   % Time: 1253s | Batch: 1631 | Train loss: 0.3674\n",
      "   % Time: 1261s | Batch: 1641 | Train loss: 0.4174\n",
      "   % Time: 1269s | Batch: 1651 | Train loss: 0.4100\n",
      "   % Time: 1276s | Batch: 1661 | Train loss: 0.3850\n",
      "   % Time: 1284s | Batch: 1671 | Train loss: 0.3673\n",
      "   % Time: 1292s | Batch: 1681 | Train loss: 0.3995\n",
      "   % Time: 1300s | Batch: 1691 | Train loss: 0.3605\n",
      "   % Time: 1308s | Batch: 1701 | Train loss: 0.3909\n",
      "   % Time: 1316s | Batch: 1711 | Train loss: 0.3379\n",
      "   % Time: 1323s | Batch: 1721 | Train loss: 0.3631\n",
      "   % Time: 1331s | Batch: 1731 | Train loss: 0.3746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   % Time: 1339s | Batch: 1741 | Train loss: 0.3794\n",
      "   % Time: 1347s | Batch: 1751 | Train loss: 0.4205\n",
      "   % Time: 1355s | Batch: 1761 | Train loss: 0.3718\n",
      "   % Time: 1363s | Batch: 1771 | Train loss: 0.3656\n",
      "   % Time: 1370s | Batch: 1781 | Train loss: 0.3819\n",
      "   % Time: 1378s | Batch: 1791 | Train loss: 0.4100\n",
      "   % Time: 1386s | Batch: 1801 | Train loss: 0.4017\n",
      "   % Time: 1394s | Batch: 1811 | Train loss: 0.3221\n",
      "   % Time: 1401s | Batch: 1821 | Train loss: 0.4319\n",
      "   % Time: 1409s | Batch: 1831 | Train loss: 0.3773\n",
      "   % Time: 1417s | Batch: 1841 | Train loss: 0.4076\n",
      "   % Time: 1425s | Batch: 1851 | Train loss: 0.3475\n",
      "   % Time: 1433s | Batch: 1861 | Train loss: 0.3193\n",
      "   % Time: 1441s | Batch: 1871 | Train loss: 0.3219\n",
      "   % Time: 1449s | Batch: 1881 | Train loss: 0.3812\n",
      "   % Time: 1457s | Batch: 1891 | Train loss: 0.4112\n",
      "   % Time: 1465s | Batch: 1901 | Train loss: 0.3805\n",
      "   % Time: 1472s | Batch: 1911 | Train loss: 0.3883\n",
      "   % Time: 1480s | Batch: 1921 | Train loss: 0.3818\n",
      "   % Time: 1488s | Batch: 1931 | Train loss: 0.3719\n",
      "   % Time: 1496s | Batch: 1941 | Train loss: 0.3750\n",
      "   % Time: 1504s | Batch: 1951 | Train loss: 0.3676\n",
      "   % Time: 1512s | Batch: 1961 | Train loss: 0.3635\n",
      "   % Time: 1520s | Batch: 1971 | Train loss: 0.3733\n",
      "   % Time: 1528s | Batch: 1981 | Train loss: 0.3590\n",
      "   % Time: 1536s | Batch: 1991 | Train loss: 0.3725\n",
      "   % Time: 1543s | Batch: 2001 | Train loss: 0.4669\n",
      "   % Time: 1551s | Batch: 2011 | Train loss: 0.3740\n",
      "   % Time: 1559s | Batch: 2021 | Train loss: 0.3814\n",
      "   % Time: 1566s | Batch: 2031 | Train loss: 0.3218\n",
      "   % Time: 1574s | Batch: 2041 | Train loss: 0.3607\n",
      "   % Time: 1582s | Batch: 2051 | Train loss: 0.4096\n",
      "   % Time: 1589s | Batch: 2061 | Train loss: 0.4074\n",
      "   % Time: 1597s | Batch: 2071 | Train loss: 0.3463\n",
      "   % Time: 1604s | Batch: 2081 | Train loss: 0.3649\n",
      "   % Time: 1612s | Batch: 2091 | Train loss: 0.3987\n",
      "   % Time: 1620s | Batch: 2101 | Train loss: 0.3502\n",
      "   % Time: 1627s | Batch: 2111 | Train loss: 0.4210\n",
      "   % Time: 1635s | Batch: 2121 | Train loss: 0.3623\n",
      "   % Time: 1643s | Batch: 2131 | Train loss: 0.4211\n",
      "   % Time: 1650s | Batch: 2141 | Train loss: 0.3481\n",
      "   % Time: 1658s | Batch: 2151 | Train loss: 0.3698\n",
      "   % Time: 1665s | Batch: 2161 | Train loss: 0.4465\n",
      "   % Time: 1673s | Batch: 2171 | Train loss: 0.3509\n",
      "   % Time: 1681s | Batch: 2181 | Train loss: 0.4367\n",
      "   % Time: 1689s | Batch: 2191 | Train loss: 0.3702\n",
      "   % Time: 1697s | Batch: 2201 | Train loss: 0.3902\n",
      "   % Time: 1705s | Batch: 2211 | Train loss: 0.3741\n",
      "   % Time: 1712s | Batch: 2221 | Train loss: 0.3776\n",
      "   % Time: 1720s | Batch: 2231 | Train loss: 0.3935\n",
      "   % Time: 1728s | Batch: 2241 | Train loss: 0.3894\n",
      "   % Time: 1736s | Batch: 2251 | Train loss: 0.3629\n",
      "   % Time: 1744s | Batch: 2261 | Train loss: 0.3941\n",
      "==========\n",
      "   % Epoch: 3 | Time: 1749s | Train loss: 0.3807 | Val loss: 27.0500\n",
      "==========\n",
      "=> EPOCH 4 with lr 0.0001\n",
      "   % Time:    1s | Batch:    1 | Train loss: 0.4079\n",
      "   % Time:    9s | Batch:   11 | Train loss: 0.3496\n",
      "   % Time:   16s | Batch:   21 | Train loss: 0.3730\n",
      "   % Time:   24s | Batch:   31 | Train loss: 0.3314\n",
      "   % Time:   32s | Batch:   41 | Train loss: 0.3472\n",
      "   % Time:   40s | Batch:   51 | Train loss: 0.3630\n",
      "   % Time:   48s | Batch:   61 | Train loss: 0.3791\n",
      "   % Time:   56s | Batch:   71 | Train loss: 0.4486\n",
      "   % Time:   63s | Batch:   81 | Train loss: 0.3802\n",
      "   % Time:   71s | Batch:   91 | Train loss: 0.4020\n",
      "   % Time:   79s | Batch:  101 | Train loss: 0.3572\n",
      "   % Time:   86s | Batch:  111 | Train loss: 0.3335\n",
      "   % Time:   94s | Batch:  121 | Train loss: 0.4011\n",
      "   % Time:  101s | Batch:  131 | Train loss: 0.3747\n",
      "   % Time:  109s | Batch:  141 | Train loss: 0.3845\n",
      "   % Time:  117s | Batch:  151 | Train loss: 0.3371\n",
      "   % Time:  124s | Batch:  161 | Train loss: 0.3779\n",
      "   % Time:  132s | Batch:  171 | Train loss: 0.3837\n",
      "   % Time:  139s | Batch:  181 | Train loss: 0.3923\n",
      "   % Time:  147s | Batch:  191 | Train loss: 0.3950\n",
      "   % Time:  155s | Batch:  201 | Train loss: 0.3515\n",
      "   % Time:  163s | Batch:  211 | Train loss: 0.3864\n",
      "   % Time:  170s | Batch:  221 | Train loss: 0.3876\n",
      "   % Time:  178s | Batch:  231 | Train loss: 0.4262\n",
      "   % Time:  186s | Batch:  241 | Train loss: 0.3629\n",
      "   % Time:  193s | Batch:  251 | Train loss: 0.3780\n",
      "   % Time:  201s | Batch:  261 | Train loss: 0.3966\n",
      "   % Time:  208s | Batch:  271 | Train loss: 0.3366\n",
      "   % Time:  216s | Batch:  281 | Train loss: 0.3880\n",
      "   % Time:  223s | Batch:  291 | Train loss: 0.4108\n",
      "   % Time:  231s | Batch:  301 | Train loss: 0.3485\n",
      "   % Time:  238s | Batch:  311 | Train loss: 0.4024\n",
      "   % Time:  246s | Batch:  321 | Train loss: 0.4166\n",
      "   % Time:  253s | Batch:  331 | Train loss: 0.3708\n",
      "   % Time:  261s | Batch:  341 | Train loss: 0.3805\n",
      "   % Time:  268s | Batch:  351 | Train loss: 0.3609\n",
      "   % Time:  276s | Batch:  361 | Train loss: 0.3814\n",
      "   % Time:  284s | Batch:  371 | Train loss: 0.3705\n",
      "   % Time:  291s | Batch:  381 | Train loss: 0.3986\n",
      "   % Time:  299s | Batch:  391 | Train loss: 0.3749\n",
      "   % Time:  306s | Batch:  401 | Train loss: 0.3899\n",
      "   % Time:  314s | Batch:  411 | Train loss: 0.3890\n",
      "   % Time:  321s | Batch:  421 | Train loss: 0.3976\n",
      "   % Time:  329s | Batch:  431 | Train loss: 0.4233\n",
      "   % Time:  336s | Batch:  441 | Train loss: 0.3717\n",
      "   % Time:  344s | Batch:  451 | Train loss: 0.3655\n",
      "   % Time:  352s | Batch:  461 | Train loss: 0.3605\n",
      "   % Time:  359s | Batch:  471 | Train loss: 0.3220\n",
      "   % Time:  367s | Batch:  481 | Train loss: 0.3212\n",
      "   % Time:  374s | Batch:  491 | Train loss: 0.3895\n",
      "   % Time:  382s | Batch:  501 | Train loss: 0.3755\n",
      "   % Time:  389s | Batch:  511 | Train loss: 0.4242\n",
      "   % Time:  397s | Batch:  521 | Train loss: 0.4095\n",
      "   % Time:  404s | Batch:  531 | Train loss: 0.4122\n",
      "   % Time:  412s | Batch:  541 | Train loss: 0.4047\n",
      "   % Time:  420s | Batch:  551 | Train loss: 0.3830\n",
      "   % Time:  427s | Batch:  561 | Train loss: 0.3855\n",
      "   % Time:  435s | Batch:  571 | Train loss: 0.4004\n",
      "   % Time:  442s | Batch:  581 | Train loss: 0.4090\n",
      "   % Time:  450s | Batch:  591 | Train loss: 0.3890\n",
      "   % Time:  457s | Batch:  601 | Train loss: 0.4013\n",
      "   % Time:  465s | Batch:  611 | Train loss: 0.3860\n",
      "   % Time:  472s | Batch:  621 | Train loss: 0.3500\n",
      "   % Time:  480s | Batch:  631 | Train loss: 0.3817\n",
      "   % Time:  488s | Batch:  641 | Train loss: 0.3925\n",
      "   % Time:  495s | Batch:  651 | Train loss: 0.3834\n",
      "   % Time:  503s | Batch:  661 | Train loss: 0.3947\n",
      "   % Time:  510s | Batch:  671 | Train loss: 0.3360\n",
      "   % Time:  518s | Batch:  681 | Train loss: 0.3688\n",
      "   % Time:  526s | Batch:  691 | Train loss: 0.3866\n",
      "   % Time:  533s | Batch:  701 | Train loss: 0.3121\n",
      "   % Time:  541s | Batch:  711 | Train loss: 0.3708\n",
      "   % Time:  548s | Batch:  721 | Train loss: 0.3670\n",
      "   % Time:  556s | Batch:  731 | Train loss: 0.3943\n",
      "   % Time:  564s | Batch:  741 | Train loss: 0.4107\n",
      "   % Time:  571s | Batch:  751 | Train loss: 0.3850\n",
      "   % Time:  579s | Batch:  761 | Train loss: 0.3432\n",
      "   % Time:  587s | Batch:  771 | Train loss: 0.3343\n",
      "   % Time:  594s | Batch:  781 | Train loss: 0.3577\n",
      "   % Time:  602s | Batch:  791 | Train loss: 0.3847\n",
      "   % Time:  609s | Batch:  801 | Train loss: 0.3892\n",
      "   % Time:  617s | Batch:  811 | Train loss: 0.3509\n",
      "   % Time:  624s | Batch:  821 | Train loss: 0.3502\n",
      "   % Time:  632s | Batch:  831 | Train loss: 0.4100\n",
      "   % Time:  640s | Batch:  841 | Train loss: 0.3691\n",
      "   % Time:  647s | Batch:  851 | Train loss: 0.3672\n",
      "   % Time:  655s | Batch:  861 | Train loss: 0.3350\n",
      "   % Time:  663s | Batch:  871 | Train loss: 0.3571\n",
      "   % Time:  671s | Batch:  881 | Train loss: 0.3838\n",
      "   % Time:  679s | Batch:  891 | Train loss: 0.3671\n",
      "   % Time:  687s | Batch:  901 | Train loss: 0.3605\n",
      "   % Time:  695s | Batch:  911 | Train loss: 0.3764\n",
      "   % Time:  703s | Batch:  921 | Train loss: 0.3773\n",
      "   % Time:  711s | Batch:  931 | Train loss: 0.4024\n",
      "   % Time:  719s | Batch:  941 | Train loss: 0.4093\n",
      "   % Time:  727s | Batch:  951 | Train loss: 0.3936\n",
      "   % Time:  735s | Batch:  961 | Train loss: 0.4322\n",
      "   % Time:  743s | Batch:  971 | Train loss: 0.3657\n",
      "   % Time:  751s | Batch:  981 | Train loss: 0.4162\n",
      "   % Time:  759s | Batch:  991 | Train loss: 0.3685\n",
      "   % Time:  767s | Batch: 1001 | Train loss: 0.3969\n",
      "   % Time:  775s | Batch: 1011 | Train loss: 0.3905\n",
      "   % Time:  783s | Batch: 1021 | Train loss: 0.3738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   % Time:  790s | Batch: 1031 | Train loss: 0.3952\n",
      "   % Time:  798s | Batch: 1041 | Train loss: 0.3612\n",
      "   % Time:  806s | Batch: 1051 | Train loss: 0.3735\n",
      "   % Time:  814s | Batch: 1061 | Train loss: 0.4095\n",
      "   % Time:  822s | Batch: 1071 | Train loss: 0.3898\n",
      "   % Time:  830s | Batch: 1081 | Train loss: 0.3397\n",
      "   % Time:  838s | Batch: 1091 | Train loss: 0.3780\n",
      "   % Time:  846s | Batch: 1101 | Train loss: 0.3657\n",
      "   % Time:  853s | Batch: 1111 | Train loss: 0.3586\n",
      "   % Time:  861s | Batch: 1121 | Train loss: 0.3822\n",
      "   % Time:  869s | Batch: 1131 | Train loss: 0.4108\n",
      "   % Time:  877s | Batch: 1141 | Train loss: 0.3730\n",
      "   % Time:  885s | Batch: 1151 | Train loss: 0.3816\n",
      "   % Time:  893s | Batch: 1161 | Train loss: 0.3823\n",
      "   % Time:  901s | Batch: 1171 | Train loss: 0.3428\n",
      "   % Time:  909s | Batch: 1181 | Train loss: 0.3990\n",
      "   % Time:  917s | Batch: 1191 | Train loss: 0.4203\n",
      "   % Time:  924s | Batch: 1201 | Train loss: 0.3770\n",
      "   % Time:  933s | Batch: 1211 | Train loss: 0.3309\n",
      "   % Time:  940s | Batch: 1221 | Train loss: 0.4071\n",
      "   % Time:  948s | Batch: 1231 | Train loss: 0.4002\n",
      "   % Time:  956s | Batch: 1241 | Train loss: 0.3407\n",
      "   % Time:  964s | Batch: 1251 | Train loss: 0.3867\n",
      "   % Time:  972s | Batch: 1261 | Train loss: 0.3732\n",
      "   % Time:  980s | Batch: 1271 | Train loss: 0.3646\n",
      "   % Time:  987s | Batch: 1281 | Train loss: 0.3843\n",
      "   % Time:  995s | Batch: 1291 | Train loss: 0.4298\n",
      "   % Time: 1003s | Batch: 1301 | Train loss: 0.3917\n",
      "   % Time: 1011s | Batch: 1311 | Train loss: 0.3217\n",
      "   % Time: 1019s | Batch: 1321 | Train loss: 0.3008\n",
      "   % Time: 1026s | Batch: 1331 | Train loss: 0.4266\n",
      "   % Time: 1034s | Batch: 1341 | Train loss: 0.3466\n",
      "   % Time: 1042s | Batch: 1351 | Train loss: 0.3422\n",
      "   % Time: 1050s | Batch: 1361 | Train loss: 0.3844\n",
      "   % Time: 1058s | Batch: 1371 | Train loss: 0.4396\n",
      "   % Time: 1066s | Batch: 1381 | Train loss: 0.3633\n",
      "   % Time: 1074s | Batch: 1391 | Train loss: 0.4058\n",
      "   % Time: 1082s | Batch: 1401 | Train loss: 0.4001\n",
      "   % Time: 1091s | Batch: 1411 | Train loss: 0.3468\n",
      "   % Time: 1099s | Batch: 1421 | Train loss: 0.3885\n",
      "   % Time: 1107s | Batch: 1431 | Train loss: 0.3602\n",
      "   % Time: 1115s | Batch: 1441 | Train loss: 0.3527\n",
      "   % Time: 1123s | Batch: 1451 | Train loss: 0.3909\n",
      "   % Time: 1131s | Batch: 1461 | Train loss: 0.4432\n",
      "   % Time: 1139s | Batch: 1471 | Train loss: 0.3828\n",
      "   % Time: 1147s | Batch: 1481 | Train loss: 0.4083\n",
      "   % Time: 1155s | Batch: 1491 | Train loss: 0.3718\n",
      "   % Time: 1163s | Batch: 1501 | Train loss: 0.3644\n",
      "   % Time: 1172s | Batch: 1511 | Train loss: 0.2935\n",
      "   % Time: 1180s | Batch: 1521 | Train loss: 0.3739\n",
      "   % Time: 1187s | Batch: 1531 | Train loss: 0.4157\n",
      "   % Time: 1195s | Batch: 1541 | Train loss: 0.3129\n",
      "   % Time: 1203s | Batch: 1551 | Train loss: 0.3519\n",
      "   % Time: 1211s | Batch: 1561 | Train loss: 0.3522\n",
      "   % Time: 1219s | Batch: 1571 | Train loss: 0.3224\n",
      "   % Time: 1227s | Batch: 1581 | Train loss: 0.3729\n",
      "   % Time: 1234s | Batch: 1591 | Train loss: 0.4055\n",
      "   % Time: 1242s | Batch: 1601 | Train loss: 0.3694\n",
      "   % Time: 1250s | Batch: 1611 | Train loss: 0.3459\n",
      "   % Time: 1258s | Batch: 1621 | Train loss: 0.3807\n",
      "   % Time: 1265s | Batch: 1631 | Train loss: 0.3967\n",
      "   % Time: 1273s | Batch: 1641 | Train loss: 0.3790\n",
      "   % Time: 1282s | Batch: 1651 | Train loss: 0.3474\n",
      "   % Time: 1290s | Batch: 1661 | Train loss: 0.3859\n",
      "   % Time: 1297s | Batch: 1671 | Train loss: 0.3081\n",
      "   % Time: 1305s | Batch: 1681 | Train loss: 0.3644\n",
      "   % Time: 1313s | Batch: 1691 | Train loss: 0.3377\n",
      "   % Time: 1321s | Batch: 1701 | Train loss: 0.3953\n",
      "   % Time: 1328s | Batch: 1711 | Train loss: 0.3532\n",
      "   % Time: 1336s | Batch: 1721 | Train loss: 0.3485\n",
      "   % Time: 1344s | Batch: 1731 | Train loss: 0.3249\n",
      "   % Time: 1352s | Batch: 1741 | Train loss: 0.3748\n",
      "   % Time: 1360s | Batch: 1751 | Train loss: 0.3791\n",
      "   % Time: 1367s | Batch: 1761 | Train loss: 0.4032\n",
      "   % Time: 1375s | Batch: 1771 | Train loss: 0.3606\n",
      "   % Time: 1383s | Batch: 1781 | Train loss: 0.4129\n",
      "   % Time: 1391s | Batch: 1791 | Train loss: 0.3904\n",
      "   % Time: 1398s | Batch: 1801 | Train loss: 0.2939\n",
      "   % Time: 1406s | Batch: 1811 | Train loss: 0.3834\n",
      "   % Time: 1414s | Batch: 1821 | Train loss: 0.3389\n",
      "   % Time: 1422s | Batch: 1831 | Train loss: 0.3772\n",
      "   % Time: 1430s | Batch: 1841 | Train loss: 0.3516\n",
      "   % Time: 1437s | Batch: 1851 | Train loss: 0.3839\n",
      "   % Time: 1445s | Batch: 1861 | Train loss: 0.3761\n",
      "   % Time: 1452s | Batch: 1871 | Train loss: 0.4146\n",
      "   % Time: 1460s | Batch: 1881 | Train loss: 0.3300\n",
      "   % Time: 1468s | Batch: 1891 | Train loss: 0.3854\n",
      "   % Time: 1476s | Batch: 1901 | Train loss: 0.3632\n",
      "   % Time: 1483s | Batch: 1911 | Train loss: 0.3638\n",
      "   % Time: 1491s | Batch: 1921 | Train loss: 0.4395\n",
      "   % Time: 1499s | Batch: 1931 | Train loss: 0.3275\n",
      "   % Time: 1507s | Batch: 1941 | Train loss: 0.4297\n",
      "   % Time: 1514s | Batch: 1951 | Train loss: 0.3220\n",
      "   % Time: 1522s | Batch: 1961 | Train loss: 0.4279\n",
      "   % Time: 1530s | Batch: 1971 | Train loss: 0.3377\n",
      "   % Time: 1538s | Batch: 1981 | Train loss: 0.3438\n",
      "   % Time: 1545s | Batch: 1991 | Train loss: 0.4205\n",
      "   % Time: 1553s | Batch: 2001 | Train loss: 0.3964\n",
      "   % Time: 1561s | Batch: 2011 | Train loss: 0.3737\n",
      "   % Time: 1569s | Batch: 2021 | Train loss: 0.3312\n",
      "   % Time: 1576s | Batch: 2031 | Train loss: 0.3263\n",
      "   % Time: 1584s | Batch: 2041 | Train loss: 0.3978\n",
      "   % Time: 1592s | Batch: 2051 | Train loss: 0.3435\n",
      "   % Time: 1600s | Batch: 2061 | Train loss: 0.3481\n",
      "   % Time: 1608s | Batch: 2071 | Train loss: 0.3702\n",
      "   % Time: 1615s | Batch: 2081 | Train loss: 0.3551\n",
      "   % Time: 1623s | Batch: 2091 | Train loss: 0.4002\n",
      "   % Time: 1631s | Batch: 2101 | Train loss: 0.3498\n",
      "   % Time: 1638s | Batch: 2111 | Train loss: 0.3814\n",
      "   % Time: 1646s | Batch: 2121 | Train loss: 0.3718\n",
      "   % Time: 1654s | Batch: 2131 | Train loss: 0.3494\n",
      "   % Time: 1662s | Batch: 2141 | Train loss: 0.3630\n",
      "   % Time: 1669s | Batch: 2151 | Train loss: 0.3837\n",
      "   % Time: 1677s | Batch: 2161 | Train loss: 0.3548\n",
      "   % Time: 1685s | Batch: 2171 | Train loss: 0.3169\n",
      "   % Time: 1693s | Batch: 2181 | Train loss: 0.3996\n",
      "   % Time: 1701s | Batch: 2191 | Train loss: 0.4066\n",
      "   % Time: 1708s | Batch: 2201 | Train loss: 0.3709\n",
      "   % Time: 1716s | Batch: 2211 | Train loss: 0.3438\n",
      "   % Time: 1724s | Batch: 2221 | Train loss: 0.3589\n",
      "   % Time: 1731s | Batch: 2231 | Train loss: 0.3716\n",
      "   % Time: 1739s | Batch: 2241 | Train loss: 0.3461\n",
      "   % Time: 1747s | Batch: 2251 | Train loss: 0.3772\n",
      "   % Time: 1754s | Batch: 2261 | Train loss: 0.3675\n",
      "==========\n",
      "   % Epoch: 4 | Time: 1759s | Train loss: 0.3738 | Val loss: 26.5779\n",
      "==========\n",
      "=> EPOCH 5 with lr 0.0001\n",
      "   % Time:    1s | Batch:    1 | Train loss: 0.3998\n",
      "   % Time:    9s | Batch:   11 | Train loss: 0.3296\n",
      "   % Time:   16s | Batch:   21 | Train loss: 0.3656\n",
      "   % Time:   24s | Batch:   31 | Train loss: 0.3892\n",
      "   % Time:   32s | Batch:   41 | Train loss: 0.3402\n",
      "   % Time:   40s | Batch:   51 | Train loss: 0.3595\n",
      "   % Time:   48s | Batch:   61 | Train loss: 0.3655\n",
      "   % Time:   55s | Batch:   71 | Train loss: 0.4117\n",
      "   % Time:   63s | Batch:   81 | Train loss: 0.3254\n",
      "   % Time:   71s | Batch:   91 | Train loss: 0.3928\n",
      "   % Time:   78s | Batch:  101 | Train loss: 0.3714\n",
      "   % Time:   86s | Batch:  111 | Train loss: 0.3754\n",
      "   % Time:   94s | Batch:  121 | Train loss: 0.3968\n",
      "   % Time:  102s | Batch:  131 | Train loss: 0.3647\n",
      "   % Time:  110s | Batch:  141 | Train loss: 0.3749\n",
      "   % Time:  117s | Batch:  151 | Train loss: 0.4053\n",
      "   % Time:  125s | Batch:  161 | Train loss: 0.4357\n",
      "   % Time:  133s | Batch:  171 | Train loss: 0.3911\n",
      "   % Time:  141s | Batch:  181 | Train loss: 0.3484\n",
      "   % Time:  148s | Batch:  191 | Train loss: 0.3408\n",
      "   % Time:  156s | Batch:  201 | Train loss: 0.3768\n",
      "   % Time:  164s | Batch:  211 | Train loss: 0.3799\n",
      "   % Time:  172s | Batch:  221 | Train loss: 0.3148\n",
      "   % Time:  180s | Batch:  231 | Train loss: 0.3505\n",
      "   % Time:  188s | Batch:  241 | Train loss: 0.3735\n",
      "   % Time:  196s | Batch:  251 | Train loss: 0.3658\n",
      "   % Time:  203s | Batch:  261 | Train loss: 0.4095\n",
      "   % Time:  211s | Batch:  271 | Train loss: 0.3624\n",
      "   % Time:  219s | Batch:  281 | Train loss: 0.3719\n",
      "   % Time:  227s | Batch:  291 | Train loss: 0.4101\n",
      "   % Time:  235s | Batch:  301 | Train loss: 0.4304\n",
      "   % Time:  243s | Batch:  311 | Train loss: 0.4066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   % Time:  251s | Batch:  321 | Train loss: 0.3903\n",
      "   % Time:  258s | Batch:  331 | Train loss: 0.3988\n",
      "   % Time:  266s | Batch:  341 | Train loss: 0.3731\n",
      "   % Time:  274s | Batch:  351 | Train loss: 0.3640\n",
      "   % Time:  281s | Batch:  361 | Train loss: 0.3889\n",
      "   % Time:  289s | Batch:  371 | Train loss: 0.3474\n",
      "   % Time:  297s | Batch:  381 | Train loss: 0.3876\n",
      "   % Time:  304s | Batch:  391 | Train loss: 0.3685\n",
      "   % Time:  312s | Batch:  401 | Train loss: 0.3918\n",
      "   % Time:  320s | Batch:  411 | Train loss: 0.3810\n",
      "   % Time:  327s | Batch:  421 | Train loss: 0.4009\n",
      "   % Time:  335s | Batch:  431 | Train loss: 0.3767\n",
      "   % Time:  343s | Batch:  441 | Train loss: 0.3567\n",
      "   % Time:  350s | Batch:  451 | Train loss: 0.4135\n",
      "   % Time:  359s | Batch:  461 | Train loss: 0.4067\n",
      "   % Time:  367s | Batch:  471 | Train loss: 0.4197\n",
      "   % Time:  375s | Batch:  481 | Train loss: 0.3328\n",
      "   % Time:  382s | Batch:  491 | Train loss: 0.3981\n",
      "   % Time:  390s | Batch:  501 | Train loss: 0.3920\n",
      "   % Time:  398s | Batch:  511 | Train loss: 0.3732\n",
      "   % Time:  405s | Batch:  521 | Train loss: 0.3504\n",
      "   % Time:  413s | Batch:  531 | Train loss: 0.3254\n",
      "   % Time:  420s | Batch:  541 | Train loss: 0.3602\n",
      "   % Time:  428s | Batch:  551 | Train loss: 0.4013\n",
      "   % Time:  436s | Batch:  561 | Train loss: 0.3789\n",
      "   % Time:  443s | Batch:  571 | Train loss: 0.3768\n",
      "   % Time:  451s | Batch:  581 | Train loss: 0.4160\n",
      "   % Time:  459s | Batch:  591 | Train loss: 0.3623\n",
      "   % Time:  466s | Batch:  601 | Train loss: 0.3679\n",
      "   % Time:  474s | Batch:  611 | Train loss: 0.3548\n",
      "   % Time:  482s | Batch:  621 | Train loss: 0.4193\n",
      "   % Time:  489s | Batch:  631 | Train loss: 0.3604\n",
      "   % Time:  497s | Batch:  641 | Train loss: 0.3932\n",
      "   % Time:  505s | Batch:  651 | Train loss: 0.3366\n",
      "   % Time:  512s | Batch:  661 | Train loss: 0.4039\n",
      "   % Time:  520s | Batch:  671 | Train loss: 0.4119\n",
      "   % Time:  528s | Batch:  681 | Train loss: 0.4031\n",
      "   % Time:  535s | Batch:  691 | Train loss: 0.3398\n",
      "   % Time:  543s | Batch:  701 | Train loss: 0.3576\n",
      "   % Time:  551s | Batch:  711 | Train loss: 0.3837\n",
      "   % Time:  558s | Batch:  721 | Train loss: 0.3586\n",
      "   % Time:  566s | Batch:  731 | Train loss: 0.4066\n",
      "   % Time:  574s | Batch:  741 | Train loss: 0.3786\n",
      "   % Time:  581s | Batch:  751 | Train loss: 0.3746\n",
      "   % Time:  589s | Batch:  761 | Train loss: 0.3579\n",
      "   % Time:  597s | Batch:  771 | Train loss: 0.4038\n",
      "   % Time:  604s | Batch:  781 | Train loss: 0.3596\n",
      "   % Time:  612s | Batch:  791 | Train loss: 0.3246\n",
      "   % Time:  620s | Batch:  801 | Train loss: 0.3996\n",
      "   % Time:  627s | Batch:  811 | Train loss: 0.3940\n",
      "   % Time:  635s | Batch:  821 | Train loss: 0.3459\n",
      "   % Time:  643s | Batch:  831 | Train loss: 0.3773\n",
      "   % Time:  650s | Batch:  841 | Train loss: 0.3649\n",
      "   % Time:  658s | Batch:  851 | Train loss: 0.4116\n",
      "   % Time:  666s | Batch:  861 | Train loss: 0.3532\n",
      "   % Time:  673s | Batch:  871 | Train loss: 0.3653\n",
      "   % Time:  681s | Batch:  881 | Train loss: 0.3577\n",
      "   % Time:  689s | Batch:  891 | Train loss: 0.3772\n",
      "   % Time:  697s | Batch:  901 | Train loss: 0.3626\n",
      "   % Time:  704s | Batch:  911 | Train loss: 0.3312\n",
      "   % Time:  712s | Batch:  921 | Train loss: 0.3698\n",
      "   % Time:  720s | Batch:  931 | Train loss: 0.3179\n",
      "   % Time:  727s | Batch:  941 | Train loss: 0.3489\n",
      "   % Time:  735s | Batch:  951 | Train loss: 0.3906\n",
      "   % Time:  743s | Batch:  961 | Train loss: 0.3356\n",
      "   % Time:  750s | Batch:  971 | Train loss: 0.3338\n",
      "   % Time:  758s | Batch:  981 | Train loss: 0.3473\n",
      "   % Time:  766s | Batch:  991 | Train loss: 0.3497\n",
      "   % Time:  773s | Batch: 1001 | Train loss: 0.4123\n",
      "   % Time:  781s | Batch: 1011 | Train loss: 0.3524\n",
      "   % Time:  788s | Batch: 1021 | Train loss: 0.3671\n",
      "   % Time:  796s | Batch: 1031 | Train loss: 0.3661\n",
      "   % Time:  804s | Batch: 1041 | Train loss: 0.3540\n",
      "   % Time:  811s | Batch: 1051 | Train loss: 0.3954\n",
      "   % Time:  819s | Batch: 1061 | Train loss: 0.3478\n",
      "   % Time:  827s | Batch: 1071 | Train loss: 0.3932\n",
      "   % Time:  834s | Batch: 1081 | Train loss: 0.4105\n",
      "   % Time:  842s | Batch: 1091 | Train loss: 0.3624\n",
      "   % Time:  849s | Batch: 1101 | Train loss: 0.3560\n",
      "   % Time:  857s | Batch: 1111 | Train loss: 0.3798\n",
      "   % Time:  865s | Batch: 1121 | Train loss: 0.4052\n",
      "   % Time:  872s | Batch: 1131 | Train loss: 0.3686\n",
      "   % Time:  880s | Batch: 1141 | Train loss: 0.4160\n",
      "   % Time:  887s | Batch: 1151 | Train loss: 0.3388\n",
      "   % Time:  895s | Batch: 1161 | Train loss: 0.3659\n",
      "   % Time:  903s | Batch: 1171 | Train loss: 0.4163\n",
      "   % Time:  910s | Batch: 1181 | Train loss: 0.3628\n",
      "   % Time:  918s | Batch: 1191 | Train loss: 0.3939\n",
      "   % Time:  926s | Batch: 1201 | Train loss: 0.3775\n",
      "   % Time:  933s | Batch: 1211 | Train loss: 0.3325\n",
      "   % Time:  941s | Batch: 1221 | Train loss: 0.3732\n",
      "   % Time:  948s | Batch: 1231 | Train loss: 0.3698\n",
      "   % Time:  956s | Batch: 1241 | Train loss: 0.3739\n",
      "   % Time:  964s | Batch: 1251 | Train loss: 0.3387\n",
      "   % Time:  971s | Batch: 1261 | Train loss: 0.3243\n",
      "   % Time:  979s | Batch: 1271 | Train loss: 0.3317\n",
      "   % Time:  987s | Batch: 1281 | Train loss: 0.4002\n",
      "   % Time:  994s | Batch: 1291 | Train loss: 0.3635\n",
      "   % Time: 1002s | Batch: 1301 | Train loss: 0.3716\n",
      "   % Time: 1009s | Batch: 1311 | Train loss: 0.4047\n",
      "   % Time: 1017s | Batch: 1321 | Train loss: 0.4254\n",
      "   % Time: 1025s | Batch: 1331 | Train loss: 0.3901\n",
      "   % Time: 1032s | Batch: 1341 | Train loss: 0.3831\n",
      "   % Time: 1040s | Batch: 1351 | Train loss: 0.3716\n",
      "   % Time: 1048s | Batch: 1361 | Train loss: 0.3788\n",
      "   % Time: 1055s | Batch: 1371 | Train loss: 0.4265\n",
      "   % Time: 1063s | Batch: 1381 | Train loss: 0.3825\n",
      "   % Time: 1071s | Batch: 1391 | Train loss: 0.3672\n",
      "   % Time: 1078s | Batch: 1401 | Train loss: 0.3511\n",
      "   % Time: 1086s | Batch: 1411 | Train loss: 0.3660\n",
      "   % Time: 1094s | Batch: 1421 | Train loss: 0.3426\n",
      "   % Time: 1101s | Batch: 1431 | Train loss: 0.3643\n",
      "   % Time: 1109s | Batch: 1441 | Train loss: 0.4014\n",
      "   % Time: 1117s | Batch: 1451 | Train loss: 0.4137\n",
      "   % Time: 1124s | Batch: 1461 | Train loss: 0.3685\n",
      "   % Time: 1132s | Batch: 1471 | Train loss: 0.4001\n",
      "   % Time: 1139s | Batch: 1481 | Train loss: 0.3946\n",
      "   % Time: 1147s | Batch: 1491 | Train loss: 0.3611\n",
      "   % Time: 1155s | Batch: 1501 | Train loss: 0.3873\n",
      "   % Time: 1162s | Batch: 1511 | Train loss: 0.3460\n",
      "   % Time: 1170s | Batch: 1521 | Train loss: 0.3423\n",
      "   % Time: 1178s | Batch: 1531 | Train loss: 0.3934\n",
      "   % Time: 1185s | Batch: 1541 | Train loss: 0.3022\n",
      "   % Time: 1193s | Batch: 1551 | Train loss: 0.3402\n",
      "   % Time: 1201s | Batch: 1561 | Train loss: 0.3553\n",
      "   % Time: 1208s | Batch: 1571 | Train loss: 0.3405\n",
      "   % Time: 1216s | Batch: 1581 | Train loss: 0.4290\n",
      "   % Time: 1224s | Batch: 1591 | Train loss: 0.3831\n",
      "   % Time: 1231s | Batch: 1601 | Train loss: 0.3839\n",
      "   % Time: 1239s | Batch: 1611 | Train loss: 0.3527\n",
      "   % Time: 1246s | Batch: 1621 | Train loss: 0.3774\n",
      "   % Time: 1254s | Batch: 1631 | Train loss: 0.3819\n",
      "   % Time: 1262s | Batch: 1641 | Train loss: 0.3879\n",
      "   % Time: 1269s | Batch: 1651 | Train loss: 0.3725\n",
      "   % Time: 1277s | Batch: 1661 | Train loss: 0.3933\n",
      "   % Time: 1285s | Batch: 1671 | Train loss: 0.3960\n",
      "   % Time: 1292s | Batch: 1681 | Train loss: 0.3750\n",
      "   % Time: 1300s | Batch: 1691 | Train loss: 0.3666\n",
      "   % Time: 1307s | Batch: 1701 | Train loss: 0.3354\n",
      "   % Time: 1315s | Batch: 1711 | Train loss: 0.3727\n",
      "   % Time: 1323s | Batch: 1721 | Train loss: 0.3752\n",
      "   % Time: 1330s | Batch: 1731 | Train loss: 0.3403\n",
      "   % Time: 1338s | Batch: 1741 | Train loss: 0.3713\n",
      "   % Time: 1346s | Batch: 1751 | Train loss: 0.3675\n",
      "   % Time: 1353s | Batch: 1761 | Train loss: 0.3933\n",
      "   % Time: 1361s | Batch: 1771 | Train loss: 0.3125\n",
      "   % Time: 1369s | Batch: 1781 | Train loss: 0.3958\n",
      "   % Time: 1376s | Batch: 1791 | Train loss: 0.3635\n",
      "   % Time: 1384s | Batch: 1801 | Train loss: 0.3976\n",
      "   % Time: 1392s | Batch: 1811 | Train loss: 0.3406\n",
      "   % Time: 1399s | Batch: 1821 | Train loss: 0.3568\n",
      "   % Time: 1407s | Batch: 1831 | Train loss: 0.3362\n",
      "   % Time: 1415s | Batch: 1841 | Train loss: 0.3734\n",
      "   % Time: 1422s | Batch: 1851 | Train loss: 0.3812\n",
      "   % Time: 1430s | Batch: 1861 | Train loss: 0.3613\n",
      "   % Time: 1437s | Batch: 1871 | Train loss: 0.3424\n",
      "   % Time: 1445s | Batch: 1881 | Train loss: 0.3899\n",
      "   % Time: 1453s | Batch: 1891 | Train loss: 0.3538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   % Time: 1460s | Batch: 1901 | Train loss: 0.4014\n",
      "   % Time: 1468s | Batch: 1911 | Train loss: 0.4233\n",
      "   % Time: 1476s | Batch: 1921 | Train loss: 0.3907\n",
      "   % Time: 1483s | Batch: 1931 | Train loss: 0.4052\n",
      "   % Time: 1491s | Batch: 1941 | Train loss: 0.4062\n",
      "   % Time: 1499s | Batch: 1951 | Train loss: 0.3176\n",
      "   % Time: 1506s | Batch: 1961 | Train loss: 0.3664\n",
      "   % Time: 1514s | Batch: 1971 | Train loss: 0.3542\n",
      "   % Time: 1521s | Batch: 1981 | Train loss: 0.3849\n",
      "   % Time: 1529s | Batch: 1991 | Train loss: 0.3617\n",
      "   % Time: 1537s | Batch: 2001 | Train loss: 0.3583\n",
      "   % Time: 1544s | Batch: 2011 | Train loss: 0.3381\n",
      "   % Time: 1552s | Batch: 2021 | Train loss: 0.3645\n",
      "   % Time: 1560s | Batch: 2031 | Train loss: 0.3804\n",
      "   % Time: 1567s | Batch: 2041 | Train loss: 0.4283\n",
      "   % Time: 1575s | Batch: 2051 | Train loss: 0.3823\n",
      "   % Time: 1583s | Batch: 2061 | Train loss: 0.3322\n",
      "   % Time: 1590s | Batch: 2071 | Train loss: 0.3426\n",
      "   % Time: 1598s | Batch: 2081 | Train loss: 0.3663\n",
      "   % Time: 1605s | Batch: 2091 | Train loss: 0.3380\n",
      "   % Time: 1613s | Batch: 2101 | Train loss: 0.3827\n",
      "   % Time: 1621s | Batch: 2111 | Train loss: 0.4003\n",
      "   % Time: 1628s | Batch: 2121 | Train loss: 0.3472\n",
      "   % Time: 1636s | Batch: 2131 | Train loss: 0.3619\n",
      "   % Time: 1643s | Batch: 2141 | Train loss: 0.3945\n",
      "   % Time: 1651s | Batch: 2151 | Train loss: 0.3511\n",
      "   % Time: 1659s | Batch: 2161 | Train loss: 0.3748\n",
      "   % Time: 1666s | Batch: 2171 | Train loss: 0.4192\n",
      "   % Time: 1674s | Batch: 2181 | Train loss: 0.3484\n",
      "   % Time: 1681s | Batch: 2191 | Train loss: 0.4037\n",
      "   % Time: 1689s | Batch: 2201 | Train loss: 0.3576\n",
      "   % Time: 1697s | Batch: 2211 | Train loss: 0.3602\n",
      "   % Time: 1704s | Batch: 2221 | Train loss: 0.3149\n",
      "   % Time: 1712s | Batch: 2231 | Train loss: 0.3800\n",
      "   % Time: 1720s | Batch: 2241 | Train loss: 0.3595\n",
      "   % Time: 1727s | Batch: 2251 | Train loss: 0.3747\n",
      "   % Time: 1735s | Batch: 2261 | Train loss: 0.3863\n",
      "==========\n",
      "   % Epoch: 5 | Time: 1740s | Train loss: 0.3720 | Val loss: 26.5083\n",
      "==========\n",
      "=> EPOCH 6 with lr 0.0001\n",
      "   % Time:    1s | Batch:    1 | Train loss: 0.4571\n",
      "   % Time:    8s | Batch:   11 | Train loss: 0.3639\n",
      "   % Time:   16s | Batch:   21 | Train loss: 0.3698\n",
      "   % Time:   23s | Batch:   31 | Train loss: 0.4046\n",
      "   % Time:   31s | Batch:   41 | Train loss: 0.4112\n",
      "   % Time:   39s | Batch:   51 | Train loss: 0.3788\n",
      "   % Time:   46s | Batch:   61 | Train loss: 0.3658\n",
      "   % Time:   54s | Batch:   71 | Train loss: 0.3193\n",
      "   % Time:   61s | Batch:   81 | Train loss: 0.3451\n",
      "   % Time:   69s | Batch:   91 | Train loss: 0.3482\n",
      "   % Time:   77s | Batch:  101 | Train loss: 0.3996\n",
      "   % Time:   84s | Batch:  111 | Train loss: 0.3478\n",
      "   % Time:   92s | Batch:  121 | Train loss: 0.3801\n",
      "   % Time:   99s | Batch:  131 | Train loss: 0.3146\n",
      "   % Time:  107s | Batch:  141 | Train loss: 0.3408\n",
      "   % Time:  115s | Batch:  151 | Train loss: 0.3380\n",
      "   % Time:  122s | Batch:  161 | Train loss: 0.3648\n",
      "   % Time:  130s | Batch:  171 | Train loss: 0.3755\n",
      "   % Time:  137s | Batch:  181 | Train loss: 0.3228\n",
      "   % Time:  145s | Batch:  191 | Train loss: 0.3780\n",
      "   % Time:  153s | Batch:  201 | Train loss: 0.3543\n",
      "   % Time:  160s | Batch:  211 | Train loss: 0.3640\n",
      "   % Time:  168s | Batch:  221 | Train loss: 0.3813\n",
      "   % Time:  175s | Batch:  231 | Train loss: 0.3674\n",
      "   % Time:  183s | Batch:  241 | Train loss: 0.4171\n",
      "   % Time:  191s | Batch:  251 | Train loss: 0.3865\n",
      "   % Time:  198s | Batch:  261 | Train loss: 0.3972\n",
      "   % Time:  206s | Batch:  271 | Train loss: 0.3298\n",
      "   % Time:  213s | Batch:  281 | Train loss: 0.3599\n",
      "   % Time:  221s | Batch:  291 | Train loss: 0.3416\n",
      "   % Time:  229s | Batch:  301 | Train loss: 0.4123\n",
      "   % Time:  236s | Batch:  311 | Train loss: 0.3857\n",
      "   % Time:  244s | Batch:  321 | Train loss: 0.3719\n",
      "   % Time:  251s | Batch:  331 | Train loss: 0.3672\n",
      "   % Time:  259s | Batch:  341 | Train loss: 0.3895\n",
      "   % Time:  267s | Batch:  351 | Train loss: 0.4192\n",
      "   % Time:  274s | Batch:  361 | Train loss: 0.3825\n",
      "   % Time:  282s | Batch:  371 | Train loss: 0.4309\n",
      "   % Time:  289s | Batch:  381 | Train loss: 0.3602\n",
      "   % Time:  297s | Batch:  391 | Train loss: 0.4076\n",
      "   % Time:  305s | Batch:  401 | Train loss: 0.4484\n",
      "   % Time:  312s | Batch:  411 | Train loss: 0.3534\n",
      "   % Time:  320s | Batch:  421 | Train loss: 0.3595\n",
      "   % Time:  328s | Batch:  431 | Train loss: 0.4046\n",
      "   % Time:  335s | Batch:  441 | Train loss: 0.3697\n",
      "   % Time:  343s | Batch:  451 | Train loss: 0.3699\n",
      "   % Time:  351s | Batch:  461 | Train loss: 0.3965\n",
      "   % Time:  358s | Batch:  471 | Train loss: 0.3579\n",
      "   % Time:  366s | Batch:  481 | Train loss: 0.3874\n",
      "   % Time:  373s | Batch:  491 | Train loss: 0.4220\n",
      "   % Time:  381s | Batch:  501 | Train loss: 0.3815\n",
      "   % Time:  389s | Batch:  511 | Train loss: 0.4267\n",
      "   % Time:  396s | Batch:  521 | Train loss: 0.3994\n",
      "   % Time:  404s | Batch:  531 | Train loss: 0.3846\n",
      "   % Time:  412s | Batch:  541 | Train loss: 0.3476\n",
      "   % Time:  419s | Batch:  551 | Train loss: 0.4391\n",
      "   % Time:  427s | Batch:  561 | Train loss: 0.3315\n",
      "   % Time:  435s | Batch:  571 | Train loss: 0.3575\n",
      "   % Time:  442s | Batch:  581 | Train loss: 0.3987\n",
      "   % Time:  450s | Batch:  591 | Train loss: 0.3342\n",
      "   % Time:  458s | Batch:  601 | Train loss: 0.3428\n",
      "   % Time:  465s | Batch:  611 | Train loss: 0.3391\n",
      "   % Time:  473s | Batch:  621 | Train loss: 0.3602\n",
      "   % Time:  481s | Batch:  631 | Train loss: 0.3763\n",
      "   % Time:  488s | Batch:  641 | Train loss: 0.3585\n",
      "   % Time:  496s | Batch:  651 | Train loss: 0.3791\n",
      "   % Time:  504s | Batch:  661 | Train loss: 0.3922\n",
      "   % Time:  511s | Batch:  671 | Train loss: 0.4265\n",
      "   % Time:  519s | Batch:  681 | Train loss: 0.4055\n",
      "   % Time:  527s | Batch:  691 | Train loss: 0.3959\n",
      "   % Time:  534s | Batch:  701 | Train loss: 0.3618\n",
      "   % Time:  542s | Batch:  711 | Train loss: 0.3730\n",
      "   % Time:  550s | Batch:  721 | Train loss: 0.3449\n",
      "   % Time:  557s | Batch:  731 | Train loss: 0.3880\n",
      "   % Time:  565s | Batch:  741 | Train loss: 0.4001\n",
      "   % Time:  572s | Batch:  751 | Train loss: 0.3708\n",
      "   % Time:  580s | Batch:  761 | Train loss: 0.3948\n",
      "   % Time:  588s | Batch:  771 | Train loss: 0.3422\n",
      "   % Time:  595s | Batch:  781 | Train loss: 0.3786\n",
      "   % Time:  603s | Batch:  791 | Train loss: 0.3443\n",
      "   % Time:  611s | Batch:  801 | Train loss: 0.3776\n",
      "   % Time:  618s | Batch:  811 | Train loss: 0.3722\n",
      "   % Time:  626s | Batch:  821 | Train loss: 0.3311\n",
      "   % Time:  634s | Batch:  831 | Train loss: 0.3609\n",
      "   % Time:  642s | Batch:  841 | Train loss: 0.3687\n",
      "   % Time:  649s | Batch:  851 | Train loss: 0.3354\n",
      "   % Time:  657s | Batch:  861 | Train loss: 0.3692\n",
      "   % Time:  664s | Batch:  871 | Train loss: 0.3140\n",
      "   % Time:  672s | Batch:  881 | Train loss: 0.3209\n",
      "   % Time:  680s | Batch:  891 | Train loss: 0.3260\n",
      "   % Time:  687s | Batch:  901 | Train loss: 0.3763\n",
      "   % Time:  695s | Batch:  911 | Train loss: 0.3420\n",
      "   % Time:  703s | Batch:  921 | Train loss: 0.3882\n",
      "   % Time:  710s | Batch:  931 | Train loss: 0.3973\n",
      "   % Time:  718s | Batch:  941 | Train loss: 0.3886\n",
      "   % Time:  725s | Batch:  951 | Train loss: 0.3580\n",
      "   % Time:  733s | Batch:  961 | Train loss: 0.2965\n",
      "   % Time:  740s | Batch:  971 | Train loss: 0.3965\n",
      "   % Time:  748s | Batch:  981 | Train loss: 0.3966\n",
      "   % Time:  756s | Batch:  991 | Train loss: 0.3705\n",
      "   % Time:  763s | Batch: 1001 | Train loss: 0.3581\n",
      "   % Time:  771s | Batch: 1011 | Train loss: 0.3621\n",
      "   % Time:  778s | Batch: 1021 | Train loss: 0.3787\n",
      "   % Time:  786s | Batch: 1031 | Train loss: 0.3896\n",
      "   % Time:  793s | Batch: 1041 | Train loss: 0.3759\n",
      "   % Time:  801s | Batch: 1051 | Train loss: 0.3228\n",
      "   % Time:  809s | Batch: 1061 | Train loss: 0.4090\n",
      "   % Time:  816s | Batch: 1071 | Train loss: 0.3686\n",
      "   % Time:  824s | Batch: 1081 | Train loss: 0.4056\n",
      "   % Time:  832s | Batch: 1091 | Train loss: 0.4114\n",
      "   % Time:  839s | Batch: 1101 | Train loss: 0.3499\n",
      "   % Time:  847s | Batch: 1111 | Train loss: 0.3148\n",
      "   % Time:  855s | Batch: 1121 | Train loss: 0.3734\n",
      "   % Time:  862s | Batch: 1131 | Train loss: 0.3651\n",
      "   % Time:  870s | Batch: 1141 | Train loss: 0.3474\n",
      "   % Time:  877s | Batch: 1151 | Train loss: 0.3607\n",
      "   % Time:  885s | Batch: 1161 | Train loss: 0.3336\n",
      "   % Time:  893s | Batch: 1171 | Train loss: 0.3826\n",
      "   % Time:  900s | Batch: 1181 | Train loss: 0.3184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   % Time:  908s | Batch: 1191 | Train loss: 0.3284\n",
      "   % Time:  916s | Batch: 1201 | Train loss: 0.4092\n",
      "   % Time:  923s | Batch: 1211 | Train loss: 0.3933\n",
      "   % Time:  931s | Batch: 1221 | Train loss: 0.4096\n",
      "   % Time:  939s | Batch: 1231 | Train loss: 0.3272\n",
      "   % Time:  946s | Batch: 1241 | Train loss: 0.3274\n",
      "   % Time:  954s | Batch: 1251 | Train loss: 0.4061\n",
      "   % Time:  962s | Batch: 1261 | Train loss: 0.3778\n",
      "   % Time:  969s | Batch: 1271 | Train loss: 0.4012\n",
      "   % Time:  977s | Batch: 1281 | Train loss: 0.4063\n",
      "   % Time:  984s | Batch: 1291 | Train loss: 0.3485\n",
      "   % Time:  992s | Batch: 1301 | Train loss: 0.3892\n",
      "   % Time: 1000s | Batch: 1311 | Train loss: 0.4077\n",
      "   % Time: 1007s | Batch: 1321 | Train loss: 0.3635\n",
      "   % Time: 1015s | Batch: 1331 | Train loss: 0.3745\n",
      "   % Time: 1023s | Batch: 1341 | Train loss: 0.3830\n",
      "   % Time: 1030s | Batch: 1351 | Train loss: 0.3694\n",
      "   % Time: 1038s | Batch: 1361 | Train loss: 0.3176\n",
      "   % Time: 1046s | Batch: 1371 | Train loss: 0.4014\n",
      "   % Time: 1053s | Batch: 1381 | Train loss: 0.4123\n",
      "   % Time: 1061s | Batch: 1391 | Train loss: 0.4325\n",
      "   % Time: 1069s | Batch: 1401 | Train loss: 0.3841\n",
      "   % Time: 1076s | Batch: 1411 | Train loss: 0.3235\n",
      "   % Time: 1084s | Batch: 1421 | Train loss: 0.3408\n",
      "   % Time: 1092s | Batch: 1431 | Train loss: 0.3766\n",
      "   % Time: 1099s | Batch: 1441 | Train loss: 0.3523\n",
      "   % Time: 1107s | Batch: 1451 | Train loss: 0.3533\n",
      "   % Time: 1115s | Batch: 1461 | Train loss: 0.3893\n",
      "   % Time: 1122s | Batch: 1471 | Train loss: 0.3384\n",
      "   % Time: 1130s | Batch: 1481 | Train loss: 0.3948\n",
      "   % Time: 1137s | Batch: 1491 | Train loss: 0.3356\n",
      "   % Time: 1145s | Batch: 1501 | Train loss: 0.3643\n",
      "   % Time: 1153s | Batch: 1511 | Train loss: 0.4047\n",
      "   % Time: 1160s | Batch: 1521 | Train loss: 0.3437\n",
      "   % Time: 1168s | Batch: 1531 | Train loss: 0.3723\n",
      "   % Time: 1176s | Batch: 1541 | Train loss: 0.3613\n",
      "   % Time: 1183s | Batch: 1551 | Train loss: 0.3131\n",
      "   % Time: 1191s | Batch: 1561 | Train loss: 0.4296\n",
      "   % Time: 1199s | Batch: 1571 | Train loss: 0.4187\n",
      "   % Time: 1206s | Batch: 1581 | Train loss: 0.3387\n",
      "   % Time: 1214s | Batch: 1591 | Train loss: 0.3293\n",
      "   % Time: 1222s | Batch: 1601 | Train loss: 0.3877\n",
      "   % Time: 1229s | Batch: 1611 | Train loss: 0.3824\n",
      "   % Time: 1237s | Batch: 1621 | Train loss: 0.3800\n",
      "   % Time: 1245s | Batch: 1631 | Train loss: 0.3591\n",
      "   % Time: 1252s | Batch: 1641 | Train loss: 0.3584\n",
      "   % Time: 1260s | Batch: 1651 | Train loss: 0.3860\n",
      "   % Time: 1267s | Batch: 1661 | Train loss: 0.3597\n",
      "   % Time: 1275s | Batch: 1671 | Train loss: 0.4108\n",
      "   % Time: 1283s | Batch: 1681 | Train loss: 0.3933\n",
      "   % Time: 1290s | Batch: 1691 | Train loss: 0.3841\n",
      "   % Time: 1298s | Batch: 1701 | Train loss: 0.3745\n",
      "   % Time: 1306s | Batch: 1711 | Train loss: 0.3848\n",
      "   % Time: 1313s | Batch: 1721 | Train loss: 0.3738\n",
      "   % Time: 1321s | Batch: 1731 | Train loss: 0.3967\n",
      "   % Time: 1329s | Batch: 1741 | Train loss: 0.3855\n",
      "   % Time: 1336s | Batch: 1751 | Train loss: 0.3974\n",
      "   % Time: 1344s | Batch: 1761 | Train loss: 0.3911\n",
      "   % Time: 1352s | Batch: 1771 | Train loss: 0.3649\n",
      "   % Time: 1359s | Batch: 1781 | Train loss: 0.4444\n",
      "   % Time: 1367s | Batch: 1791 | Train loss: 0.4042\n",
      "   % Time: 1375s | Batch: 1801 | Train loss: 0.3825\n",
      "   % Time: 1382s | Batch: 1811 | Train loss: 0.3875\n",
      "   % Time: 1390s | Batch: 1821 | Train loss: 0.3539\n",
      "   % Time: 1398s | Batch: 1831 | Train loss: 0.3362\n",
      "   % Time: 1405s | Batch: 1841 | Train loss: 0.3723\n",
      "   % Time: 1413s | Batch: 1851 | Train loss: 0.4028\n",
      "   % Time: 1421s | Batch: 1861 | Train loss: 0.3607\n",
      "   % Time: 1428s | Batch: 1871 | Train loss: 0.3486\n",
      "   % Time: 1436s | Batch: 1881 | Train loss: 0.2953\n",
      "   % Time: 1444s | Batch: 1891 | Train loss: 0.3081\n",
      "   % Time: 1451s | Batch: 1901 | Train loss: 0.3722\n",
      "   % Time: 1459s | Batch: 1911 | Train loss: 0.3792\n",
      "   % Time: 1467s | Batch: 1921 | Train loss: 0.3405\n",
      "   % Time: 1474s | Batch: 1931 | Train loss: 0.3853\n",
      "   % Time: 1482s | Batch: 1941 | Train loss: 0.4165\n",
      "   % Time: 1489s | Batch: 1951 | Train loss: 0.3658\n",
      "   % Time: 1497s | Batch: 1961 | Train loss: 0.4304\n",
      "   % Time: 1505s | Batch: 1971 | Train loss: 0.3847\n",
      "   % Time: 1513s | Batch: 1981 | Train loss: 0.3587\n",
      "   % Time: 1520s | Batch: 1991 | Train loss: 0.4182\n",
      "   % Time: 1528s | Batch: 2001 | Train loss: 0.3963\n",
      "   % Time: 1536s | Batch: 2011 | Train loss: 0.3513\n",
      "   % Time: 1543s | Batch: 2021 | Train loss: 0.3872\n",
      "   % Time: 1551s | Batch: 2031 | Train loss: 0.3828\n",
      "   % Time: 1559s | Batch: 2041 | Train loss: 0.3697\n",
      "   % Time: 1566s | Batch: 2051 | Train loss: 0.3771\n",
      "   % Time: 1574s | Batch: 2061 | Train loss: 0.4323\n",
      "   % Time: 1582s | Batch: 2071 | Train loss: 0.3787\n",
      "   % Time: 1589s | Batch: 2081 | Train loss: 0.3578\n",
      "   % Time: 1597s | Batch: 2091 | Train loss: 0.3317\n",
      "   % Time: 1604s | Batch: 2101 | Train loss: 0.3511\n",
      "   % Time: 1612s | Batch: 2111 | Train loss: 0.3886\n",
      "   % Time: 1620s | Batch: 2121 | Train loss: 0.3543\n",
      "   % Time: 1627s | Batch: 2131 | Train loss: 0.3779\n",
      "   % Time: 1635s | Batch: 2141 | Train loss: 0.3505\n",
      "   % Time: 1643s | Batch: 2151 | Train loss: 0.4065\n",
      "   % Time: 1650s | Batch: 2161 | Train loss: 0.3754\n",
      "   % Time: 1658s | Batch: 2171 | Train loss: 0.3866\n",
      "   % Time: 1665s | Batch: 2181 | Train loss: 0.3919\n",
      "   % Time: 1673s | Batch: 2191 | Train loss: 0.3707\n",
      "   % Time: 1681s | Batch: 2201 | Train loss: 0.3877\n",
      "   % Time: 1688s | Batch: 2211 | Train loss: 0.3671\n",
      "   % Time: 1696s | Batch: 2221 | Train loss: 0.3733\n",
      "   % Time: 1704s | Batch: 2231 | Train loss: 0.3587\n",
      "   % Time: 1711s | Batch: 2241 | Train loss: 0.3447\n",
      "   % Time: 1719s | Batch: 2251 | Train loss: 0.3404\n",
      "   % Time: 1727s | Batch: 2261 | Train loss: 0.3681\n",
      "==========\n",
      "   % Epoch: 6 | Time: 1731s | Train loss: 0.3717 | Val loss: 26.4670\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "if args.train:\n",
    "    for epoch in range(1, args.n_epoch+1):\n",
    "        scheduler.step()\n",
    "        print(\"=> EPOCH {} with lr {}\".format(epoch, scheduler.get_lr()[0]))\n",
    "        val_loss = train(scaled_data, scaler, features,\n",
    "                         model, criterion, optimizer)\n",
    "        save_model(model, epoch, val_loss)\n",
    "else:\n",
    "    model_file = os.path.join(args.intermediate_path, args.model_name)\n",
    "    model.load_state_dict(torch.load(model_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-06T10:01:05.348006Z",
     "start_time": "2017-09-06T09:57:02.243369Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if args.forecast:\n",
    "    prediction = forecast(scaled_data, scaler, features, model)\n",
    "\n",
    "    data_df = pd.read_csv(os.path.join(args.data_path, args.train_file),\n",
    "                          index_col='Page')\n",
    "    key_df = pd.read_csv(os.path.join(args.data_path, args.key_file))\n",
    "    key_df['Date'] = key_df['Page'].apply(lambda a: a[-10:])\n",
    "    key_df['Page'] = key_df['Page'].apply(lambda a: a[:-11])\n",
    "    \n",
    "    future_start = (pd.Timestamp(args.forecast_start)\n",
    "                    - pd.Timestamp(data_df.columns[-1])).days - 1\n",
    "    future_end = (pd.Timestamp(args.forecast_end)\n",
    "                  - pd.Timestamp(data_df.columns[-1])).days\n",
    "    future_period = future_end - future_start\n",
    "\n",
    "    visits = np.zeros(key_df.shape[0])\n",
    "    for i in range(0, len(visits), future_period):\n",
    "        page = key_df['Page'][i]\n",
    "        page_index = data_df.index.get_loc(page)\n",
    "        visits[i:(i+future_period)] = prediction[page_index,\n",
    "                                                 future_start:future_end]\n",
    "\n",
    "    key_df['Visits'] = visits\n",
    "    submission_file = os.path.join(args.intermediate_path,\n",
    "                                   'submission_{}.csv'.format(args.seed))\n",
    "    key_df[['Id', 'Visits']].to_csv(submission_file, float_format='%.0f',\n",
    "                                    index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pydata)",
   "language": "python",
   "name": "pydata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
