{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-01T03:53:53.655384Z",
     "start_time": "2017-09-01T12:53:52.591036+09:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import numpy as np; np.seterr(invalid='ignore')\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-01T03:53:53.713095Z",
     "start_time": "2017-09-01T12:53:53.658326+09:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "parser = {\n",
    "    'data_path': '../data/wttsf/',\n",
    "    'train_file': 'train_1.csv',\n",
    "    'intermediate_path': '../intermediate/',\n",
    "    'n_epoch': 4,\n",
    "    'future': 73,\n",
    "    'batch_size': 128,\n",
    "    'hidden_size': 128,\n",
    "    'log_every': 10,\n",
    "    'read_from_file': True,\n",
    "    'train': True,\n",
    "    'model_name': 'model_seed20170901_epoch5_loss_0.3894.pth',\n",
    "    'cuda': True,\n",
    "    'seed': 20170901,\n",
    "}\n",
    "args = argparse.Namespace(**parser)\n",
    "\n",
    "args.cuda = args.cuda and torch.cuda.is_available()\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "args.intermediate_path = os.path.join(args.intermediate_path, str(args.seed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-01T03:53:53.900131Z",
     "start_time": "2017-09-01T12:53:53.715587+09:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class DenseLSTMForecast(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(DenseLSTMForecast, self).__init__()\n",
    "        self.lstm1 = nn.LSTMCell(15, hidden_size, bias=False)\n",
    "        self.lstm2 = nn.LSTMCell(hidden_size+15, hidden_size, bias=False)\n",
    "#        self.lstm3 = nn.LSTMCell(2*hidden_size+1, hidden_size, bias=False)\n",
    "        self.linear = nn.Linear(2*hidden_size+15, 1)\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    def forward(self, x, feature, future=1):\n",
    "        o = []\n",
    "        tt = torch.cuda if args.cuda else torch\n",
    "        h1_t = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())\n",
    "        c1_t = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())\n",
    "        h2_t = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())\n",
    "        c2_t = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())\n",
    "#        h3_t = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())\n",
    "#        c3_t = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())\n",
    "        \n",
    "        for x_t in x.chunk(x.size(1), dim=1):\n",
    "            x_t = x_t.squeeze(dim=1)\n",
    "            xd_t = torch.cat([x_t, feature], dim=1)\n",
    "            h1_t, c1_t = self.lstm1(xd_t, (h1_t, c1_t))\n",
    "            h1d_t = torch.cat([xd_t, h1_t], dim=1)\n",
    "            h2_t, c2_t = self.lstm2(h1d_t, (h2_t, c2_t))\n",
    "            h2d_t = torch.cat([xd_t, h1_t, h2_t], dim=1)\n",
    "#            h3_t, c3_t = self.lstm3(h2d_t, (h3_t, c3_t))\n",
    "#            h3d_t = torch.cat([x_t, h1_t, h2_t, h3_t], dim=1)\n",
    "            o_t = self.linear(h2d_t)\n",
    "            o.append(o_t)\n",
    "            \n",
    "        for i in range(future-1):\n",
    "            od_t = torch.cat([o_t, feature], dim=1)\n",
    "            h1_t, c1_t = self.lstm1(od_t, (h1_t, c1_t))\n",
    "            h1d_t = torch.cat([od_t, h1_t], dim=1)\n",
    "            h2_t, c2_t = self.lstm2(h1d_t, (h2_t, c2_t))\n",
    "            h2d_t = torch.cat([od_t, h1_t, h2_t], dim=1)\n",
    "#            h3_t, c3_t = self.lstm3(h2d_t, (h3_t, c3_t))\n",
    "#            h3d_t = torch.cat([o_t, h1_t, h2_t, h3_t], dim=1)\n",
    "            o_t = self.linear(h2d_t)\n",
    "            o.append(o_t)\n",
    "\n",
    "        return torch.stack(o, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-01T03:53:53.946948Z",
     "start_time": "2017-09-01T12:53:53.902766+09:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def smape(y_pred, y_true):\n",
    "    y_pred = np.around(np.clip(np.exp(y_pred)-1, 0, None))\n",
    "    y_true = np.around(np.exp(y_true) - 1)\n",
    "    raw_smape = np.abs(y_true - y_pred) / (np.abs(y_true) + np.abs(y_pred))\n",
    "    kaggle_smape = np.nan_to_num(raw_smape)\n",
    "    return np.mean(kaggle_smape) * 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-01T03:53:54.112511Z",
     "start_time": "2017-09-01T12:53:53.949227+09:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    raw_data_file = os.path.join(args.intermediate_path,\n",
    "                                 'raw_data.pkl')\n",
    "    scaled_data_file = os.path.join(args.intermediate_path,\n",
    "                                    'scaled_data.pkl')\n",
    "    scaler_file = os.path.join(args.intermediate_path, 'scaler.pkl')\n",
    "    features_file = os.path.join(args.intermediate_path, 'features.pkl')\n",
    "    \n",
    "    if not args.read_from_file:\n",
    "        data_df = pd.read_csv(os.path.join(args.data_path, args.train_file),\n",
    "                              index_col='Page')\n",
    "        data_df[\"agent\"] = data_df.index.str.rsplit('_').str.get(-1)\n",
    "        data_df[\"access\"] = data_df.index.str.rsplit('_').str.get(-2)\n",
    "        data_df[\"project\"] = data_df.index.str.rsplit('_').str.get(-3)\n",
    "        features = pd.get_dummies(data_df[[\"agent\", \"access\", \"project\"]],\n",
    "            columns=[\"agent\", \"access\", \"project\"]).values.astype('float32')\n",
    "        raw_data = np.nan_to_num(\n",
    "            data_df.iloc[:,:-3].values.astype('float32'))\n",
    "        data = np.log1p(raw_data)\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(np.swapaxes(data[:, :-args.future], 0, 1))\n",
    "        scaled_data = scaler.transform(np.swapaxes(data, 0, 1))\n",
    "        scaled_data = np.swapaxes(scaled_data, 0, 1)\n",
    "        \n",
    "        with open(raw_data_file, 'wb') as f:\n",
    "            pickle.dump(raw_data, f)\n",
    "        with open(scaled_data_file, 'wb') as f:\n",
    "            pickle.dump(scaled_data, f)\n",
    "        with open(scaler_file, 'wb') as f:\n",
    "            pickle.dump(scaler, f)\n",
    "        with open(features_file, 'wb') as f:\n",
    "            pickle.dump(features, f)\n",
    "    else:\n",
    "        with open(raw_data_file, 'rb') as f:\n",
    "            raw_data = pickle.load(f)\n",
    "        with open(scaled_data_file, 'rb') as f:\n",
    "            scaled_data = pickle.load(f)\n",
    "        with open(scaler_file, 'rb') as f:\n",
    "            scaler = pickle.load(f)\n",
    "        with open(features_file, 'rb') as f:\n",
    "            features = pickle.load(f)\n",
    "    return raw_data, scaled_data, scaler, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-01T03:53:54.373063Z",
     "start_time": "2017-09-01T12:53:54.114831+09:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def train(raw_data, scaled_data, scaler, features,\n",
    "          model, criterion, optimizer):\n",
    "    p = np.random.permutation(raw_data.shape[0])\n",
    "    input_tensor = torch.from_numpy(scaled_data[p, :-1]).unsqueeze(2)\n",
    "    target_tensor = torch.from_numpy(scaled_data[p, 1:]).unsqueeze(2)\n",
    "    features_tensor = torch.from_numpy(features[p, :])\n",
    "    dataset = TensorDataset(input_tensor, target_tensor)\n",
    "    data_loader = DataLoader(dataset, args.batch_size)\n",
    "    \n",
    "    train_loss = 0\n",
    "    val_output_list = []\n",
    "    init_time = time.time()\n",
    "    for i, (inputt, target) in enumerate(data_loader):\n",
    "        feature = features_tensor[i*args.batch_size:(i*args.batch_size\n",
    "                                                     +inputt.size(0))]\n",
    "        if args.cuda:\n",
    "            inputt = inputt.cuda()\n",
    "            target = target.cuda()\n",
    "            feature = feature.cuda()\n",
    "        inputt = Variable(inputt)\n",
    "        target = Variable(target)\n",
    "        feature = Variable(feature)\n",
    "        \n",
    "        output = model(inputt, feature)\n",
    "        pos = np.random.randint(args.future, output.size(1)-args.future+1)\n",
    "        pos_val = np.random.randint(args.future,\n",
    "                                    output.size(1)-args.future+1)\n",
    "        loss = criterion(output[:, pos:pos+args.future],\n",
    "                         target[:, pos:pos+args.future])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.data[0] * inputt.size(0)\n",
    "\n",
    "        if i % args.log_every == 0:\n",
    "            val_output = output[\n",
    "                :, pos_val:pos_val+args.future].data.squeeze(2).cpu().numpy()\n",
    "            val_target = target[\n",
    "                :, pos_val:pos_val+args.future].data.squeeze(2).cpu().numpy()\n",
    "            val_scale = scaler.scale_[p][\n",
    "                i*args.batch_size:i*args.batch_size+inputt.size(0)]\n",
    "            val_mean = scaler.mean_[p][\n",
    "                i*args.batch_size:i*args.batch_size+inputt.size(0)]\n",
    "            raw_val_output = (val_output.T * val_scale + val_mean).T\n",
    "            raw_val_target = (val_target.T * val_scale + val_mean).T\n",
    "            val_loss = smape(raw_val_output, raw_val_target)\n",
    "            print(\"   % Time: {:4.0f}s | Batch: {:4} | \"\n",
    "                  \"Train loss: {:.4f} | Val loss: {:.4f}\".format(\n",
    "                      time.time()-init_time, i+1, loss.data[0], val_loss))\n",
    "        \n",
    "#     val_output_all = np.concatenate(val_output_list, axis=0)[inverse_p]\n",
    "#     prediction = np.swapaxes(scaler.inverse_transform(\n",
    "#             np.swapaxes(val_output_all, 0, 1)), 0, 1)\n",
    "#     prediction = np.exp(prediction) - 1\n",
    "#     prediction[prediction < 0] = 0\n",
    "#     prediction = np.around(prediction)\n",
    "    \n",
    "    train_loss /= raw_data.shape[0]\n",
    "#     val_loss = smape(prediction, raw_data[:, -args.future:])\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-01T03:53:54.430676Z",
     "start_time": "2017-09-01T12:53:54.375383+09:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def forecast(scaled_data, scaler, features, model):\n",
    "    input_tensor = torch.from_numpy(scaled_data).unsqueeze(2)\n",
    "    target_tensor = torch.zeros(input_tensor.size(0))\n",
    "    features_tensor = torch.from_numpy(features)\n",
    "    dataset = torch.utils.data.TensorDataset(input_tensor, target_tensor)\n",
    "    data_loader = DataLoader(dataset, args.batch_size)\n",
    "    \n",
    "    output_list = []\n",
    "    for i, (inputt, _) in enumerate(data_loader):\n",
    "        feature = features_tensor[i*args.batch_size:(i*args.batch_size\n",
    "                                                     +inputt.size(0))]\n",
    "        if args.cuda:\n",
    "            inputt = inputt.cuda()\n",
    "            feature = feature.cuda()\n",
    "        inputt = Variable(inputt)\n",
    "        feature = Variable(feature)\n",
    "        output = model(inputt, feature, args.future)\n",
    "        output_list.append(output.data.squeeze(2).cpu().numpy()\n",
    "                           [:, -args.future:])\n",
    "        \n",
    "    output_all = np.concatenate(output_list, axis=0)\n",
    "    prediction = np.swapaxes(scaler.inverse_transform(\n",
    "            np.swapaxes(output_all, 0, 1)), 0, 1)\n",
    "    prediction = np.around(np.clip(np.exp(prediction) - 1, 0, None))\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-01T03:53:54.470574Z",
     "start_time": "2017-09-01T12:53:54.432955+09:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def save_model(model, epoch, loss):\n",
    "    model_file = os.path.join(args.intermediate_path,\n",
    "                              \"model_seed{}_epoch{}_loss_{:.4f}.pth\"\n",
    "                              .format(args.seed, epoch, loss))\n",
    "    torch.save(model.state_dict(), os.path.join(model_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-01T03:53:55.107512Z",
     "start_time": "2017-09-01T12:53:54.472949+09:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "raw_data, scaled_data, scaler, features = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-01T03:53:57.494846Z",
     "start_time": "2017-09-01T12:53:55.110214+09:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model = DenseLSTMForecast(args.hidden_size)\n",
    "if args.cuda:\n",
    "    model.cuda()\n",
    "criterion = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-01T03:53:57.501864Z",
     "start_time": "2017-09-01T12:53:57.497730+09:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-01T05:54:15.609505Z",
     "start_time": "2017-09-01T12:53:57.504200+09:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> EPOCH 1\n",
      "   % Time:    2s | Batch:    1 | Train loss: 0.6384 | Val loss: 55.0431\n",
      "   % Time:   18s | Batch:   11 | Train loss: 0.5400 | Val loss: 41.6493\n",
      "   % Time:   34s | Batch:   21 | Train loss: 0.4547 | Val loss: 30.7615\n",
      "   % Time:   49s | Batch:   31 | Train loss: 0.5189 | Val loss: 36.6601\n",
      "   % Time:   65s | Batch:   41 | Train loss: 0.4714 | Val loss: 37.8609\n",
      "   % Time:   81s | Batch:   51 | Train loss: 0.5411 | Val loss: 34.3930\n",
      "   % Time:   97s | Batch:   61 | Train loss: 0.4675 | Val loss: 30.5571\n",
      "   % Time:  112s | Batch:   71 | Train loss: 0.4547 | Val loss: 35.0308\n",
      "   % Time:  128s | Batch:   81 | Train loss: 0.4996 | Val loss: 32.0318\n",
      "   % Time:  144s | Batch:   91 | Train loss: 0.4490 | Val loss: 31.3885\n",
      "   % Time:  160s | Batch:  101 | Train loss: 0.4388 | Val loss: 32.4970\n",
      "   % Time:  175s | Batch:  111 | Train loss: 0.5211 | Val loss: 31.3125\n",
      "   % Time:  191s | Batch:  121 | Train loss: 0.4598 | Val loss: 30.4110\n",
      "   % Time:  207s | Batch:  131 | Train loss: 0.4345 | Val loss: 32.6981\n",
      "   % Time:  223s | Batch:  141 | Train loss: 0.5257 | Val loss: 34.7931\n",
      "   % Time:  239s | Batch:  151 | Train loss: 0.4399 | Val loss: 29.4485\n",
      "   % Time:  254s | Batch:  161 | Train loss: 0.4438 | Val loss: 29.9712\n",
      "   % Time:  270s | Batch:  171 | Train loss: 0.4291 | Val loss: 32.0818\n",
      "   % Time:  286s | Batch:  181 | Train loss: 0.4174 | Val loss: 28.9267\n",
      "   % Time:  302s | Batch:  191 | Train loss: 0.4568 | Val loss: 29.2464\n",
      "   % Time:  317s | Batch:  201 | Train loss: 0.4544 | Val loss: 34.4364\n",
      "   % Time:  333s | Batch:  211 | Train loss: 0.4577 | Val loss: 30.7870\n",
      "   % Time:  349s | Batch:  221 | Train loss: 0.4407 | Val loss: 28.0270\n",
      "   % Time:  365s | Batch:  231 | Train loss: 0.4624 | Val loss: 34.8244\n",
      "   % Time:  381s | Batch:  241 | Train loss: 0.4621 | Val loss: 33.0251\n",
      "   % Time:  397s | Batch:  251 | Train loss: 0.4503 | Val loss: 29.9922\n",
      "   % Time:  413s | Batch:  261 | Train loss: 0.4425 | Val loss: 28.5577\n",
      "   % Time:  429s | Batch:  271 | Train loss: 0.4369 | Val loss: 29.4155\n",
      "   % Time:  445s | Batch:  281 | Train loss: 0.4296 | Val loss: 30.2982\n",
      "   % Time:  461s | Batch:  291 | Train loss: 0.4419 | Val loss: 31.5758\n",
      "   % Time:  477s | Batch:  301 | Train loss: 0.4001 | Val loss: 28.5656\n",
      "   % Time:  493s | Batch:  311 | Train loss: 0.4268 | Val loss: 31.5736\n",
      "   % Time:  509s | Batch:  321 | Train loss: 0.4476 | Val loss: 34.0044\n",
      "   % Time:  524s | Batch:  331 | Train loss: 0.4081 | Val loss: 29.8557\n",
      "   % Time:  540s | Batch:  341 | Train loss: 0.4522 | Val loss: 31.2691\n",
      "   % Time:  556s | Batch:  351 | Train loss: 0.4310 | Val loss: 29.2637\n",
      "   % Time:  571s | Batch:  361 | Train loss: 0.4833 | Val loss: 29.9843\n",
      "   % Time:  587s | Batch:  371 | Train loss: 0.5275 | Val loss: 28.5887\n",
      "   % Time:  603s | Batch:  381 | Train loss: 0.4538 | Val loss: 31.1588\n",
      "   % Time:  619s | Batch:  391 | Train loss: 0.3997 | Val loss: 26.8078\n",
      "   % Time:  635s | Batch:  401 | Train loss: 0.4715 | Val loss: 30.6676\n",
      "   % Time:  650s | Batch:  411 | Train loss: 0.4897 | Val loss: 32.1452\n",
      "   % Time:  666s | Batch:  421 | Train loss: 0.4662 | Val loss: 33.6598\n",
      "   % Time:  682s | Batch:  431 | Train loss: 0.4759 | Val loss: 32.0051\n",
      "   % Time:  699s | Batch:  441 | Train loss: 0.4616 | Val loss: 34.7093\n",
      "   % Time:  715s | Batch:  451 | Train loss: 0.4559 | Val loss: 31.0738\n",
      "   % Time:  731s | Batch:  461 | Train loss: 0.4343 | Val loss: 35.1809\n",
      "   % Time:  747s | Batch:  471 | Train loss: 0.4820 | Val loss: 29.3971\n",
      "   % Time:  763s | Batch:  481 | Train loss: 0.4783 | Val loss: 31.2726\n",
      "   % Time:  779s | Batch:  491 | Train loss: 0.4052 | Val loss: 29.4366\n",
      "   % Time:  795s | Batch:  501 | Train loss: 0.4484 | Val loss: 32.9709\n",
      "   % Time:  810s | Batch:  511 | Train loss: 0.4688 | Val loss: 29.5264\n",
      "   % Time:  826s | Batch:  521 | Train loss: 0.4753 | Val loss: 29.9390\n",
      "   % Time:  842s | Batch:  531 | Train loss: 0.4542 | Val loss: 29.5417\n",
      "   % Time:  858s | Batch:  541 | Train loss: 0.4890 | Val loss: 36.2983\n",
      "   % Time:  874s | Batch:  551 | Train loss: 0.4239 | Val loss: 29.7869\n",
      "   % Time:  889s | Batch:  561 | Train loss: 0.4463 | Val loss: 33.9112\n",
      "   % Time:  905s | Batch:  571 | Train loss: 0.5049 | Val loss: 32.1929\n",
      "   % Time:  921s | Batch:  581 | Train loss: 0.4401 | Val loss: 28.9294\n",
      "   % Time:  937s | Batch:  591 | Train loss: 0.4414 | Val loss: 29.5639\n",
      "   % Time:  953s | Batch:  601 | Train loss: 0.4491 | Val loss: 29.3244\n",
      "   % Time:  969s | Batch:  611 | Train loss: 0.4412 | Val loss: 32.9711\n",
      "   % Time:  985s | Batch:  621 | Train loss: 0.4172 | Val loss: 28.0755\n",
      "   % Time: 1001s | Batch:  631 | Train loss: 0.4144 | Val loss: 28.7724\n",
      "   % Time: 1017s | Batch:  641 | Train loss: 0.4791 | Val loss: 29.4967\n",
      "   % Time: 1033s | Batch:  651 | Train loss: 0.4833 | Val loss: 28.0047\n",
      "   % Time: 1049s | Batch:  661 | Train loss: 0.4291 | Val loss: 29.7514\n",
      "   % Time: 1064s | Batch:  671 | Train loss: 0.4801 | Val loss: 32.2787\n",
      "   % Time: 1080s | Batch:  681 | Train loss: 0.4548 | Val loss: 29.7593\n",
      "   % Time: 1096s | Batch:  691 | Train loss: 0.4158 | Val loss: 30.2288\n",
      "   % Time: 1112s | Batch:  701 | Train loss: 0.5059 | Val loss: 28.0240\n",
      "   % Time: 1128s | Batch:  711 | Train loss: 0.4220 | Val loss: 27.9153\n",
      "   % Time: 1144s | Batch:  721 | Train loss: 0.4611 | Val loss: 30.7524\n",
      "   % Time: 1160s | Batch:  731 | Train loss: 0.4185 | Val loss: 27.5976\n",
      "   % Time: 1176s | Batch:  741 | Train loss: 0.4384 | Val loss: 29.1647\n",
      "   % Time: 1192s | Batch:  751 | Train loss: 0.4102 | Val loss: 30.3290\n",
      "   % Time: 1208s | Batch:  761 | Train loss: 0.4504 | Val loss: 34.9372\n",
      "   % Time: 1223s | Batch:  771 | Train loss: 0.4392 | Val loss: 28.4454\n",
      "   % Time: 1239s | Batch:  781 | Train loss: 0.5454 | Val loss: 28.6555\n",
      "   % Time: 1255s | Batch:  791 | Train loss: 0.5364 | Val loss: 32.6362\n",
      "   % Time: 1271s | Batch:  801 | Train loss: 0.4547 | Val loss: 32.6835\n",
      "   % Time: 1287s | Batch:  811 | Train loss: 0.4613 | Val loss: 27.6969\n",
      "   % Time: 1303s | Batch:  821 | Train loss: 0.4252 | Val loss: 28.2022\n",
      "   % Time: 1319s | Batch:  831 | Train loss: 0.4834 | Val loss: 35.1999\n",
      "   % Time: 1335s | Batch:  841 | Train loss: 0.4581 | Val loss: 27.8334\n",
      "   % Time: 1351s | Batch:  851 | Train loss: 0.5158 | Val loss: 33.9849\n",
      "   % Time: 1367s | Batch:  861 | Train loss: 0.4305 | Val loss: 31.5584\n",
      "   % Time: 1383s | Batch:  871 | Train loss: 0.4429 | Val loss: 31.7125\n",
      "   % Time: 1399s | Batch:  881 | Train loss: 0.4373 | Val loss: 28.2066\n",
      "   % Time: 1415s | Batch:  891 | Train loss: 0.4589 | Val loss: 31.7558\n",
      "   % Time: 1431s | Batch:  901 | Train loss: 0.4440 | Val loss: 31.0773\n",
      "   % Time: 1447s | Batch:  911 | Train loss: 0.4599 | Val loss: 29.7784\n",
      "   % Time: 1463s | Batch:  921 | Train loss: 0.4432 | Val loss: 30.9394\n",
      "   % Time: 1479s | Batch:  931 | Train loss: 0.4286 | Val loss: 33.6397\n",
      "   % Time: 1495s | Batch:  941 | Train loss: 0.4465 | Val loss: 28.7650\n",
      "   % Time: 1511s | Batch:  951 | Train loss: 0.4973 | Val loss: 29.2431\n",
      "   % Time: 1527s | Batch:  961 | Train loss: 0.4374 | Val loss: 30.5285\n",
      "   % Time: 1543s | Batch:  971 | Train loss: 0.4997 | Val loss: 36.0007\n",
      "   % Time: 1559s | Batch:  981 | Train loss: 0.4913 | Val loss: 33.7892\n",
      "   % Time: 1575s | Batch:  991 | Train loss: 0.4332 | Val loss: 31.9195\n",
      "   % Time: 1591s | Batch: 1001 | Train loss: 0.4685 | Val loss: 33.8304\n",
      "   % Time: 1607s | Batch: 1011 | Train loss: 0.4484 | Val loss: 31.4259\n",
      "   % Time: 1623s | Batch: 1021 | Train loss: 0.4696 | Val loss: 28.7297\n",
      "   % Time: 1638s | Batch: 1031 | Train loss: 0.4361 | Val loss: 32.2370\n",
      "   % Time: 1654s | Batch: 1041 | Train loss: 0.4390 | Val loss: 30.5756\n",
      "   % Time: 1670s | Batch: 1051 | Train loss: 0.4432 | Val loss: 30.1815\n",
      "   % Time: 1686s | Batch: 1061 | Train loss: 0.4540 | Val loss: 27.6814\n",
      "   % Time: 1702s | Batch: 1071 | Train loss: 0.4437 | Val loss: 30.8545\n",
      "   % Time: 1718s | Batch: 1081 | Train loss: 0.4692 | Val loss: 28.4440\n",
      "   % Time: 1734s | Batch: 1091 | Train loss: 0.4836 | Val loss: 32.0485\n",
      "   % Time: 1750s | Batch: 1101 | Train loss: 0.4644 | Val loss: 27.8906\n",
      "   % Time: 1766s | Batch: 1111 | Train loss: 0.4403 | Val loss: 27.2591\n",
      "   % Time: 1782s | Batch: 1121 | Train loss: 0.4376 | Val loss: 33.4496\n",
      "   % Time: 1797s | Batch: 1131 | Train loss: 0.4372 | Val loss: 29.5985\n",
      "   % Train loss 0.4579\n",
      "=> EPOCH 2\n",
      "   % Time:    1s | Batch:    1 | Train loss: 0.4352 | Val loss: 29.5250\n",
      "   % Time:   17s | Batch:   11 | Train loss: 0.4254 | Val loss: 29.1247\n",
      "   % Time:   33s | Batch:   21 | Train loss: 0.4370 | Val loss: 28.7382\n",
      "   % Time:   48s | Batch:   31 | Train loss: 0.4777 | Val loss: 31.2105\n",
      "   % Time:   64s | Batch:   41 | Train loss: 0.4187 | Val loss: 28.3324\n",
      "   % Time:   80s | Batch:   51 | Train loss: 0.4320 | Val loss: 30.6024\n",
      "   % Time:   96s | Batch:   61 | Train loss: 0.7063 | Val loss: 29.5048\n",
      "   % Time:  112s | Batch:   71 | Train loss: 0.4290 | Val loss: 27.5808\n",
      "   % Time:  128s | Batch:   81 | Train loss: 0.4532 | Val loss: 30.1478\n",
      "   % Time:  144s | Batch:   91 | Train loss: 0.4417 | Val loss: 30.0568\n",
      "   % Time:  160s | Batch:  101 | Train loss: 0.4488 | Val loss: 30.7437\n",
      "   % Time:  176s | Batch:  111 | Train loss: 0.4393 | Val loss: 31.6977\n",
      "   % Time:  192s | Batch:  121 | Train loss: 0.4845 | Val loss: 31.3432\n",
      "   % Time:  208s | Batch:  131 | Train loss: 0.4392 | Val loss: 31.8698\n",
      "   % Time:  224s | Batch:  141 | Train loss: 0.4301 | Val loss: 27.9144\n",
      "   % Time:  240s | Batch:  151 | Train loss: 0.4487 | Val loss: 29.2055\n",
      "   % Time:  256s | Batch:  161 | Train loss: 0.4226 | Val loss: 31.9155\n",
      "   % Time:  272s | Batch:  171 | Train loss: 0.3746 | Val loss: 29.8817\n",
      "   % Time:  288s | Batch:  181 | Train loss: 0.4418 | Val loss: 29.3215\n",
      "   % Time:  303s | Batch:  191 | Train loss: 0.4929 | Val loss: 28.5357\n",
      "   % Time:  319s | Batch:  201 | Train loss: 0.4353 | Val loss: 31.2774\n",
      "   % Time:  335s | Batch:  211 | Train loss: 0.4572 | Val loss: 29.8874\n",
      "   % Time:  351s | Batch:  221 | Train loss: 0.4932 | Val loss: 31.6927\n",
      "   % Time:  367s | Batch:  231 | Train loss: 0.4038 | Val loss: 27.4675\n",
      "   % Time:  382s | Batch:  241 | Train loss: 0.4319 | Val loss: 29.9422\n",
      "   % Time:  398s | Batch:  251 | Train loss: 0.4372 | Val loss: 30.7796\n",
      "   % Time:  414s | Batch:  261 | Train loss: 0.4372 | Val loss: 31.3790\n",
      "   % Time:  430s | Batch:  271 | Train loss: 0.4383 | Val loss: 30.6106\n",
      "   % Time:  446s | Batch:  281 | Train loss: 0.4452 | Val loss: 31.8475\n",
      "   % Time:  462s | Batch:  291 | Train loss: 0.4757 | Val loss: 35.5314\n",
      "   % Time:  478s | Batch:  301 | Train loss: 0.4237 | Val loss: 30.1624\n",
      "   % Time:  494s | Batch:  311 | Train loss: 0.4647 | Val loss: 29.4312\n",
      "   % Time:  510s | Batch:  321 | Train loss: 0.5592 | Val loss: 30.4171\n",
      "   % Time:  526s | Batch:  331 | Train loss: 0.4321 | Val loss: 28.1973\n",
      "   % Time:  542s | Batch:  341 | Train loss: 0.4438 | Val loss: 34.7137\n",
      "   % Time:  558s | Batch:  351 | Train loss: 0.4515 | Val loss: 31.7157\n",
      "   % Time:  574s | Batch:  361 | Train loss: 0.4618 | Val loss: 32.2371\n",
      "   % Time:  590s | Batch:  371 | Train loss: 0.4345 | Val loss: 26.9831\n",
      "   % Time:  606s | Batch:  381 | Train loss: 0.4626 | Val loss: 33.8865\n",
      "   % Time:  622s | Batch:  391 | Train loss: 0.4295 | Val loss: 29.6874\n",
      "   % Time:  638s | Batch:  401 | Train loss: 0.4835 | Val loss: 30.7238\n",
      "   % Time:  654s | Batch:  411 | Train loss: 0.4812 | Val loss: 28.4820\n",
      "   % Time:  670s | Batch:  421 | Train loss: 0.4416 | Val loss: 30.0957\n",
      "   % Time:  686s | Batch:  431 | Train loss: 0.4316 | Val loss: 28.5766\n",
      "   % Time:  702s | Batch:  441 | Train loss: 0.4688 | Val loss: 31.9896\n",
      "   % Time:  718s | Batch:  451 | Train loss: 0.4049 | Val loss: 27.6258\n",
      "   % Time:  734s | Batch:  461 | Train loss: 0.4168 | Val loss: 27.9624\n",
      "   % Time:  750s | Batch:  471 | Train loss: 0.4301 | Val loss: 33.3613\n",
      "   % Time:  766s | Batch:  481 | Train loss: 0.4386 | Val loss: 27.9958\n",
      "   % Time:  782s | Batch:  491 | Train loss: 0.4559 | Val loss: 31.0857\n",
      "   % Time:  798s | Batch:  501 | Train loss: 0.4498 | Val loss: 31.6163\n",
      "   % Time:  814s | Batch:  511 | Train loss: 0.4338 | Val loss: 30.6765\n",
      "   % Time:  830s | Batch:  521 | Train loss: 0.4934 | Val loss: 33.1448\n",
      "   % Time:  845s | Batch:  531 | Train loss: 0.4703 | Val loss: 30.8808\n",
      "   % Time:  861s | Batch:  541 | Train loss: 0.4336 | Val loss: 29.7680\n",
      "   % Time:  877s | Batch:  551 | Train loss: 0.4625 | Val loss: 31.3648\n",
      "   % Time:  893s | Batch:  561 | Train loss: 0.4185 | Val loss: 29.0874\n",
      "   % Time:  909s | Batch:  571 | Train loss: 0.4363 | Val loss: 34.1484\n",
      "   % Time:  924s | Batch:  581 | Train loss: 0.4332 | Val loss: 32.1847\n",
      "   % Time:  940s | Batch:  591 | Train loss: 0.4466 | Val loss: 27.8407\n",
      "   % Time:  956s | Batch:  601 | Train loss: 0.4555 | Val loss: 35.2937\n",
      "   % Time:  971s | Batch:  611 | Train loss: 0.4849 | Val loss: 30.2518\n",
      "   % Time:  987s | Batch:  621 | Train loss: 0.4260 | Val loss: 27.1665\n",
      "   % Time: 1003s | Batch:  631 | Train loss: 0.4390 | Val loss: 27.6211\n",
      "   % Time: 1019s | Batch:  641 | Train loss: 0.4119 | Val loss: 29.2659\n",
      "   % Time: 1035s | Batch:  651 | Train loss: 0.4281 | Val loss: 28.2386\n",
      "   % Time: 1050s | Batch:  661 | Train loss: 0.4887 | Val loss: 34.2035\n",
      "   % Time: 1066s | Batch:  671 | Train loss: 0.4526 | Val loss: 33.9475\n",
      "   % Time: 1082s | Batch:  681 | Train loss: 0.4454 | Val loss: 28.7325\n",
      "   % Time: 1098s | Batch:  691 | Train loss: 0.4327 | Val loss: 33.3079\n",
      "   % Time: 1114s | Batch:  701 | Train loss: 0.4132 | Val loss: 29.0415\n",
      "   % Time: 1129s | Batch:  711 | Train loss: 0.4612 | Val loss: 32.8433\n",
      "   % Time: 1145s | Batch:  721 | Train loss: 0.4231 | Val loss: 30.2424\n",
      "   % Time: 1161s | Batch:  731 | Train loss: 0.4152 | Val loss: 29.4649\n",
      "   % Time: 1176s | Batch:  741 | Train loss: 0.4400 | Val loss: 30.4986\n",
      "   % Time: 1192s | Batch:  751 | Train loss: 0.4580 | Val loss: 32.7492\n",
      "   % Time: 1208s | Batch:  761 | Train loss: 0.4657 | Val loss: 33.0550\n",
      "   % Time: 1224s | Batch:  771 | Train loss: 0.4581 | Val loss: 28.8406\n",
      "   % Time: 1240s | Batch:  781 | Train loss: 0.4310 | Val loss: 30.7792\n",
      "   % Time: 1255s | Batch:  791 | Train loss: 0.4613 | Val loss: 30.4626\n",
      "   % Time: 1271s | Batch:  801 | Train loss: 0.4387 | Val loss: 30.4237\n",
      "   % Time: 1287s | Batch:  811 | Train loss: 0.4698 | Val loss: 30.1432\n",
      "   % Time: 1303s | Batch:  821 | Train loss: 0.4311 | Val loss: 29.3634\n",
      "   % Time: 1319s | Batch:  831 | Train loss: 0.4533 | Val loss: 31.1643\n",
      "   % Time: 1335s | Batch:  841 | Train loss: 0.4523 | Val loss: 29.8781\n",
      "   % Time: 1351s | Batch:  851 | Train loss: 0.4138 | Val loss: 26.8104\n",
      "   % Time: 1366s | Batch:  861 | Train loss: 0.4461 | Val loss: 29.2168\n",
      "   % Time: 1382s | Batch:  871 | Train loss: 0.4473 | Val loss: 31.4581\n",
      "   % Time: 1398s | Batch:  881 | Train loss: 0.4373 | Val loss: 34.0992\n",
      "   % Time: 1413s | Batch:  891 | Train loss: 0.5749 | Val loss: 28.7450\n",
      "   % Time: 1429s | Batch:  901 | Train loss: 0.4356 | Val loss: 30.0665\n",
      "   % Time: 1445s | Batch:  911 | Train loss: 0.4291 | Val loss: 29.7926\n",
      "   % Time: 1461s | Batch:  921 | Train loss: 0.4250 | Val loss: 26.0559\n",
      "   % Time: 1476s | Batch:  931 | Train loss: 0.4855 | Val loss: 31.1965\n",
      "   % Time: 1492s | Batch:  941 | Train loss: 0.4588 | Val loss: 27.5144\n",
      "   % Time: 1508s | Batch:  951 | Train loss: 0.3997 | Val loss: 28.3364\n",
      "   % Time: 1524s | Batch:  961 | Train loss: 0.4609 | Val loss: 29.6783\n",
      "   % Time: 1540s | Batch:  971 | Train loss: 0.4218 | Val loss: 30.8152\n",
      "   % Time: 1556s | Batch:  981 | Train loss: 0.4551 | Val loss: 31.1337\n",
      "   % Time: 1572s | Batch:  991 | Train loss: 0.4478 | Val loss: 33.7817\n",
      "   % Time: 1588s | Batch: 1001 | Train loss: 0.4750 | Val loss: 31.4351\n",
      "   % Time: 1603s | Batch: 1011 | Train loss: 0.4695 | Val loss: 32.4168\n",
      "   % Time: 1619s | Batch: 1021 | Train loss: 0.4181 | Val loss: 26.7625\n",
      "   % Time: 1635s | Batch: 1031 | Train loss: 0.4924 | Val loss: 31.9184\n",
      "   % Time: 1650s | Batch: 1041 | Train loss: 0.4456 | Val loss: 32.1656\n",
      "   % Time: 1666s | Batch: 1051 | Train loss: 0.4466 | Val loss: 31.0820\n",
      "   % Time: 1682s | Batch: 1061 | Train loss: 0.4226 | Val loss: 31.8157\n",
      "   % Time: 1697s | Batch: 1071 | Train loss: 0.4303 | Val loss: 32.0249\n",
      "   % Time: 1713s | Batch: 1081 | Train loss: 0.4411 | Val loss: 30.8791\n",
      "   % Time: 1729s | Batch: 1091 | Train loss: 0.4456 | Val loss: 34.7154\n",
      "   % Time: 1744s | Batch: 1101 | Train loss: 0.5013 | Val loss: 29.2353\n",
      "   % Time: 1760s | Batch: 1111 | Train loss: 0.4494 | Val loss: 29.2648\n",
      "   % Time: 1776s | Batch: 1121 | Train loss: 0.4521 | Val loss: 29.9676\n",
      "   % Time: 1791s | Batch: 1131 | Train loss: 0.4236 | Val loss: 30.4148\n",
      "   % Train loss 0.4472\n",
      "=> EPOCH 3\n",
      "   % Time:    1s | Batch:    1 | Train loss: 0.4291 | Val loss: 30.2028\n",
      "   % Time:   17s | Batch:   11 | Train loss: 0.4441 | Val loss: 31.2365\n",
      "   % Time:   33s | Batch:   21 | Train loss: 0.4519 | Val loss: 30.5766\n",
      "   % Time:   48s | Batch:   31 | Train loss: 0.4637 | Val loss: 33.4052\n",
      "   % Time:   64s | Batch:   41 | Train loss: 0.4531 | Val loss: 27.8635\n",
      "   % Time:   80s | Batch:   51 | Train loss: 0.4686 | Val loss: 32.5248\n",
      "   % Time:   96s | Batch:   61 | Train loss: 0.4831 | Val loss: 31.5098\n",
      "   % Time:  112s | Batch:   71 | Train loss: 0.4321 | Val loss: 26.6361\n",
      "   % Time:  127s | Batch:   81 | Train loss: 0.4686 | Val loss: 29.1663\n",
      "   % Time:  143s | Batch:   91 | Train loss: 0.4820 | Val loss: 31.2988\n",
      "   % Time:  159s | Batch:  101 | Train loss: 0.4133 | Val loss: 27.9559\n",
      "   % Time:  175s | Batch:  111 | Train loss: 0.4202 | Val loss: 27.2910\n",
      "   % Time:  191s | Batch:  121 | Train loss: 0.4261 | Val loss: 31.5635\n",
      "   % Time:  207s | Batch:  131 | Train loss: 0.4612 | Val loss: 28.8617\n",
      "   % Time:  223s | Batch:  141 | Train loss: 0.4334 | Val loss: 28.7483\n",
      "   % Time:  239s | Batch:  151 | Train loss: 0.4103 | Val loss: 32.4398\n",
      "   % Time:  255s | Batch:  161 | Train loss: 0.4685 | Val loss: 33.5900\n",
      "   % Time:  271s | Batch:  171 | Train loss: 0.4376 | Val loss: 31.7721\n",
      "   % Time:  287s | Batch:  181 | Train loss: 0.4632 | Val loss: 26.4061\n",
      "   % Time:  303s | Batch:  191 | Train loss: 0.4108 | Val loss: 29.8218\n",
      "   % Time:  319s | Batch:  201 | Train loss: 0.4323 | Val loss: 26.9741\n",
      "   % Time:  335s | Batch:  211 | Train loss: 0.4381 | Val loss: 27.5509\n",
      "   % Time:  351s | Batch:  221 | Train loss: 0.4099 | Val loss: 28.6923\n",
      "   % Time:  367s | Batch:  231 | Train loss: 0.4552 | Val loss: 28.8006\n",
      "   % Time:  383s | Batch:  241 | Train loss: 0.4328 | Val loss: 28.1235\n",
      "   % Time:  399s | Batch:  251 | Train loss: 0.4943 | Val loss: 28.6496\n",
      "   % Time:  415s | Batch:  261 | Train loss: 0.4227 | Val loss: 26.0680\n",
      "   % Time:  430s | Batch:  271 | Train loss: 0.4375 | Val loss: 29.3125\n",
      "   % Time:  446s | Batch:  281 | Train loss: 0.4274 | Val loss: 31.0625\n",
      "   % Time:  462s | Batch:  291 | Train loss: 0.4839 | Val loss: 30.1661\n",
      "   % Time:  478s | Batch:  301 | Train loss: 0.4139 | Val loss: 29.9173\n",
      "   % Time:  494s | Batch:  311 | Train loss: 0.4488 | Val loss: 27.4989\n",
      "   % Time:  510s | Batch:  321 | Train loss: 0.4441 | Val loss: 29.1001\n",
      "   % Time:  526s | Batch:  331 | Train loss: 0.4116 | Val loss: 30.2949\n",
      "   % Time:  542s | Batch:  341 | Train loss: 0.4444 | Val loss: 31.3150\n",
      "   % Time:  558s | Batch:  351 | Train loss: 0.4644 | Val loss: 30.7792\n",
      "   % Time:  573s | Batch:  361 | Train loss: 0.4568 | Val loss: 31.8236\n",
      "   % Time:  589s | Batch:  371 | Train loss: 0.4209 | Val loss: 27.8807\n",
      "   % Time:  605s | Batch:  381 | Train loss: 0.4335 | Val loss: 34.5023\n",
      "   % Time:  621s | Batch:  391 | Train loss: 0.4325 | Val loss: 29.4638\n",
      "   % Time:  637s | Batch:  401 | Train loss: 0.4954 | Val loss: 30.3557\n",
      "   % Time:  652s | Batch:  411 | Train loss: 0.4413 | Val loss: 27.5915\n",
      "   % Time:  668s | Batch:  421 | Train loss: 0.4123 | Val loss: 30.4934\n",
      "   % Time:  684s | Batch:  431 | Train loss: 0.5430 | Val loss: 31.1808\n",
      "   % Time:  700s | Batch:  441 | Train loss: 0.4571 | Val loss: 30.3257\n",
      "   % Time:  715s | Batch:  451 | Train loss: 0.4222 | Val loss: 26.4922\n",
      "   % Time:  731s | Batch:  461 | Train loss: 0.3910 | Val loss: 28.9415\n",
      "   % Time:  747s | Batch:  471 | Train loss: 0.4357 | Val loss: 32.4598\n",
      "   % Time:  763s | Batch:  481 | Train loss: 0.3999 | Val loss: 29.0383\n",
      "   % Time:  779s | Batch:  491 | Train loss: 0.4171 | Val loss: 27.0570\n",
      "   % Time:  794s | Batch:  501 | Train loss: 0.4528 | Val loss: 27.4667\n",
      "   % Time:  810s | Batch:  511 | Train loss: 0.4577 | Val loss: 32.4157\n",
      "   % Time:  826s | Batch:  521 | Train loss: 0.4625 | Val loss: 33.5901\n",
      "   % Time:  842s | Batch:  531 | Train loss: 0.4155 | Val loss: 30.9645\n",
      "   % Time:  858s | Batch:  541 | Train loss: 0.4701 | Val loss: 27.8126\n",
      "   % Time:  874s | Batch:  551 | Train loss: 0.4390 | Val loss: 29.2538\n",
      "   % Time:  890s | Batch:  561 | Train loss: 0.4636 | Val loss: 29.2012\n",
      "   % Time:  906s | Batch:  571 | Train loss: 0.4016 | Val loss: 27.9039\n",
      "   % Time:  922s | Batch:  581 | Train loss: 0.4422 | Val loss: 30.9846\n",
      "   % Time:  938s | Batch:  591 | Train loss: 0.4485 | Val loss: 29.7058\n",
      "   % Time:  954s | Batch:  601 | Train loss: 0.4519 | Val loss: 36.7225\n",
      "   % Time:  970s | Batch:  611 | Train loss: 0.4602 | Val loss: 32.1666\n",
      "   % Time:  985s | Batch:  621 | Train loss: 0.4484 | Val loss: 33.4031\n",
      "   % Time: 1001s | Batch:  631 | Train loss: 0.4545 | Val loss: 28.0509\n",
      "   % Time: 1017s | Batch:  641 | Train loss: 0.4219 | Val loss: 25.7889\n",
      "   % Time: 1032s | Batch:  651 | Train loss: 0.4466 | Val loss: 31.3172\n",
      "   % Time: 1048s | Batch:  661 | Train loss: 0.4415 | Val loss: 33.6174\n",
      "   % Time: 1064s | Batch:  671 | Train loss: 0.4262 | Val loss: 31.3313\n",
      "   % Time: 1080s | Batch:  681 | Train loss: 0.3863 | Val loss: 29.6971\n",
      "   % Time: 1096s | Batch:  691 | Train loss: 0.4781 | Val loss: 30.6733\n",
      "   % Time: 1111s | Batch:  701 | Train loss: 0.4462 | Val loss: 31.3456\n",
      "   % Time: 1127s | Batch:  711 | Train loss: 0.4452 | Val loss: 32.7468\n",
      "   % Time: 1143s | Batch:  721 | Train loss: 0.4311 | Val loss: 31.9633\n",
      "   % Time: 1159s | Batch:  731 | Train loss: 0.4485 | Val loss: 29.8109\n",
      "   % Time: 1175s | Batch:  741 | Train loss: 0.4630 | Val loss: 32.5503\n",
      "   % Time: 1190s | Batch:  751 | Train loss: 0.4502 | Val loss: 28.5469\n",
      "   % Time: 1206s | Batch:  761 | Train loss: 0.4579 | Val loss: 31.1858\n",
      "   % Time: 1222s | Batch:  771 | Train loss: 0.4418 | Val loss: 32.8549\n",
      "   % Time: 1238s | Batch:  781 | Train loss: 0.4745 | Val loss: 31.0206\n",
      "   % Time: 1254s | Batch:  791 | Train loss: 0.3755 | Val loss: 29.5515\n",
      "   % Time: 1270s | Batch:  801 | Train loss: 0.4345 | Val loss: 27.6106\n",
      "   % Time: 1286s | Batch:  811 | Train loss: 0.4397 | Val loss: 29.5561\n",
      "   % Time: 1301s | Batch:  821 | Train loss: 0.4395 | Val loss: 29.3282\n",
      "   % Time: 1317s | Batch:  831 | Train loss: 0.4490 | Val loss: 27.6718\n",
      "   % Time: 1333s | Batch:  841 | Train loss: 0.4550 | Val loss: 29.6969\n",
      "   % Time: 1348s | Batch:  851 | Train loss: 0.4654 | Val loss: 31.7679\n",
      "   % Time: 1364s | Batch:  861 | Train loss: 0.4418 | Val loss: 27.6335\n",
      "   % Time: 1380s | Batch:  871 | Train loss: 0.4520 | Val loss: 30.6883\n",
      "   % Time: 1396s | Batch:  881 | Train loss: 0.4599 | Val loss: 30.9400\n",
      "   % Time: 1412s | Batch:  891 | Train loss: 0.4554 | Val loss: 34.3134\n",
      "   % Time: 1428s | Batch:  901 | Train loss: 0.4643 | Val loss: 32.8638\n",
      "   % Time: 1444s | Batch:  911 | Train loss: 0.5116 | Val loss: 30.8301\n",
      "   % Time: 1460s | Batch:  921 | Train loss: 0.4326 | Val loss: 29.1177\n",
      "   % Time: 1476s | Batch:  931 | Train loss: 0.4493 | Val loss: 29.7420\n",
      "   % Time: 1492s | Batch:  941 | Train loss: 0.4182 | Val loss: 27.4124\n",
      "   % Time: 1508s | Batch:  951 | Train loss: 0.4546 | Val loss: 33.6759\n",
      "   % Time: 1524s | Batch:  961 | Train loss: 0.4280 | Val loss: 31.8613\n",
      "   % Time: 1540s | Batch:  971 | Train loss: 0.4351 | Val loss: 31.2079\n",
      "   % Time: 1556s | Batch:  981 | Train loss: 0.4510 | Val loss: 28.9097\n",
      "   % Time: 1572s | Batch:  991 | Train loss: 0.4388 | Val loss: 29.5730\n",
      "   % Time: 1588s | Batch: 1001 | Train loss: 0.4449 | Val loss: 28.9636\n",
      "   % Time: 1604s | Batch: 1011 | Train loss: 0.4555 | Val loss: 31.8801\n",
      "   % Time: 1620s | Batch: 1021 | Train loss: 0.4328 | Val loss: 27.9615\n",
      "   % Time: 1636s | Batch: 1031 | Train loss: 0.4255 | Val loss: 28.7537\n",
      "   % Time: 1651s | Batch: 1041 | Train loss: 0.4452 | Val loss: 30.8842\n",
      "   % Time: 1667s | Batch: 1051 | Train loss: 0.4465 | Val loss: 28.3270\n",
      "   % Time: 1683s | Batch: 1061 | Train loss: 0.4006 | Val loss: 28.7401\n",
      "   % Time: 1699s | Batch: 1071 | Train loss: 0.4353 | Val loss: 28.7847\n",
      "   % Time: 1715s | Batch: 1081 | Train loss: 0.4482 | Val loss: 28.8081\n",
      "   % Time: 1731s | Batch: 1091 | Train loss: 0.4116 | Val loss: 30.0351\n",
      "   % Time: 1747s | Batch: 1101 | Train loss: 0.4425 | Val loss: 28.7441\n",
      "   % Time: 1762s | Batch: 1111 | Train loss: 0.4662 | Val loss: 30.8629\n",
      "   % Time: 1778s | Batch: 1121 | Train loss: 0.4095 | Val loss: 27.9022\n",
      "   % Time: 1794s | Batch: 1131 | Train loss: 0.4161 | Val loss: 27.8894\n",
      "   % Train loss 0.4466\n",
      "=> EPOCH 4\n",
      "   % Time:    1s | Batch:    1 | Train loss: 0.4876 | Val loss: 31.3974\n",
      "   % Time:   17s | Batch:   11 | Train loss: 0.4774 | Val loss: 30.6006\n",
      "   % Time:   33s | Batch:   21 | Train loss: 0.4570 | Val loss: 30.5100\n",
      "   % Time:   49s | Batch:   31 | Train loss: 0.4070 | Val loss: 26.0292\n",
      "   % Time:   65s | Batch:   41 | Train loss: 0.4457 | Val loss: 30.6008\n",
      "   % Time:   81s | Batch:   51 | Train loss: 0.4440 | Val loss: 31.0770\n",
      "   % Time:   97s | Batch:   61 | Train loss: 0.4261 | Val loss: 28.2864\n",
      "   % Time:  112s | Batch:   71 | Train loss: 0.4075 | Val loss: 32.8133\n",
      "   % Time:  128s | Batch:   81 | Train loss: 0.4219 | Val loss: 30.6506\n",
      "   % Time:  144s | Batch:   91 | Train loss: 0.4390 | Val loss: 28.8943\n",
      "   % Time:  160s | Batch:  101 | Train loss: 0.4298 | Val loss: 31.6338\n",
      "   % Time:  176s | Batch:  111 | Train loss: 0.4400 | Val loss: 32.3603\n",
      "   % Time:  192s | Batch:  121 | Train loss: 0.4577 | Val loss: 27.4028\n",
      "   % Time:  208s | Batch:  131 | Train loss: 0.5028 | Val loss: 32.4937\n",
      "   % Time:  224s | Batch:  141 | Train loss: 0.4867 | Val loss: 32.5061\n",
      "   % Time:  240s | Batch:  151 | Train loss: 0.4018 | Val loss: 25.4131\n",
      "   % Time:  256s | Batch:  161 | Train loss: 0.4638 | Val loss: 32.8584\n",
      "   % Time:  272s | Batch:  171 | Train loss: 0.4790 | Val loss: 27.2155\n",
      "   % Time:  288s | Batch:  181 | Train loss: 0.4379 | Val loss: 28.3438\n",
      "   % Time:  304s | Batch:  191 | Train loss: 0.4200 | Val loss: 25.0577\n",
      "   % Time:  319s | Batch:  201 | Train loss: 0.4259 | Val loss: 27.2143\n",
      "   % Time:  335s | Batch:  211 | Train loss: 0.4720 | Val loss: 31.8307\n",
      "   % Time:  351s | Batch:  221 | Train loss: 0.5404 | Val loss: 30.3025\n",
      "   % Time:  367s | Batch:  231 | Train loss: 0.4068 | Val loss: 26.0969\n",
      "   % Time:  383s | Batch:  241 | Train loss: 0.4557 | Val loss: 25.8142\n",
      "   % Time:  398s | Batch:  251 | Train loss: 0.4506 | Val loss: 27.0300\n",
      "   % Time:  414s | Batch:  261 | Train loss: 0.4357 | Val loss: 27.7097\n",
      "   % Time:  430s | Batch:  271 | Train loss: 0.4022 | Val loss: 29.3586\n",
      "   % Time:  446s | Batch:  281 | Train loss: 0.4876 | Val loss: 31.0099\n",
      "   % Time:  462s | Batch:  291 | Train loss: 0.4849 | Val loss: 29.7495\n",
      "   % Time:  478s | Batch:  301 | Train loss: 0.4637 | Val loss: 33.2661\n",
      "   % Time:  494s | Batch:  311 | Train loss: 0.3852 | Val loss: 24.2855\n",
      "   % Time:  510s | Batch:  321 | Train loss: 0.4170 | Val loss: 30.6657\n",
      "   % Time:  526s | Batch:  331 | Train loss: 0.4489 | Val loss: 30.5571\n",
      "   % Time:  542s | Batch:  341 | Train loss: 0.4416 | Val loss: 29.3314\n",
      "   % Time:  558s | Batch:  351 | Train loss: 0.4520 | Val loss: 28.5631\n",
      "   % Time:  574s | Batch:  361 | Train loss: 0.4439 | Val loss: 30.8476\n",
      "   % Time:  590s | Batch:  371 | Train loss: 0.4618 | Val loss: 31.9670\n",
      "   % Time:  606s | Batch:  381 | Train loss: 0.4385 | Val loss: 31.0216\n",
      "   % Time:  622s | Batch:  391 | Train loss: 0.4628 | Val loss: 30.7534\n",
      "   % Time:  638s | Batch:  401 | Train loss: 0.4219 | Val loss: 32.6519\n",
      "   % Time:  654s | Batch:  411 | Train loss: 0.4177 | Val loss: 27.9197\n",
      "   % Time:  670s | Batch:  421 | Train loss: 0.4541 | Val loss: 30.6238\n",
      "   % Time:  686s | Batch:  431 | Train loss: 0.5020 | Val loss: 28.9925\n",
      "   % Time:  702s | Batch:  441 | Train loss: 0.4542 | Val loss: 29.7865\n",
      "   % Time:  718s | Batch:  451 | Train loss: 0.4428 | Val loss: 35.3166\n",
      "   % Time:  734s | Batch:  461 | Train loss: 0.4555 | Val loss: 29.5697\n",
      "   % Time:  750s | Batch:  471 | Train loss: 0.4539 | Val loss: 30.8285\n",
      "   % Time:  766s | Batch:  481 | Train loss: 0.4399 | Val loss: 30.5567\n",
      "   % Time:  781s | Batch:  491 | Train loss: 0.4595 | Val loss: 28.9788\n",
      "   % Time:  797s | Batch:  501 | Train loss: 0.4701 | Val loss: 32.5834\n",
      "   % Time:  813s | Batch:  511 | Train loss: 0.4287 | Val loss: 28.3798\n",
      "   % Time:  829s | Batch:  521 | Train loss: 0.4474 | Val loss: 30.3274\n",
      "   % Time:  845s | Batch:  531 | Train loss: 0.4363 | Val loss: 29.7242\n",
      "   % Time:  861s | Batch:  541 | Train loss: 0.4396 | Val loss: 31.5946\n",
      "   % Time:  877s | Batch:  551 | Train loss: 0.4983 | Val loss: 29.5316\n",
      "   % Time:  892s | Batch:  561 | Train loss: 0.4309 | Val loss: 28.9834\n",
      "   % Time:  908s | Batch:  571 | Train loss: 0.4347 | Val loss: 29.6422\n",
      "   % Time:  924s | Batch:  581 | Train loss: 0.4144 | Val loss: 27.4818\n",
      "   % Time:  940s | Batch:  591 | Train loss: 0.4610 | Val loss: 28.9024\n",
      "   % Time:  956s | Batch:  601 | Train loss: 0.4629 | Val loss: 31.1062\n",
      "   % Time:  972s | Batch:  611 | Train loss: 0.4880 | Val loss: 33.7204\n",
      "   % Time:  988s | Batch:  621 | Train loss: 0.4177 | Val loss: 27.0742\n",
      "   % Time: 1004s | Batch:  631 | Train loss: 0.4462 | Val loss: 31.1620\n",
      "   % Time: 1020s | Batch:  641 | Train loss: 0.4369 | Val loss: 30.3522\n",
      "   % Time: 1035s | Batch:  651 | Train loss: 0.4363 | Val loss: 28.9403\n",
      "   % Time: 1051s | Batch:  661 | Train loss: 0.4860 | Val loss: 32.4832\n",
      "   % Time: 1067s | Batch:  671 | Train loss: 0.4260 | Val loss: 29.2299\n",
      "   % Time: 1083s | Batch:  681 | Train loss: 0.4502 | Val loss: 33.6423\n",
      "   % Time: 1099s | Batch:  691 | Train loss: 0.4749 | Val loss: 29.1184\n",
      "   % Time: 1115s | Batch:  701 | Train loss: 0.4657 | Val loss: 32.5896\n",
      "   % Time: 1131s | Batch:  711 | Train loss: 0.4896 | Val loss: 35.3312\n",
      "   % Time: 1147s | Batch:  721 | Train loss: 0.4262 | Val loss: 30.4469\n",
      "   % Time: 1163s | Batch:  731 | Train loss: 0.3995 | Val loss: 27.5728\n",
      "   % Time: 1179s | Batch:  741 | Train loss: 0.4187 | Val loss: 26.6966\n",
      "   % Time: 1194s | Batch:  751 | Train loss: 0.4698 | Val loss: 32.1279\n",
      "   % Time: 1210s | Batch:  761 | Train loss: 0.4363 | Val loss: 26.1586\n",
      "   % Time: 1226s | Batch:  771 | Train loss: 0.4519 | Val loss: 30.3299\n",
      "   % Time: 1242s | Batch:  781 | Train loss: 0.4322 | Val loss: 29.1035\n",
      "   % Time: 1258s | Batch:  791 | Train loss: 0.4761 | Val loss: 29.7801\n",
      "   % Time: 1274s | Batch:  801 | Train loss: 0.4203 | Val loss: 33.2475\n",
      "   % Time: 1290s | Batch:  811 | Train loss: 0.4300 | Val loss: 30.5389\n",
      "   % Time: 1306s | Batch:  821 | Train loss: 0.4037 | Val loss: 28.8006\n",
      "   % Time: 1322s | Batch:  831 | Train loss: 0.4354 | Val loss: 28.2945\n",
      "   % Time: 1338s | Batch:  841 | Train loss: 0.4516 | Val loss: 31.2952\n",
      "   % Time: 1354s | Batch:  851 | Train loss: 0.4503 | Val loss: 28.5179\n",
      "   % Time: 1370s | Batch:  861 | Train loss: 0.4540 | Val loss: 29.9156\n",
      "   % Time: 1386s | Batch:  871 | Train loss: 0.4426 | Val loss: 33.3456\n",
      "   % Time: 1402s | Batch:  881 | Train loss: 0.4089 | Val loss: 27.8028\n",
      "   % Time: 1418s | Batch:  891 | Train loss: 0.4166 | Val loss: 29.2038\n",
      "   % Time: 1434s | Batch:  901 | Train loss: 0.4262 | Val loss: 29.8894\n",
      "   % Time: 1450s | Batch:  911 | Train loss: 0.4217 | Val loss: 29.4024\n",
      "   % Time: 1466s | Batch:  921 | Train loss: 0.4565 | Val loss: 29.4320\n",
      "   % Time: 1482s | Batch:  931 | Train loss: 0.4459 | Val loss: 30.3823\n",
      "   % Time: 1498s | Batch:  941 | Train loss: 0.4508 | Val loss: 28.1521\n",
      "   % Time: 1513s | Batch:  951 | Train loss: 0.4451 | Val loss: 31.5424\n",
      "   % Time: 1529s | Batch:  961 | Train loss: 0.4270 | Val loss: 28.7485\n",
      "   % Time: 1545s | Batch:  971 | Train loss: 0.4549 | Val loss: 29.6217\n",
      "   % Time: 1561s | Batch:  981 | Train loss: 0.4765 | Val loss: 30.8128\n",
      "   % Time: 1577s | Batch:  991 | Train loss: 0.4288 | Val loss: 26.6950\n",
      "   % Time: 1593s | Batch: 1001 | Train loss: 0.4636 | Val loss: 29.8196\n",
      "   % Time: 1609s | Batch: 1011 | Train loss: 0.4391 | Val loss: 31.6831\n",
      "   % Time: 1625s | Batch: 1021 | Train loss: 0.4378 | Val loss: 27.7073\n",
      "   % Time: 1641s | Batch: 1031 | Train loss: 0.4417 | Val loss: 32.5664\n",
      "   % Time: 1657s | Batch: 1041 | Train loss: 0.4396 | Val loss: 31.0946\n",
      "   % Time: 1673s | Batch: 1051 | Train loss: 0.4290 | Val loss: 29.8455\n",
      "   % Time: 1689s | Batch: 1061 | Train loss: 0.4220 | Val loss: 30.6979\n",
      "   % Time: 1704s | Batch: 1071 | Train loss: 0.4141 | Val loss: 27.8304\n",
      "   % Time: 1720s | Batch: 1081 | Train loss: 0.4091 | Val loss: 26.7727\n",
      "   % Time: 1737s | Batch: 1091 | Train loss: 0.4722 | Val loss: 30.4721\n",
      "   % Time: 1753s | Batch: 1101 | Train loss: 0.4186 | Val loss: 30.9958\n",
      "   % Time: 1769s | Batch: 1111 | Train loss: 0.4389 | Val loss: 25.7540\n",
      "   % Time: 1785s | Batch: 1121 | Train loss: 0.4905 | Val loss: 29.9354\n",
      "   % Time: 1801s | Batch: 1131 | Train loss: 0.4472 | Val loss: 29.6908\n",
      "   % Train loss 0.4461\n"
     ]
    }
   ],
   "source": [
    "if args.train:\n",
    "    for epoch in range(1, args.n_epoch+1):\n",
    "        print(\"=> EPOCH {}\".format(epoch))\n",
    "        scheduler.step()\n",
    "        train_loss = train(raw_data, scaled_data, scaler, features,\n",
    "                           model, criterion, optimizer)\n",
    "        print(\"   % Train loss {:.4f}\".format(train_loss))\n",
    "#        save_model(model, epoch, val_loss)\n",
    "else:\n",
    "    model_file = os.path.join(args.intermediate_path, args.model_name)\n",
    "    model.load_state_dict(torch.load(model_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-01T06:06:15.319233Z",
     "start_time": "2017-09-01T14:54:15.616192+09:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "prediction = forecast(scaled_data, scaler, features, model)\n",
    "prediction_file = os.path.join(args.intermediate_path,\n",
    "                               'prediction_seed{}.pkl'.format(args.seed))\n",
    "\n",
    "with open(prediction_file, 'wb') as f:\n",
    "    pickle.dump(prediction, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pydata]",
   "language": "python",
   "name": "conda-env-pydata-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
