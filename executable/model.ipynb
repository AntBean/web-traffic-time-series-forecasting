{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-27T15:54:40.051240Z",
     "start_time": "2017-08-28T00:54:38.986872+09:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import numpy as np; np.seterr(invalid='ignore')\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-27T15:54:40.103202Z",
     "start_time": "2017-08-28T00:54:40.054039+09:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "parser = {\n",
    "    'data_path': '../data/wttsf/',\n",
    "    'train_file': 'train_1.csv',\n",
    "    'key_file': 'key_1.csv',\n",
    "    'intermediate_path': '../intermediate/',\n",
    "    'n_epoch': 10,\n",
    "    'future': 70,\n",
    "    'batch_size': 128,\n",
    "    'hidden_size': 256,\n",
    "    'log_every': 10,\n",
    "    'read_from_file': True,\n",
    "    'cuda': True,\n",
    "    'seed': 28082017,\n",
    "}\n",
    "args = argparse.Namespace(**parser)\n",
    "\n",
    "args.cuda = args.cuda and torch.cuda.is_available()\n",
    "torch.manual_seed(args.seed);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-27T15:54:40.403503Z",
     "start_time": "2017-08-28T00:54:40.105537+09:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class DenseLSTMForecast(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(DenseLSTMForecast, self).__init__()\n",
    "        self.lstm1 = nn.LSTMCell(1, hidden_size, bias=False)\n",
    "        self.lstm2 = nn.LSTMCell(hidden_size+1, hidden_size, bias=False)\n",
    "        self.lstm3 = nn.LSTMCell(2*hidden_size+1, hidden_size, bias=False)\n",
    "        self.lstm4 = nn.LSTMCell(3*hidden_size+1, hidden_size, bias=False)\n",
    "        self.linear = nn.Linear(4*hidden_size+1, 1, bias=False)\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    def forward(self, x, future=1):\n",
    "        o = []\n",
    "        tt = torch.cuda if args.cuda else torch\n",
    "        h1_t = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())\n",
    "        c1_t = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())\n",
    "        h2_t = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())\n",
    "        c2_t = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())\n",
    "        h3_t = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())\n",
    "        c3_t = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())\n",
    "        h4_t = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())\n",
    "        c4_t = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())\n",
    "        \n",
    "        for x_t in x.chunk(x.size(1), dim=1):\n",
    "            x_t = x_t.squeeze(dim=1)\n",
    "            h1_t, c1_t = self.lstm1(x_t, (h1_t, c1_t))\n",
    "            h1d_t = torch.cat([x_t, h1_t], dim=1)\n",
    "            h2_t, c2_t = self.lstm2(h1d_t, (h2_t, c2_t))\n",
    "            h2d_t = torch.cat([x_t, h1_t, h2_t], dim=1)\n",
    "            h3_t, c3_t = self.lstm3(h2d_t, (h3_t, c3_t))\n",
    "            h3d_t = torch.cat([x_t, h1_t, h2_t, h3_t], dim=1)\n",
    "            h4_t, c4_t = self.lstm4(h3d_t, (h4_t, c4_t))\n",
    "            h4d_t = torch.cat([x_t, h1_t, h2_t, h3_t, h4_t], dim=1)\n",
    "            o_t = self.linear(h4d_t)\n",
    "            o.append(o_t)\n",
    "            \n",
    "        for i in range(future-1):\n",
    "            h1_t, c1_t = self.lstm1(o_t, (h1_t, c1_t))\n",
    "            h1d_t = torch.cat([x_t, h1_t], dim=1)\n",
    "            h2_t, c2_t = self.lstm2(h1d_t, (h2_t, c2_t))\n",
    "            h2d_t = torch.cat([x_t, h1_t, h2_t], dim=1)\n",
    "            h3_t, c3_t = self.lstm3(h2d_t, (h3_t, c3_t))\n",
    "            h3d_t = torch.cat([x_t, h1_t, h2_t, h3_t], dim=1)\n",
    "            h4_t, c4_t = self.lstm4(h3d_t, (h4_t, c4_t))\n",
    "            h4d_t = torch.cat([x_t, h1_t, h2_t, h3_t, h4_t], dim=1)\n",
    "            o_t = self.linear(h4d_t)\n",
    "            o.append(o_t)\n",
    "\n",
    "        return torch.stack(o, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-27T15:54:40.466006Z",
     "start_time": "2017-08-28T00:54:40.405850+09:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def smape(y_pred, y_true):\n",
    "    raw_smape = np.abs(y_true - y_pred) / (np.abs(y_true) + np.abs(y_pred))\n",
    "    kaggle_smape = np.nan_to_num(raw_smape)\n",
    "    return np.mean(kaggle_smape) * 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-27T15:54:40.537797Z",
     "start_time": "2017-08-28T00:54:40.468399+09:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_scaled_data():\n",
    "    raw_data_file = os.path.join(args.intermediate_path,\n",
    "                                 'raw_data.pkl')\n",
    "    scaled_data_file = os.path.join(args.intermediate_path,\n",
    "                                    'scaled_data.pkl')\n",
    "    scaler_file = os.path.join(args.intermediate_path, 'scaler.pkl')\n",
    "    if not args.read_from_file:\n",
    "        data_df = pd.read_csv(os.path.join(args.data_path, args.train_file),\n",
    "                              index_col='Page')\n",
    "        raw_data = np.nan_to_num(data_df.values.astype('float32'))\n",
    "        data = np.log1p(raw_data)\n",
    "        scaler = StandardScaler()\n",
    "        scaled_data = np.swapaxes(scaler.fit_transform(\n",
    "                np.swapaxes(data, 0, 1)), 0, 1)\n",
    "        \n",
    "        with open(raw_data_file, 'wb') as f:\n",
    "            pickle.dump(raw_data, f)\n",
    "        with open(scaled_data_file, 'wb') as f:\n",
    "            pickle.dump(scaled_data, f)\n",
    "        with open(scaler_file, 'wb') as f:\n",
    "            pickle.dump(scaler, f)\n",
    "    else:\n",
    "        with open(raw_data_file, 'rb') as f:\n",
    "            raw_data = pickle.load(f)\n",
    "        with open(scaled_data_file, 'rb') as f:\n",
    "            scaled_data = pickle.load(f)\n",
    "        with open(scaler_file, 'rb') as f:\n",
    "            scaler = pickle.load(f)\n",
    "    return raw_data, scaled_data, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-27T15:54:40.657076Z",
     "start_time": "2017-08-28T00:54:40.540223+09:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def train_val_split(raw_data, scaled_data, scaler):\n",
    "    train_file = os.path.join(args.intermediate_path,\n",
    "                              'train_{}.pkl'.format(args.seed))\n",
    "    val_file = os.path.join(args.intermediate_path,\n",
    "                            'val_{}.pkl'.format(args.seed))\n",
    "    raw_val_file = os.path.join(args.intermediate_path,\n",
    "                                'raw_val_{}.pkl'.format(args.seed))\n",
    "    val_scaler_file = os.path.join(args.intermediate_path,\n",
    "                                   'val_scaler_{}.pkl'.format(args.seed))\n",
    "    if not args.read_from_file:\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=args.seed)\n",
    "        train_index, val_index = next(kf.split(scaled_data))\n",
    "        train = scaled_data[train_index]\n",
    "        val = scaled_data[val_index]\n",
    "        raw_val = raw_data[val_index]\n",
    "        val_scaler = StandardScaler()\n",
    "        val_scaler.mean_ = scaler.mean_[val_index]\n",
    "        val_scaler.scale_ = scaler.scale_[val_index]\n",
    "        \n",
    "        with open(train_file, 'wb') as f:\n",
    "            pickle.dump(train, f)\n",
    "        with open(val_file, 'wb') as f:\n",
    "            pickle.dump(val, f)\n",
    "        with open(raw_val_file, 'wb') as f:\n",
    "            pickle.dump(raw_val, f)\n",
    "        with open(val_scaler_file, 'wb') as f:\n",
    "            pickle.dump(val_scaler, f)\n",
    "    else:\n",
    "        with open(train_file, 'rb') as f:\n",
    "            train = pickle.load(f)\n",
    "        with open(val_file, 'rb') as f:\n",
    "            val = pickle.load(f)\n",
    "        with open(raw_val_file, 'rb') as f:\n",
    "            raw_val = pickle.load(f)\n",
    "        with open(val_scaler_file, 'rb') as f:\n",
    "            val_scaler = pickle.load(f)\n",
    "    return train, val, raw_val, val_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-27T15:54:40.707676Z",
     "start_time": "2017-08-28T00:54:40.659500+09:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def input_target_split(data, raw_data=None):\n",
    "    inputt = data[:, :-1]\n",
    "    if raw_data is None:\n",
    "        target = data[:, 1:]\n",
    "    else:\n",
    "        target = raw_data[:, 1:]\n",
    "    return inputt, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-27T15:54:40.758548Z",
     "start_time": "2017-08-28T00:54:40.710357+09:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer):\n",
    "    n_total = train_loss = 0\n",
    "    init_time = time.time()\n",
    "\n",
    "    for i, (inputt, target) in enumerate(train_loader):\n",
    "        if args.cuda:\n",
    "            inputt = inputt.cuda()\n",
    "            target = target.cuda()\n",
    "        inputt = Variable(inputt)\n",
    "        target = Variable(target)\n",
    "        \n",
    "        output = model(inputt)\n",
    "        loss = criterion(output, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        n_total += inputt.size(0)\n",
    "        train_loss += loss.data[0] * inputt.size(0)\n",
    "        \n",
    "        if i % args.log_every == 0:\n",
    "            print(\"   % Time: {:4.0f}s | Batch: {:3} | Loss: {:.4f}\"\n",
    "                  .format(time.time()-init_time, i+1, loss.data[0]))\n",
    "        \n",
    "    train_loss /= n_total\n",
    "    print(\"=\"*10 + \"\\n   % Time: {:4.0f}s | Train loss: {:.4f}\"\n",
    "          .format(time.time()-init_time, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-27T15:54:40.810890Z",
     "start_time": "2017-08-28T00:54:40.760726+09:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def validate(val_loader, model, val_scaler, val_target):\n",
    "    init_time = time.time()\n",
    "    \n",
    "    output_list = []\n",
    "    for inputt, _ in val_loader:\n",
    "        if args.cuda:\n",
    "            inputt = inputt.cuda()\n",
    "        inputt = Variable(inputt)\n",
    "        \n",
    "        output = model(inputt)\n",
    "        output_list.append(output.data.squeeze(2).cpu().numpy())\n",
    "        \n",
    "    output_all = np.concatenate(output_list, axis=0)\n",
    "    prediction = np.swapaxes(val_scaler.inverse_transform(\n",
    "            np.swapaxes(output_all, 0, 1)), 0, 1)\n",
    "    prediction = np.exp(prediction) - 1\n",
    "    prediction[prediction < 0.5] = 0\n",
    "    \n",
    "    val_loss = smape(prediction, val_target)\n",
    "    print(\"   % Time: {:4.0f}s | Val loss: {:.4f}\"\n",
    "          .format(time.time()-init_time, val_loss))\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-27T15:54:40.855075Z",
     "start_time": "2017-08-28T00:54:40.813211+09:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def forecast(scaled_data, scaler, model):\n",
    "    full_tensor = torch.from_numpy(scaled_data).unsqueeze(2)\n",
    "    empty_tensor = torch.zeros(full_tensor.size(0))\n",
    "    full_dataset = torch.utils.data.TensorDataset(full_tensor, empty_tensor)\n",
    "    data_loader = DataLoader(full_dataset, args.batch_size, shuffle=False)\n",
    "    \n",
    "    output_list = []\n",
    "    for inputt, _ in data_loader:\n",
    "        if args.cuda:\n",
    "            inputt = inputt.cuda()\n",
    "        inputt = Variable(inputt)\n",
    "        output = model(inputt, args.future)\n",
    "        output_list.append(output.data.squeeze(2).cpu().numpy()\n",
    "                           [:, -args.future:])\n",
    "        \n",
    "    output_all = np.concatenate(output_list, axis=0)\n",
    "    prediction = np.swapaxes(scaler.inverse_transform(\n",
    "            np.swapaxes(output_all, 0, 1)), 0, 1)\n",
    "    prediction = np.exp(prediction) - 1\n",
    "    prediction[prediction < 0.5] = 0\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-27T15:54:40.913473Z",
     "start_time": "2017-08-28T00:54:40.857295+09:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def save_model(model, epoch, val_loss):\n",
    "    model_file = os.path.join(args.intermediate_path,\n",
    "                              \"model_seed{}_epoch{}_loss_{:.4f}.pth\"\n",
    "                              .format(args.seed, epoch, val_loss))\n",
    "    torch.save(model.state_dict(), os.path.join(model_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-27T15:55:04.653210Z",
     "start_time": "2017-08-28T00:54:40.915897+09:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# scale data\n",
    "raw_data, scaled_data, scaler = get_scaled_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-27T15:55:08.468551Z",
     "start_time": "2017-08-28T00:55:04.655967+09:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# train/val split\n",
    "train_data, val_data, raw_val_data, val_scaler = train_val_split(\n",
    "    raw_data, scaled_data, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-27T15:55:08.475546Z",
     "start_time": "2017-08-28T00:55:08.471353+09:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# input/target split\n",
    "train_input, train_target = input_target_split(train_data)\n",
    "val_input, val_target = input_target_split(val_data, raw_val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-27T15:55:09.830719Z",
     "start_time": "2017-08-28T00:55:08.477844+09:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# convert to tensors\n",
    "train_input_tensor = torch.from_numpy(train_input).unsqueeze(2)\n",
    "val_input_tensor = torch.from_numpy(val_input).unsqueeze(2)\n",
    "train_target_tensor = torch.from_numpy(train_target).unsqueeze(2)\n",
    "val_target_tensor = torch.zeros(val_input_tensor.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-27T16:01:10.916647Z",
     "start_time": "2017-08-28T01:01:10.910239+09:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make dataset, data loader\n",
    "train_dataset = TensorDataset(train_input_tensor, train_target_tensor)\n",
    "val_dataset = TensorDataset(val_input_tensor, val_target_tensor)\n",
    "train_loader = DataLoader(train_dataset, args.batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, args.batch_size//2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-27T15:55:12.179032Z",
     "start_time": "2017-08-28T00:55:09.833506+09:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# init model\n",
    "model = DenseLSTMForecast(args.hidden_size)\n",
    "if args.cuda:\n",
    "    model.cuda()\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "scheduler = MultiStepLR(optimizer, milestones=[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-08-27T16:01:16.940Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "=> EPOCH 1\n",
      "   % Time:    4s | Batch:   1 | Loss: 0.7561\n",
      "   % Time:   33s | Batch:  11 | Loss: 0.4923\n",
      "   % Time:   62s | Batch:  21 | Loss: 0.4512\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, args.n_epoch+1):\n",
    "    scheduler.step()\n",
    "    print(\"=\"*10 + \"\\n=> EPOCH {}\".format(epoch))\n",
    "    train(train_loader, model, criterion, optimizer)\n",
    "    val_loss = validate(val_loader, model, val_scaler, val_target)\n",
    "    save_model(model, epoch, val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-27T15:55:12.395505Z",
     "start_time": "2017-08-27T15:54:40.426Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "prediction = forecast(scaled_data, model, scaler)\n",
    "prediction_file = os.path.join(args.intermediate_path,\n",
    "                               'prediction_seed{}.pkl'.format(args.seed)))\n",
    "\n",
    "with open(prediction_file, 'wb') as f:\n",
    "    pickle.dump(prediction, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pydata]",
   "language": "python",
   "name": "conda-env-pydata-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
