{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-04T03:10:13.806779Z",
     "start_time": "2017-09-04T12:10:12.758395+09:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import numpy as np; np.seterr(invalid='ignore')\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-04T03:10:13.861371Z",
     "start_time": "2017-09-04T12:10:13.809499+09:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "parser = {\n",
    "    'data_path': '../data/wttsf/',\n",
    "    'train_file': 'train_1.csv',\n",
    "    'intermediate_path': '../intermediate/',\n",
    "    'n_epoch': 4,\n",
    "    'future': 73,\n",
    "    'batch_size': 128,\n",
    "    'hidden_size': 256,\n",
    "    'log_every': 10,\n",
    "    'read_from_file': False,\n",
    "    'train': True,\n",
    "    'model_name': '',\n",
    "    'cuda': True,\n",
    "    'seed': 20170903,\n",
    "}\n",
    "args = argparse.Namespace(**parser)\n",
    "\n",
    "args.cuda = args.cuda and torch.cuda.is_available()\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "args.intermediate_path = os.path.join(args.intermediate_path, str(args.seed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-04T03:10:14.049819Z",
     "start_time": "2017-09-04T12:10:13.864120+09:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class DenseLSTMForecast(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(DenseLSTMForecast, self).__init__()\n",
    "        self.lstm1 = nn.LSTMCell(15, hidden_size, bias=False)\n",
    "        self.lstm2 = nn.LSTMCell(hidden_size+15, hidden_size, bias=False)\n",
    "#        self.lstm3 = nn.LSTMCell(2*hidden_size+1, hidden_size, bias=False)\n",
    "        self.linear = nn.Linear(2*hidden_size+15, 1)\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    def forward(self, x, feature, future=1):\n",
    "        o = []\n",
    "        tt = torch.cuda if args.cuda else torch\n",
    "        h1_t = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())\n",
    "        c1_t = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())\n",
    "        h2_t = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())\n",
    "        c2_t = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())\n",
    "#        h3_t = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())\n",
    "#        c3_t = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())\n",
    "        \n",
    "        for x_t in x.chunk(x.size(1), dim=1):\n",
    "            x_t = x_t.squeeze(dim=1)\n",
    "            xd_t = torch.cat([x_t, feature], dim=1)\n",
    "            h1_t, c1_t = self.lstm1(xd_t, (h1_t, c1_t))\n",
    "            h1d_t = torch.cat([xd_t, h1_t], dim=1)\n",
    "            h2_t, c2_t = self.lstm2(h1d_t, (h2_t, c2_t))\n",
    "            h2d_t = torch.cat([xd_t, h1_t, h2_t], dim=1)\n",
    "#            h3_t, c3_t = self.lstm3(h2d_t, (h3_t, c3_t))\n",
    "#            h3d_t = torch.cat([x_t, h1_t, h2_t, h3_t], dim=1)\n",
    "            o_t = self.linear(h2d_t)\n",
    "            o.append(o_t)\n",
    "            \n",
    "        for i in range(future-1):\n",
    "            od_t = torch.cat([o_t, feature], dim=1)\n",
    "            h1_t, c1_t = self.lstm1(od_t, (h1_t, c1_t))\n",
    "            h1d_t = torch.cat([od_t, h1_t], dim=1)\n",
    "            h2_t, c2_t = self.lstm2(h1d_t, (h2_t, c2_t))\n",
    "            h2d_t = torch.cat([od_t, h1_t, h2_t], dim=1)\n",
    "#            h3_t, c3_t = self.lstm3(h2d_t, (h3_t, c3_t))\n",
    "#            h3d_t = torch.cat([o_t, h1_t, h2_t, h3_t], dim=1)\n",
    "            o_t = self.linear(h2d_t)\n",
    "            o.append(o_t)\n",
    "\n",
    "        return torch.stack(o, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-04T03:10:14.098190Z",
     "start_time": "2017-09-04T12:10:14.052211+09:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def smape(y_pred, y_true):\n",
    "    y_pred = np.around(np.clip(np.exp(y_pred)-1, 0, None))\n",
    "    y_true = np.around(np.exp(y_true) - 1)\n",
    "    raw_smape = np.abs(y_true - y_pred) / (np.abs(y_true) + np.abs(y_pred))\n",
    "    kaggle_smape = np.nan_to_num(raw_smape)\n",
    "    return np.mean(kaggle_smape) * 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-04T03:10:27.520243Z",
     "start_time": "2017-09-04T12:10:14.100569+09:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_df = pd.read_csv(os.path.join(args.data_path, args.train_file),\n",
    "                              index_col='Page')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-04T03:10:27.658757Z",
     "start_time": "2017-09-04T12:10:27.522558+09:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    scaled_data_file = os.path.join(args.intermediate_path,\n",
    "                                    'scaled_data.pkl')\n",
    "    scaler_file = os.path.join(args.intermediate_path, 'scaler.pkl')\n",
    "    features_file = os.path.join(args.intermediate_path, 'features.pkl')\n",
    "    \n",
    "    if not args.read_from_file:\n",
    "        data_df = pd.read_csv(os.path.join(args.data_path, args.train_file),\n",
    "                              index_col='Page')\n",
    "        data_df = data_df.fillna(method='ffill', axis=1).fillna(\n",
    "            method='bfill', axis=1)\n",
    "        data_df[\"agent\"] = data_df.index.str.rsplit('_').str.get(-1)\n",
    "        data_df[\"access\"] = data_df.index.str.rsplit('_').str.get(-2)\n",
    "        data_df[\"project\"] = data_df.index.str.rsplit('_').str.get(-3)\n",
    "        features = pd.get_dummies(data_df[[\"agent\", \"access\", \"project\"]],\n",
    "            columns=[\"agent\", \"access\", \"project\"]).values.astype('float32')\n",
    "        raw_data = np.nan_to_num(\n",
    "            data_df.iloc[:,:-3].values.astype('float32'))\n",
    "        data = np.log1p(raw_data)\n",
    "        scaler = RobustScaler()\n",
    "        scaler.fit(np.swapaxes(data[:, :-args.future], 0, 1))\n",
    "        scaled_data = scaler.transform(np.swapaxes(data, 0, 1))\n",
    "        scaled_data = np.swapaxes(scaled_data, 0, 1)\n",
    "        \n",
    "        with open(scaled_data_file, 'wb') as f:\n",
    "            pickle.dump(scaled_data, f)\n",
    "        with open(scaler_file, 'wb') as f:\n",
    "            pickle.dump(scaler, f)\n",
    "        with open(features_file, 'wb') as f:\n",
    "            pickle.dump(features, f)\n",
    "    else:\n",
    "        with open(scaled_data_file, 'rb') as f:\n",
    "            scaled_data = pickle.load(f)\n",
    "        with open(scaler_file, 'rb') as f:\n",
    "            scaler = pickle.load(f)\n",
    "        with open(features_file, 'rb') as f:\n",
    "            features = pickle.load(f)\n",
    "    return scaled_data, scaler, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-04T03:10:27.856654Z",
     "start_time": "2017-09-04T12:10:27.661213+09:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def train(scaled_data, scaler, features, model, criterion, optimizer):\n",
    "    p = np.random.permutation(scaled_data.shape[0])\n",
    "    inverse_p = np.argsort(p)\n",
    "    \n",
    "    input_tensor = torch.from_numpy(scaled_data[p, :-1]).unsqueeze(2)\n",
    "    target_tensor = torch.from_numpy(scaled_data[p, 1:]).unsqueeze(2)\n",
    "    features_tensor = torch.from_numpy(features[p, :])\n",
    "    dataset = TensorDataset(input_tensor, target_tensor)\n",
    "    data_loader = DataLoader(dataset, args.batch_size)\n",
    "    \n",
    "    train_loss = 0\n",
    "    val_output_list = []\n",
    "    init_time = time.time()\n",
    "    for i, (inputt, target) in enumerate(data_loader):\n",
    "        feature = features_tensor[i*args.batch_size:(i*args.batch_size\n",
    "                                                     +inputt.size(0))]\n",
    "        if args.cuda:\n",
    "            inputt = inputt.cuda()\n",
    "            target = target.cuda()\n",
    "            feature = feature.cuda()\n",
    "        inputt = Variable(inputt)\n",
    "        target = Variable(target)\n",
    "        feature = Variable(feature)\n",
    "        \n",
    "        output = model(inputt, feature)\n",
    "        pos = np.random.randint(args.future, output.size(1)-2*args.future+1)\n",
    "        loss = criterion(output[:, pos:pos+args.future],\n",
    "                         target[:, pos:pos+args.future])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.data[0] * inputt.size(0)\n",
    "        val_output_list.append(output[:, -args.future:]\n",
    "                               .data.squeeze(2).cpu().numpy())\n",
    "        \n",
    "        if i % args.log_every == 0:\n",
    "            print(\"   % Time: {:4.0f}s | Batch: {:4} | \"\n",
    "                  \"Train loss: {:.4f}\".format(\n",
    "                      time.time()-init_time, i+1, loss.data[0]))\n",
    "        \n",
    "    val_output_all = np.concatenate(val_output_list, axis=0)[inverse_p]\n",
    "    prediction = np.swapaxes(scaler.inverse_transform(\n",
    "            np.swapaxes(val_output_all, 0, 1)), 0, 1)\n",
    "    var_target = np.swapaxes(scaler.inverse_transform(\n",
    "            np.swapaxes(scaled_data[:, -args.future:], 0, 1)), 0, 1)\n",
    "    \n",
    "    train_loss /= scaled_data.shape[0]\n",
    "    val_loss = smape(prediction, var_target)\n",
    "    print(\"=\"*10)\n",
    "    print(\"   % Epoch: {} | Time: {:4.0f}s | \"\n",
    "          \"Train loss: {:.4f} | Val loss: {:.4f}\"\n",
    "          .format(epoch, time.time()-init_time, train_loss , val_loss))\n",
    "    print(\"=\"*10)\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-04T03:10:27.914601Z",
     "start_time": "2017-09-04T12:10:27.858994+09:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def forecast(scaled_data, scaler, features, model):\n",
    "    input_tensor = torch.from_numpy(scaled_data).unsqueeze(2)\n",
    "    target_tensor = torch.zeros(input_tensor.size(0))\n",
    "    features_tensor = torch.from_numpy(features)\n",
    "    dataset = torch.utils.data.TensorDataset(input_tensor, target_tensor)\n",
    "    data_loader = DataLoader(dataset, args.batch_size)\n",
    "    \n",
    "    output_list = []\n",
    "    for i, (inputt, _) in enumerate(data_loader):\n",
    "        feature = features_tensor[i*args.batch_size:(i*args.batch_size\n",
    "                                                     +inputt.size(0))]\n",
    "        if args.cuda:\n",
    "            inputt = inputt.cuda()\n",
    "            feature = feature.cuda()\n",
    "        inputt = Variable(inputt)\n",
    "        feature = Variable(feature)\n",
    "        output = model(inputt, feature, args.future)\n",
    "        output_list.append(output.data.squeeze(2).cpu().numpy()\n",
    "                           [:, -args.future:])\n",
    "        \n",
    "    output_all = np.concatenate(output_list, axis=0)\n",
    "    prediction = np.swapaxes(scaler.inverse_transform(\n",
    "            np.swapaxes(output_all, 0, 1)), 0, 1)\n",
    "    prediction = np.around(np.clip(np.exp(prediction) - 1, 0, None))\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-04T03:10:27.945785Z",
     "start_time": "2017-09-04T12:10:27.916747+09:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def save_model(model, epoch, loss):\n",
    "    model_file = os.path.join(args.intermediate_path,\n",
    "                              \"model_seed{}_epoch{}_loss_{:.4f}.pth\"\n",
    "                              .format(args.seed, epoch, loss))\n",
    "    torch.save(model.state_dict(), os.path.join(model_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-04T03:10:55.916070Z",
     "start_time": "2017-09-04T12:10:27.948179+09:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "scaled_data, scaler, features = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-04T03:10:58.127244Z",
     "start_time": "2017-09-04T12:10:55.918661+09:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model = DenseLSTMForecast(args.hidden_size)\n",
    "if args.cuda:\n",
    "    model.cuda()\n",
    "criterion = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-04T03:10:58.133747Z",
     "start_time": "2017-09-04T12:10:58.129797+09:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "scheduler = MultiStepLR(optimizer, milestones=[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-04T05:11:57.590001Z",
     "start_time": "2017-09-04T12:10:58.136235+09:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> EPOCH 1 with lr 0.001\n",
      "   % Time:    2s | Batch:    1 | Train loss: 0.5501\n",
      "   % Time:   18s | Batch:   11 | Train loss: 0.4557\n",
      "   % Time:   34s | Batch:   21 | Train loss: 0.4303\n",
      "   % Time:   50s | Batch:   31 | Train loss: 0.5042\n",
      "   % Time:   66s | Batch:   41 | Train loss: 0.4039\n",
      "   % Time:   82s | Batch:   51 | Train loss: 0.4294\n",
      "   % Time:   98s | Batch:   61 | Train loss: 0.4464\n",
      "   % Time:  114s | Batch:   71 | Train loss: 0.4411\n",
      "   % Time:  130s | Batch:   81 | Train loss: 0.4276\n",
      "   % Time:  146s | Batch:   91 | Train loss: 0.3899\n",
      "   % Time:  162s | Batch:  101 | Train loss: 0.4372\n",
      "   % Time:  178s | Batch:  111 | Train loss: 0.3984\n",
      "   % Time:  194s | Batch:  121 | Train loss: 0.4330\n",
      "   % Time:  210s | Batch:  131 | Train loss: 0.4113\n",
      "   % Time:  226s | Batch:  141 | Train loss: 0.3802\n",
      "   % Time:  242s | Batch:  151 | Train loss: 0.4080\n",
      "   % Time:  258s | Batch:  161 | Train loss: 0.4449\n",
      "   % Time:  274s | Batch:  171 | Train loss: 0.3948\n",
      "   % Time:  290s | Batch:  181 | Train loss: 0.4014\n",
      "   % Time:  306s | Batch:  191 | Train loss: 0.3879\n",
      "   % Time:  322s | Batch:  201 | Train loss: 0.4032\n",
      "   % Time:  338s | Batch:  211 | Train loss: 0.3907\n",
      "   % Time:  353s | Batch:  221 | Train loss: 0.3931\n",
      "   % Time:  369s | Batch:  231 | Train loss: 0.3883\n",
      "   % Time:  385s | Batch:  241 | Train loss: 0.4019\n",
      "   % Time:  401s | Batch:  251 | Train loss: 0.3636\n",
      "   % Time:  417s | Batch:  261 | Train loss: 0.4008\n",
      "   % Time:  433s | Batch:  271 | Train loss: 0.3800\n",
      "   % Time:  449s | Batch:  281 | Train loss: 0.3684\n",
      "   % Time:  465s | Batch:  291 | Train loss: 0.4053\n",
      "   % Time:  481s | Batch:  301 | Train loss: 0.3624\n",
      "   % Time:  497s | Batch:  311 | Train loss: 0.3582\n",
      "   % Time:  513s | Batch:  321 | Train loss: 0.3759\n",
      "   % Time:  529s | Batch:  331 | Train loss: 0.4101\n",
      "   % Time:  545s | Batch:  341 | Train loss: 0.4087\n",
      "   % Time:  561s | Batch:  351 | Train loss: 0.4066\n",
      "   % Time:  577s | Batch:  361 | Train loss: 0.3926\n",
      "   % Time:  593s | Batch:  371 | Train loss: 0.3765\n",
      "   % Time:  609s | Batch:  381 | Train loss: 0.3885\n",
      "   % Time:  625s | Batch:  391 | Train loss: 0.3929\n",
      "   % Time:  641s | Batch:  401 | Train loss: 0.3810\n",
      "   % Time:  657s | Batch:  411 | Train loss: 0.3809\n",
      "   % Time:  673s | Batch:  421 | Train loss: 0.4089\n",
      "   % Time:  688s | Batch:  431 | Train loss: 0.4198\n",
      "   % Time:  704s | Batch:  441 | Train loss: 0.3976\n",
      "   % Time:  720s | Batch:  451 | Train loss: 0.3537\n",
      "   % Time:  736s | Batch:  461 | Train loss: 1.1398\n",
      "   % Time:  752s | Batch:  471 | Train loss: 0.3860\n",
      "   % Time:  768s | Batch:  481 | Train loss: 0.3936\n",
      "   % Time:  783s | Batch:  491 | Train loss: 0.3556\n",
      "   % Time:  799s | Batch:  501 | Train loss: 0.3872\n",
      "   % Time:  815s | Batch:  511 | Train loss: 0.4640\n",
      "   % Time:  831s | Batch:  521 | Train loss: 0.3548\n",
      "   % Time:  847s | Batch:  531 | Train loss: 0.3888\n",
      "   % Time:  863s | Batch:  541 | Train loss: 0.4559\n",
      "   % Time:  878s | Batch:  551 | Train loss: 0.3837\n",
      "   % Time:  894s | Batch:  561 | Train loss: 0.3990\n",
      "   % Time:  910s | Batch:  571 | Train loss: 0.3900\n",
      "   % Time:  927s | Batch:  581 | Train loss: 0.3736\n",
      "   % Time:  943s | Batch:  591 | Train loss: 0.3977\n",
      "   % Time:  959s | Batch:  601 | Train loss: 0.3833\n",
      "   % Time:  975s | Batch:  611 | Train loss: 0.3798\n",
      "   % Time:  991s | Batch:  621 | Train loss: 0.3827\n",
      "   % Time: 1007s | Batch:  631 | Train loss: 0.3924\n",
      "   % Time: 1023s | Batch:  641 | Train loss: 0.3803\n",
      "   % Time: 1039s | Batch:  651 | Train loss: 0.3618\n",
      "   % Time: 1055s | Batch:  661 | Train loss: 0.3685\n",
      "   % Time: 1071s | Batch:  671 | Train loss: 0.3628\n",
      "   % Time: 1087s | Batch:  681 | Train loss: 0.3826\n",
      "   % Time: 1102s | Batch:  691 | Train loss: 0.3948\n",
      "   % Time: 1118s | Batch:  701 | Train loss: 0.3590\n",
      "   % Time: 1134s | Batch:  711 | Train loss: 0.3966\n",
      "   % Time: 1150s | Batch:  721 | Train loss: 0.4124\n",
      "   % Time: 1166s | Batch:  731 | Train loss: 0.5029\n",
      "   % Time: 1182s | Batch:  741 | Train loss: 0.3705\n",
      "   % Time: 1198s | Batch:  751 | Train loss: 0.4172\n",
      "   % Time: 1214s | Batch:  761 | Train loss: 0.3727\n",
      "   % Time: 1230s | Batch:  771 | Train loss: 0.3973\n",
      "   % Time: 1246s | Batch:  781 | Train loss: 0.3599\n",
      "   % Time: 1262s | Batch:  791 | Train loss: 0.3792\n",
      "   % Time: 1278s | Batch:  801 | Train loss: 0.3582\n",
      "   % Time: 1294s | Batch:  811 | Train loss: 0.3972\n",
      "   % Time: 1311s | Batch:  821 | Train loss: 0.3783\n",
      "   % Time: 1327s | Batch:  831 | Train loss: 0.3442\n",
      "   % Time: 1343s | Batch:  841 | Train loss: 0.3629\n",
      "   % Time: 1359s | Batch:  851 | Train loss: 0.3311\n",
      "   % Time: 1375s | Batch:  861 | Train loss: 0.4002\n",
      "   % Time: 1391s | Batch:  871 | Train loss: 0.3319\n",
      "   % Time: 1407s | Batch:  881 | Train loss: 0.3355\n",
      "   % Time: 1423s | Batch:  891 | Train loss: 0.3687\n",
      "   % Time: 1439s | Batch:  901 | Train loss: 0.3576\n",
      "   % Time: 1455s | Batch:  911 | Train loss: 0.3761\n",
      "   % Time: 1471s | Batch:  921 | Train loss: 0.3777\n",
      "   % Time: 1487s | Batch:  931 | Train loss: 0.3763\n",
      "   % Time: 1503s | Batch:  941 | Train loss: 0.3475\n",
      "   % Time: 1519s | Batch:  951 | Train loss: 0.3660\n",
      "   % Time: 1535s | Batch:  961 | Train loss: 0.3838\n",
      "   % Time: 1551s | Batch:  971 | Train loss: 0.3848\n",
      "   % Time: 1567s | Batch:  981 | Train loss: 0.3109\n",
      "   % Time: 1583s | Batch:  991 | Train loss: 0.3964\n",
      "   % Time: 1599s | Batch: 1001 | Train loss: 0.3961\n",
      "   % Time: 1615s | Batch: 1011 | Train loss: 0.4605\n",
      "   % Time: 1631s | Batch: 1021 | Train loss: 0.3721\n",
      "   % Time: 1647s | Batch: 1031 | Train loss: 0.3800\n",
      "   % Time: 1663s | Batch: 1041 | Train loss: 0.3630\n",
      "   % Time: 1678s | Batch: 1051 | Train loss: 0.3391\n",
      "   % Time: 1694s | Batch: 1061 | Train loss: 0.3336\n",
      "   % Time: 1710s | Batch: 1071 | Train loss: 0.3751\n",
      "   % Time: 1726s | Batch: 1081 | Train loss: 0.3545\n",
      "   % Time: 1742s | Batch: 1091 | Train loss: 0.3547\n",
      "   % Time: 1758s | Batch: 1101 | Train loss: 0.3621\n",
      "   % Time: 1774s | Batch: 1111 | Train loss: 0.3612\n",
      "   % Time: 1789s | Batch: 1121 | Train loss: 0.3620\n",
      "   % Time: 1805s | Batch: 1131 | Train loss: 0.3153\n",
      "==========\n",
      "   % Epoch: 1 | Time: 1811s | Train loss: 0.3864 | Val loss: 29.2868\n",
      "==========\n",
      "=> EPOCH 2 with lr 0.001\n",
      "   % Time:    2s | Batch:    1 | Train loss: 0.3398\n",
      "   % Time:   17s | Batch:   11 | Train loss: 0.3416\n",
      "   % Time:   33s | Batch:   21 | Train loss: 0.3661\n",
      "   % Time:   49s | Batch:   31 | Train loss: 0.3220\n",
      "   % Time:   65s | Batch:   41 | Train loss: 0.3327\n",
      "   % Time:   81s | Batch:   51 | Train loss: 0.3506\n",
      "   % Time:   97s | Batch:   61 | Train loss: 0.3898\n",
      "   % Time:  113s | Batch:   71 | Train loss: 0.3545\n",
      "   % Time:  128s | Batch:   81 | Train loss: 0.3791\n",
      "   % Time:  144s | Batch:   91 | Train loss: 0.3777\n",
      "   % Time:  160s | Batch:  101 | Train loss: 0.3994\n",
      "   % Time:  176s | Batch:  111 | Train loss: 0.3848\n",
      "   % Time:  191s | Batch:  121 | Train loss: 0.3525\n",
      "   % Time:  207s | Batch:  131 | Train loss: 0.3453\n",
      "   % Time:  223s | Batch:  141 | Train loss: 0.4092\n",
      "   % Time:  239s | Batch:  151 | Train loss: 0.3390\n",
      "   % Time:  255s | Batch:  161 | Train loss: 0.3597\n",
      "   % Time:  271s | Batch:  171 | Train loss: 0.3873\n",
      "   % Time:  286s | Batch:  181 | Train loss: 0.3750\n",
      "   % Time:  302s | Batch:  191 | Train loss: 0.3220\n",
      "   % Time:  318s | Batch:  201 | Train loss: 0.3697\n",
      "   % Time:  334s | Batch:  211 | Train loss: 0.3539\n",
      "   % Time:  350s | Batch:  221 | Train loss: 0.3382\n",
      "   % Time:  366s | Batch:  231 | Train loss: 0.3836\n",
      "   % Time:  381s | Batch:  241 | Train loss: 0.3549\n",
      "   % Time:  397s | Batch:  251 | Train loss: 0.3951\n",
      "   % Time:  413s | Batch:  261 | Train loss: 0.3541\n",
      "   % Time:  428s | Batch:  271 | Train loss: 0.9660\n",
      "   % Time:  444s | Batch:  281 | Train loss: 0.4081\n",
      "   % Time:  460s | Batch:  291 | Train loss: 0.3554\n",
      "   % Time:  476s | Batch:  301 | Train loss: 0.3834\n",
      "   % Time:  492s | Batch:  311 | Train loss: 0.3750\n",
      "   % Time:  508s | Batch:  321 | Train loss: 0.3923\n",
      "   % Time:  524s | Batch:  331 | Train loss: 0.3580\n",
      "   % Time:  539s | Batch:  341 | Train loss: 0.4034\n",
      "   % Time:  555s | Batch:  351 | Train loss: 0.3703\n",
      "   % Time:  571s | Batch:  361 | Train loss: 0.3813\n",
      "   % Time:  587s | Batch:  371 | Train loss: 0.3545\n",
      "   % Time:  603s | Batch:  381 | Train loss: 0.3452\n",
      "   % Time:  619s | Batch:  391 | Train loss: 0.3723\n",
      "   % Time:  635s | Batch:  401 | Train loss: 0.3853\n",
      "   % Time:  651s | Batch:  411 | Train loss: 0.3499\n",
      "   % Time:  667s | Batch:  421 | Train loss: 0.3571\n",
      "   % Time:  683s | Batch:  431 | Train loss: 0.3861\n",
      "   % Time:  699s | Batch:  441 | Train loss: 0.3789\n",
      "   % Time:  715s | Batch:  451 | Train loss: 0.3522\n",
      "   % Time:  732s | Batch:  461 | Train loss: 0.3248\n",
      "   % Time:  748s | Batch:  471 | Train loss: 0.3746\n",
      "   % Time:  764s | Batch:  481 | Train loss: 0.3686\n",
      "   % Time:  780s | Batch:  491 | Train loss: 0.3512\n",
      "   % Time:  796s | Batch:  501 | Train loss: 0.3724\n",
      "   % Time:  812s | Batch:  511 | Train loss: 0.3222\n",
      "   % Time:  828s | Batch:  521 | Train loss: 0.3639\n",
      "   % Time:  844s | Batch:  531 | Train loss: 0.3692\n",
      "   % Time:  860s | Batch:  541 | Train loss: 0.3396\n",
      "   % Time:  876s | Batch:  551 | Train loss: 0.3547\n",
      "   % Time:  892s | Batch:  561 | Train loss: 0.3451\n",
      "   % Time:  908s | Batch:  571 | Train loss: 0.3590\n",
      "   % Time:  925s | Batch:  581 | Train loss: 0.3863\n",
      "   % Time:  940s | Batch:  591 | Train loss: 0.3975\n",
      "   % Time:  956s | Batch:  601 | Train loss: 0.3529\n",
      "   % Time:  972s | Batch:  611 | Train loss: 0.3663\n",
      "   % Time:  988s | Batch:  621 | Train loss: 0.3783\n",
      "   % Time: 1004s | Batch:  631 | Train loss: 0.3335\n",
      "   % Time: 1020s | Batch:  641 | Train loss: 0.3882\n",
      "   % Time: 1036s | Batch:  651 | Train loss: 0.3655\n",
      "   % Time: 1052s | Batch:  661 | Train loss: 0.3310\n",
      "   % Time: 1068s | Batch:  671 | Train loss: 0.3193\n",
      "   % Time: 1084s | Batch:  681 | Train loss: 0.3758\n",
      "   % Time: 1100s | Batch:  691 | Train loss: 0.3471\n",
      "   % Time: 1116s | Batch:  701 | Train loss: 0.3552\n",
      "   % Time: 1132s | Batch:  711 | Train loss: 0.3738\n",
      "   % Time: 1148s | Batch:  721 | Train loss: 0.3668\n",
      "   % Time: 1164s | Batch:  731 | Train loss: 0.3845\n",
      "   % Time: 1180s | Batch:  741 | Train loss: 0.3659\n",
      "   % Time: 1196s | Batch:  751 | Train loss: 0.3734\n",
      "   % Time: 1212s | Batch:  761 | Train loss: 0.4033\n",
      "   % Time: 1228s | Batch:  771 | Train loss: 0.3536\n",
      "   % Time: 1244s | Batch:  781 | Train loss: 0.3442\n",
      "   % Time: 1260s | Batch:  791 | Train loss: 0.3720\n",
      "   % Time: 1275s | Batch:  801 | Train loss: 0.3608\n",
      "   % Time: 1291s | Batch:  811 | Train loss: 0.3554\n",
      "   % Time: 1307s | Batch:  821 | Train loss: 0.3630\n",
      "   % Time: 1323s | Batch:  831 | Train loss: 0.3365\n",
      "   % Time: 1339s | Batch:  841 | Train loss: 0.3578\n",
      "   % Time: 1355s | Batch:  851 | Train loss: 0.3930\n",
      "   % Time: 1371s | Batch:  861 | Train loss: 0.3466\n",
      "   % Time: 1387s | Batch:  871 | Train loss: 0.4112\n",
      "   % Time: 1403s | Batch:  881 | Train loss: 0.3743\n",
      "   % Time: 1419s | Batch:  891 | Train loss: 0.3701\n",
      "   % Time: 1435s | Batch:  901 | Train loss: 0.3668\n",
      "   % Time: 1451s | Batch:  911 | Train loss: 0.3432\n",
      "   % Time: 1467s | Batch:  921 | Train loss: 0.4109\n",
      "   % Time: 1483s | Batch:  931 | Train loss: 0.3792\n",
      "   % Time: 1500s | Batch:  941 | Train loss: 0.3671\n",
      "   % Time: 1516s | Batch:  951 | Train loss: 0.3887\n",
      "   % Time: 1532s | Batch:  961 | Train loss: 0.3870\n",
      "   % Time: 1547s | Batch:  971 | Train loss: 0.3701\n",
      "   % Time: 1563s | Batch:  981 | Train loss: 0.3572\n",
      "   % Time: 1579s | Batch:  991 | Train loss: 0.3778\n",
      "   % Time: 1595s | Batch: 1001 | Train loss: 0.3718\n",
      "   % Time: 1611s | Batch: 1011 | Train loss: 0.3691\n",
      "   % Time: 1626s | Batch: 1021 | Train loss: 0.3719\n",
      "   % Time: 1642s | Batch: 1031 | Train loss: 0.3692\n",
      "   % Time: 1658s | Batch: 1041 | Train loss: 0.3446\n",
      "   % Time: 1674s | Batch: 1051 | Train loss: 0.3759\n",
      "   % Time: 1690s | Batch: 1061 | Train loss: 0.3466\n",
      "   % Time: 1706s | Batch: 1071 | Train loss: 0.3930\n",
      "   % Time: 1721s | Batch: 1081 | Train loss: 0.3409\n",
      "   % Time: 1737s | Batch: 1091 | Train loss: 0.3962\n",
      "   % Time: 1753s | Batch: 1101 | Train loss: 0.3948\n",
      "   % Time: 1769s | Batch: 1111 | Train loss: 0.4089\n",
      "   % Time: 1785s | Batch: 1121 | Train loss: 0.3491\n",
      "   % Time: 1801s | Batch: 1131 | Train loss: 0.3459\n",
      "==========\n",
      "   % Epoch: 2 | Time: 1806s | Train loss: 0.3682 | Val loss: 27.6588\n",
      "==========\n",
      "=> EPOCH 3 with lr 0.0001\n",
      "   % Time:    2s | Batch:    1 | Train loss: 0.3925\n",
      "   % Time:   17s | Batch:   11 | Train loss: 0.3791\n",
      "   % Time:   33s | Batch:   21 | Train loss: 0.3150\n",
      "   % Time:   50s | Batch:   31 | Train loss: 0.3265\n",
      "   % Time:   66s | Batch:   41 | Train loss: 0.3630\n",
      "   % Time:   82s | Batch:   51 | Train loss: 0.4205\n",
      "   % Time:   98s | Batch:   61 | Train loss: 0.3585\n",
      "   % Time:  114s | Batch:   71 | Train loss: 0.3742\n",
      "   % Time:  130s | Batch:   81 | Train loss: 0.3816\n",
      "   % Time:  146s | Batch:   91 | Train loss: 0.3258\n",
      "   % Time:  162s | Batch:  101 | Train loss: 0.3887\n",
      "   % Time:  178s | Batch:  111 | Train loss: 0.3979\n",
      "   % Time:  194s | Batch:  121 | Train loss: 0.3917\n",
      "   % Time:  210s | Batch:  131 | Train loss: 0.3644\n",
      "   % Time:  226s | Batch:  141 | Train loss: 0.3543\n",
      "   % Time:  242s | Batch:  151 | Train loss: 0.3523\n",
      "   % Time:  258s | Batch:  161 | Train loss: 0.3409\n",
      "   % Time:  275s | Batch:  171 | Train loss: 0.3713\n",
      "   % Time:  291s | Batch:  181 | Train loss: 0.3592\n",
      "   % Time:  307s | Batch:  191 | Train loss: 0.3664\n",
      "   % Time:  323s | Batch:  201 | Train loss: 0.3545\n",
      "   % Time:  339s | Batch:  211 | Train loss: 0.3522\n",
      "   % Time:  355s | Batch:  221 | Train loss: 0.3641\n",
      "   % Time:  371s | Batch:  231 | Train loss: 0.3511\n",
      "   % Time:  387s | Batch:  241 | Train loss: 0.3803\n",
      "   % Time:  403s | Batch:  251 | Train loss: 0.3411\n",
      "   % Time:  419s | Batch:  261 | Train loss: 0.3979\n",
      "   % Time:  435s | Batch:  271 | Train loss: 0.3784\n",
      "   % Time:  451s | Batch:  281 | Train loss: 0.3789\n",
      "   % Time:  467s | Batch:  291 | Train loss: 0.3386\n",
      "   % Time:  483s | Batch:  301 | Train loss: 0.3929\n",
      "   % Time:  499s | Batch:  311 | Train loss: 0.3809\n",
      "   % Time:  516s | Batch:  321 | Train loss: 0.3526\n",
      "   % Time:  532s | Batch:  331 | Train loss: 0.3650\n",
      "   % Time:  548s | Batch:  341 | Train loss: 0.3682\n",
      "   % Time:  564s | Batch:  351 | Train loss: 0.3827\n",
      "   % Time:  580s | Batch:  361 | Train loss: 0.3552\n",
      "   % Time:  596s | Batch:  371 | Train loss: 0.4445\n",
      "   % Time:  612s | Batch:  381 | Train loss: 0.3659\n",
      "   % Time:  628s | Batch:  391 | Train loss: 0.3882\n",
      "   % Time:  644s | Batch:  401 | Train loss: 0.3580\n",
      "   % Time:  660s | Batch:  411 | Train loss: 0.3474\n",
      "   % Time:  676s | Batch:  421 | Train loss: 0.3803\n",
      "   % Time:  692s | Batch:  431 | Train loss: 0.3506\n",
      "   % Time:  708s | Batch:  441 | Train loss: 0.3862\n",
      "   % Time:  724s | Batch:  451 | Train loss: 0.3561\n",
      "   % Time:  741s | Batch:  461 | Train loss: 0.3679\n",
      "   % Time:  757s | Batch:  471 | Train loss: 0.3766\n",
      "   % Time:  773s | Batch:  481 | Train loss: 0.3776\n",
      "   % Time:  789s | Batch:  491 | Train loss: 0.3516\n",
      "   % Time:  805s | Batch:  501 | Train loss: 0.3544\n",
      "   % Time:  821s | Batch:  511 | Train loss: 0.3341\n",
      "   % Time:  837s | Batch:  521 | Train loss: 0.3356\n",
      "   % Time:  853s | Batch:  531 | Train loss: 0.3571\n",
      "   % Time:  869s | Batch:  541 | Train loss: 0.3641\n",
      "   % Time:  885s | Batch:  551 | Train loss: 0.3557\n",
      "   % Time:  901s | Batch:  561 | Train loss: 0.3183\n",
      "   % Time:  917s | Batch:  571 | Train loss: 0.3596\n",
      "   % Time:  933s | Batch:  581 | Train loss: 0.3943\n",
      "   % Time:  949s | Batch:  591 | Train loss: 0.3500\n",
      "   % Time:  966s | Batch:  601 | Train loss: 0.4201\n",
      "   % Time:  981s | Batch:  611 | Train loss: 0.3644\n",
      "   % Time:  997s | Batch:  621 | Train loss: 0.3661\n",
      "   % Time: 1013s | Batch:  631 | Train loss: 0.3480\n",
      "   % Time: 1029s | Batch:  641 | Train loss: 0.3703\n",
      "   % Time: 1045s | Batch:  651 | Train loss: 0.3528\n",
      "   % Time: 1061s | Batch:  661 | Train loss: 0.3472\n",
      "   % Time: 1077s | Batch:  671 | Train loss: 0.3667\n",
      "   % Time: 1093s | Batch:  681 | Train loss: 0.3889\n",
      "   % Time: 1109s | Batch:  691 | Train loss: 0.3709\n",
      "   % Time: 1125s | Batch:  701 | Train loss: 0.3559\n",
      "   % Time: 1141s | Batch:  711 | Train loss: 0.3472\n",
      "   % Time: 1157s | Batch:  721 | Train loss: 0.3418\n",
      "   % Time: 1173s | Batch:  731 | Train loss: 0.3174\n",
      "   % Time: 1189s | Batch:  741 | Train loss: 0.3657\n",
      "   % Time: 1205s | Batch:  751 | Train loss: 0.3763\n",
      "   % Time: 1221s | Batch:  761 | Train loss: 0.3464\n",
      "   % Time: 1237s | Batch:  771 | Train loss: 0.3579\n",
      "   % Time: 1253s | Batch:  781 | Train loss: 0.3711\n",
      "   % Time: 1268s | Batch:  791 | Train loss: 0.3369\n",
      "   % Time: 1284s | Batch:  801 | Train loss: 0.3318\n",
      "   % Time: 1300s | Batch:  811 | Train loss: 0.3305\n",
      "   % Time: 1316s | Batch:  821 | Train loss: 0.3692\n",
      "   % Time: 1332s | Batch:  831 | Train loss: 0.3302\n",
      "   % Time: 1348s | Batch:  841 | Train loss: 0.3628\n",
      "   % Time: 1364s | Batch:  851 | Train loss: 0.3473\n",
      "   % Time: 1380s | Batch:  861 | Train loss: 0.3454\n",
      "   % Time: 1396s | Batch:  871 | Train loss: 0.3527\n",
      "   % Time: 1412s | Batch:  881 | Train loss: 0.3896\n",
      "   % Time: 1428s | Batch:  891 | Train loss: 0.3278\n",
      "   % Time: 1444s | Batch:  901 | Train loss: 0.3430\n",
      "   % Time: 1459s | Batch:  911 | Train loss: 0.3720\n",
      "   % Time: 1475s | Batch:  921 | Train loss: 0.3399\n",
      "   % Time: 1491s | Batch:  931 | Train loss: 0.3291\n",
      "   % Time: 1507s | Batch:  941 | Train loss: 0.3621\n",
      "   % Time: 1523s | Batch:  951 | Train loss: 0.3556\n",
      "   % Time: 1539s | Batch:  961 | Train loss: 0.3494\n",
      "   % Time: 1555s | Batch:  971 | Train loss: 0.3711\n",
      "   % Time: 1571s | Batch:  981 | Train loss: 0.3405\n",
      "   % Time: 1587s | Batch:  991 | Train loss: 0.3572\n",
      "   % Time: 1603s | Batch: 1001 | Train loss: 0.3511\n",
      "   % Time: 1618s | Batch: 1011 | Train loss: 0.3539\n",
      "   % Time: 1634s | Batch: 1021 | Train loss: 0.3939\n",
      "   % Time: 1650s | Batch: 1031 | Train loss: 0.3694\n",
      "   % Time: 1666s | Batch: 1041 | Train loss: 0.3621\n",
      "   % Time: 1682s | Batch: 1051 | Train loss: 0.4093\n",
      "   % Time: 1698s | Batch: 1061 | Train loss: 0.3875\n",
      "   % Time: 1714s | Batch: 1071 | Train loss: 0.3309\n",
      "   % Time: 1730s | Batch: 1081 | Train loss: 0.3961\n",
      "   % Time: 1746s | Batch: 1091 | Train loss: 0.3251\n",
      "   % Time: 1762s | Batch: 1101 | Train loss: 0.3690\n",
      "   % Time: 1779s | Batch: 1111 | Train loss: 0.3467\n",
      "   % Time: 1795s | Batch: 1121 | Train loss: 0.3637\n",
      "   % Time: 1811s | Batch: 1131 | Train loss: 0.3651\n",
      "==========\n",
      "   % Epoch: 3 | Time: 1816s | Train loss: 0.3626 | Val loss: 27.1929\n",
      "==========\n",
      "=> EPOCH 4 with lr 0.0001\n",
      "   % Time:    2s | Batch:    1 | Train loss: 0.3286\n",
      "   % Time:   18s | Batch:   11 | Train loss: 0.3889\n",
      "   % Time:   34s | Batch:   21 | Train loss: 0.3079\n",
      "   % Time:   50s | Batch:   31 | Train loss: 0.3508\n",
      "   % Time:   66s | Batch:   41 | Train loss: 0.3737\n",
      "   % Time:   82s | Batch:   51 | Train loss: 0.3373\n",
      "   % Time:   98s | Batch:   61 | Train loss: 0.3524\n",
      "   % Time:  114s | Batch:   71 | Train loss: 0.3903\n",
      "   % Time:  130s | Batch:   81 | Train loss: 0.3583\n",
      "   % Time:  146s | Batch:   91 | Train loss: 0.3429\n",
      "   % Time:  162s | Batch:  101 | Train loss: 0.3602\n",
      "   % Time:  178s | Batch:  111 | Train loss: 0.3641\n",
      "   % Time:  194s | Batch:  121 | Train loss: 0.3476\n",
      "   % Time:  210s | Batch:  131 | Train loss: 0.3649\n",
      "   % Time:  226s | Batch:  141 | Train loss: 0.3641\n",
      "   % Time:  242s | Batch:  151 | Train loss: 0.4175\n",
      "   % Time:  258s | Batch:  161 | Train loss: 0.3332\n",
      "   % Time:  274s | Batch:  171 | Train loss: 0.3204\n",
      "   % Time:  290s | Batch:  181 | Train loss: 0.3615\n",
      "   % Time:  306s | Batch:  191 | Train loss: 0.3635\n",
      "   % Time:  322s | Batch:  201 | Train loss: 0.3408\n",
      "   % Time:  338s | Batch:  211 | Train loss: 0.3589\n",
      "   % Time:  354s | Batch:  221 | Train loss: 0.3706\n",
      "   % Time:  370s | Batch:  231 | Train loss: 0.3136\n",
      "   % Time:  386s | Batch:  241 | Train loss: 0.3568\n",
      "   % Time:  401s | Batch:  251 | Train loss: 0.3397\n",
      "   % Time:  417s | Batch:  261 | Train loss: 0.3498\n",
      "   % Time:  433s | Batch:  271 | Train loss: 0.4180\n",
      "   % Time:  449s | Batch:  281 | Train loss: 0.3675\n",
      "   % Time:  465s | Batch:  291 | Train loss: 0.3389\n",
      "   % Time:  481s | Batch:  301 | Train loss: 0.3192\n",
      "   % Time:  496s | Batch:  311 | Train loss: 0.3564\n",
      "   % Time:  512s | Batch:  321 | Train loss: 0.4073\n",
      "   % Time:  528s | Batch:  331 | Train loss: 0.3545\n",
      "   % Time:  544s | Batch:  341 | Train loss: 0.3662\n",
      "   % Time:  560s | Batch:  351 | Train loss: 0.3889\n",
      "   % Time:  576s | Batch:  361 | Train loss: 0.3743\n",
      "   % Time:  592s | Batch:  371 | Train loss: 0.3601\n",
      "   % Time:  608s | Batch:  381 | Train loss: 0.3376\n",
      "   % Time:  624s | Batch:  391 | Train loss: 0.3451\n",
      "   % Time:  640s | Batch:  401 | Train loss: 0.3672\n",
      "   % Time:  657s | Batch:  411 | Train loss: 0.3427\n",
      "   % Time:  673s | Batch:  421 | Train loss: 0.3483\n",
      "   % Time:  689s | Batch:  431 | Train loss: 0.3487\n",
      "   % Time:  705s | Batch:  441 | Train loss: 0.3432\n",
      "   % Time:  721s | Batch:  451 | Train loss: 0.3221\n",
      "   % Time:  737s | Batch:  461 | Train loss: 0.3400\n",
      "   % Time:  753s | Batch:  471 | Train loss: 0.3488\n",
      "   % Time:  769s | Batch:  481 | Train loss: 0.3497\n",
      "   % Time:  785s | Batch:  491 | Train loss: 0.3796\n",
      "   % Time:  801s | Batch:  501 | Train loss: 0.3432\n",
      "   % Time:  817s | Batch:  511 | Train loss: 0.3367\n",
      "   % Time:  833s | Batch:  521 | Train loss: 0.3399\n",
      "   % Time:  849s | Batch:  531 | Train loss: 0.3427\n",
      "   % Time:  865s | Batch:  541 | Train loss: 0.3402\n",
      "   % Time:  882s | Batch:  551 | Train loss: 0.3282\n",
      "   % Time:  898s | Batch:  561 | Train loss: 0.3234\n",
      "   % Time:  914s | Batch:  571 | Train loss: 0.4077\n",
      "   % Time:  930s | Batch:  581 | Train loss: 0.3623\n",
      "   % Time:  946s | Batch:  591 | Train loss: 0.3630\n",
      "   % Time:  962s | Batch:  601 | Train loss: 0.3081\n",
      "   % Time:  978s | Batch:  611 | Train loss: 0.3632\n",
      "   % Time:  993s | Batch:  621 | Train loss: 0.3338\n",
      "   % Time: 1009s | Batch:  631 | Train loss: 0.3448\n",
      "   % Time: 1025s | Batch:  641 | Train loss: 0.3646\n",
      "   % Time: 1041s | Batch:  651 | Train loss: 0.3562\n",
      "   % Time: 1056s | Batch:  661 | Train loss: 0.3410\n",
      "   % Time: 1072s | Batch:  671 | Train loss: 0.3823\n",
      "   % Time: 1088s | Batch:  681 | Train loss: 0.3646\n",
      "   % Time: 1104s | Batch:  691 | Train loss: 0.3958\n",
      "   % Time: 1119s | Batch:  701 | Train loss: 0.3605\n",
      "   % Time: 1134s | Batch:  711 | Train loss: 0.4415\n",
      "   % Time: 1150s | Batch:  721 | Train loss: 0.3583\n",
      "   % Time: 1167s | Batch:  731 | Train loss: 0.3568\n",
      "   % Time: 1183s | Batch:  741 | Train loss: 0.3508\n",
      "   % Time: 1199s | Batch:  751 | Train loss: 0.3419\n",
      "   % Time: 1215s | Batch:  761 | Train loss: 0.3689\n",
      "   % Time: 1231s | Batch:  771 | Train loss: 0.3291\n",
      "   % Time: 1247s | Batch:  781 | Train loss: 0.3636\n",
      "   % Time: 1263s | Batch:  791 | Train loss: 0.3442\n",
      "   % Time: 1279s | Batch:  801 | Train loss: 0.3590\n",
      "   % Time: 1295s | Batch:  811 | Train loss: 0.3078\n",
      "   % Time: 1311s | Batch:  821 | Train loss: 0.3435\n",
      "   % Time: 1327s | Batch:  831 | Train loss: 0.3559\n",
      "   % Time: 1343s | Batch:  841 | Train loss: 0.5378\n",
      "   % Time: 1359s | Batch:  851 | Train loss: 0.3515\n",
      "   % Time: 1375s | Batch:  861 | Train loss: 0.3517\n",
      "   % Time: 1391s | Batch:  871 | Train loss: 0.3504\n",
      "   % Time: 1406s | Batch:  881 | Train loss: 0.3493\n",
      "   % Time: 1422s | Batch:  891 | Train loss: 0.3555\n",
      "   % Time: 1438s | Batch:  901 | Train loss: 0.3480\n",
      "   % Time: 1454s | Batch:  911 | Train loss: 0.3309\n",
      "   % Time: 1470s | Batch:  921 | Train loss: 0.3808\n",
      "   % Time: 1485s | Batch:  931 | Train loss: 0.3328\n",
      "   % Time: 1501s | Batch:  941 | Train loss: 0.3511\n",
      "   % Time: 1517s | Batch:  951 | Train loss: 0.3683\n",
      "   % Time: 1532s | Batch:  961 | Train loss: 0.3548\n",
      "   % Time: 1548s | Batch:  971 | Train loss: 0.3374\n",
      "   % Time: 1564s | Batch:  981 | Train loss: 0.3983\n",
      "   % Time: 1580s | Batch:  991 | Train loss: 0.3694\n",
      "   % Time: 1595s | Batch: 1001 | Train loss: 0.3175\n",
      "   % Time: 1611s | Batch: 1011 | Train loss: 0.3451\n",
      "   % Time: 1627s | Batch: 1021 | Train loss: 0.3670\n",
      "   % Time: 1643s | Batch: 1031 | Train loss: 0.4279\n",
      "   % Time: 1659s | Batch: 1041 | Train loss: 0.3475\n",
      "   % Time: 1675s | Batch: 1051 | Train loss: 0.3714\n",
      "   % Time: 1691s | Batch: 1061 | Train loss: 0.3643\n",
      "   % Time: 1707s | Batch: 1071 | Train loss: 0.3800\n",
      "   % Time: 1723s | Batch: 1081 | Train loss: 0.3441\n",
      "   % Time: 1739s | Batch: 1091 | Train loss: 0.4263\n",
      "   % Time: 1755s | Batch: 1101 | Train loss: 0.4028\n",
      "   % Time: 1771s | Batch: 1111 | Train loss: 0.3197\n",
      "   % Time: 1788s | Batch: 1121 | Train loss: 0.3409\n",
      "   % Time: 1804s | Batch: 1131 | Train loss: 0.3626\n",
      "==========\n",
      "   % Epoch: 4 | Time: 1809s | Train loss: 0.3627 | Val loss: 27.1195\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "if args.train:\n",
    "    for epoch in range(1, args.n_epoch+1):\n",
    "        scheduler.step()\n",
    "        print(\"=> EPOCH {} with lr {}\".format(epoch, scheduler.get_lr()[0]))\n",
    "        val_loss = train(scaled_data, scaler, features,\n",
    "                         model, criterion, optimizer)\n",
    "#        save_model(model, epoch, val_loss)\n",
    "else:\n",
    "    model_file = os.path.join(args.intermediate_path, args.model_name)\n",
    "    model.load_state_dict(torch.load(model_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-04T05:11:57.600475Z",
     "start_time": "2017-09-04T14:11:57.596444+09:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# prediction = forecast(scaled_data, scaler, features, model)\n",
    "# prediction_file = os.path.join(args.intermediate_path,\n",
    "#                                'prediction_seed{}.pkl'.format(args.seed))\n",
    "\n",
    "# with open(prediction_file, 'wb') as f:\n",
    "#     pickle.dump(prediction, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pydata]",
   "language": "python",
   "name": "conda-env-pydata-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
