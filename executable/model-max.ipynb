{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-30T13:48:22.488637Z",
     "start_time": "2017-08-30T22:48:21.351514+09:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import numpy as np; np.seterr(invalid='ignore')\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-30T13:48:22.547130Z",
     "start_time": "2017-08-30T22:48:22.491222+09:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "parser = {\n",
    "    'data_path': '../data/wttsf/',\n",
    "    'train_file': 'train_1.csv',\n",
    "    'intermediate_path': '../intermediate/',\n",
    "    'n_epoch': 4,\n",
    "    'future': 70,\n",
    "    'batch_size': 128,\n",
    "    'hidden_size': 256,\n",
    "    'log_every': 10,\n",
    "    'read_from_file': True,\n",
    "    'train': True,\n",
    "    'model_name': '',\n",
    "    'cuda': True,\n",
    "    'seed': 30082017,\n",
    "}\n",
    "args = argparse.Namespace(**parser)\n",
    "\n",
    "args.cuda = args.cuda and torch.cuda.is_available()\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "args.intermediate_path = os.path.join(args.intermediate_path, str(args.seed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-30T13:48:22.764465Z",
     "start_time": "2017-08-30T22:48:22.549788+09:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class DenseLSTMForecast(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(DenseLSTMForecast, self).__init__()\n",
    "        self.lstm1 = nn.LSTMCell(1, hidden_size, bias=False)\n",
    "        self.lstm2 = nn.LSTMCell(hidden_size+1, hidden_size, bias=False)\n",
    "        self.lstm3 = nn.LSTMCell(2*hidden_size+1, hidden_size, bias=False)\n",
    "        self.linear = nn.Linear(3*hidden_size+1, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    def forward(self, x, future=1):\n",
    "        o = []\n",
    "        tt = torch.cuda if args.cuda else torch\n",
    "        h1_t = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())\n",
    "        c1_t = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())\n",
    "        h2_t = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())\n",
    "        c2_t = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())\n",
    "        h3_t = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())\n",
    "        c3_t = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())\n",
    "        \n",
    "        for x_t in x.chunk(x.size(1), dim=1):\n",
    "            x_t = x_t.squeeze(dim=1)\n",
    "            h1_t, c1_t = self.lstm1(x_t, (h1_t, c1_t))\n",
    "            h1d_t = torch.cat([x_t, h1_t], dim=1)\n",
    "            h2_t, c2_t = self.lstm2(h1d_t, (h2_t, c2_t))\n",
    "            h2d_t = torch.cat([x_t, h1_t, h2_t], dim=1)\n",
    "            h3_t, c3_t = self.lstm3(h2d_t, (h3_t, c3_t))\n",
    "            h3d_t = torch.cat([x_t, h1_t, h2_t, h3_t], dim=1)\n",
    "            o_t = self.linear(h3d_t)\n",
    "            o.append(o_t)\n",
    "            \n",
    "        for i in range(future-1):\n",
    "            h1_t, c1_t = self.lstm1(o_t, (h1_t, c1_t))\n",
    "            h1d_t = torch.cat([o_t, h1_t], dim=1)\n",
    "            h2_t, c2_t = self.lstm2(h1d_t, (h2_t, c2_t))\n",
    "            h2d_t = torch.cat([o_t, h1_t, h2_t], dim=1)\n",
    "            h3_t, c3_t = self.lstm3(h2d_t, (h3_t, c3_t))\n",
    "            h3d_t = torch.cat([o_t, h1_t, h2_t, h3_t], dim=1)\n",
    "            o_t = self.linear(h3d_t)\n",
    "            o.append(o_t)\n",
    "\n",
    "        return torch.stack(o, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-30T13:48:22.816182Z",
     "start_time": "2017-08-30T22:48:22.767021+09:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def smape(y_pred, y_true):\n",
    "    raw_smape = np.abs(y_true - y_pred) / (np.abs(y_true) + np.abs(y_pred))\n",
    "    kaggle_smape = np.nan_to_num(raw_smape)\n",
    "    return np.mean(kaggle_smape) * 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-30T13:48:22.900531Z",
     "start_time": "2017-08-30T22:48:22.818725+09:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    raw_data_file = os.path.join(args.intermediate_path,\n",
    "                                 'raw_data.pkl')\n",
    "    scaled_data_file = os.path.join(args.intermediate_path,\n",
    "                                    'scaled_data.pkl')\n",
    "    scaler_file = os.path.join(args.intermediate_path, 'scaler.pkl')\n",
    "    if not args.read_from_file:\n",
    "        data_df = pd.read_csv(os.path.join(args.data_path, args.train_file),\n",
    "                              index_col='Page')\n",
    "        raw_data = np.nan_to_num(data_df.values.astype('float32'))\n",
    "        data = np.log1p(raw_data)\n",
    "        scaler = MaxAbsScaler()\n",
    "        scaler.fit(np.swapaxes(data[:, :-args.future], 0, 1))\n",
    "        scaled_data = scaler.transform(np.swapaxes(data, 0, 1))\n",
    "        scaled_data = np.swapaxes(scaled_data, 0, 1)\n",
    "        \n",
    "        with open(raw_data_file, 'wb') as f:\n",
    "            pickle.dump(raw_data, f)\n",
    "        with open(scaled_data_file, 'wb') as f:\n",
    "            pickle.dump(scaled_data, f)\n",
    "        with open(scaler_file, 'wb') as f:\n",
    "            pickle.dump(scaler, f)\n",
    "    else:\n",
    "        with open(raw_data_file, 'rb') as f:\n",
    "            raw_data = pickle.load(f)\n",
    "        with open(scaled_data_file, 'rb') as f:\n",
    "            scaled_data = pickle.load(f)\n",
    "        with open(scaler_file, 'rb') as f:\n",
    "            scaler = pickle.load(f)\n",
    "    return raw_data, scaled_data, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-30T13:48:23.074776Z",
     "start_time": "2017-08-30T22:48:22.903242+09:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def train(raw_data, scaled_data, scaler, model, criterion, optimizer, epoch):\n",
    "    print(\"=> EPOCH {}\".format(epoch))\n",
    "    p = np.random.permutation(raw_data.shape[0])\n",
    "    inverse_p = np.argsort(p)\n",
    "    \n",
    "    input_tensor = torch.from_numpy(scaled_data[p, :-1]).unsqueeze(2)\n",
    "    target_tensor = torch.from_numpy(scaled_data[p, 1:]).unsqueeze(2)\n",
    "    dataset = TensorDataset(input_tensor, target_tensor)\n",
    "    data_loader = DataLoader(dataset, args.batch_size)\n",
    "    \n",
    "    train_loss = 0\n",
    "    val_output_list = []\n",
    "    init_time = time.time()\n",
    "    for i, (inputt, target) in enumerate(data_loader):\n",
    "        if args.cuda:\n",
    "            inputt = inputt.cuda()\n",
    "            target = target.cuda()\n",
    "        inputt = Variable(inputt)\n",
    "        target = Variable(target)\n",
    "        \n",
    "        output = model(inputt)\n",
    "        pos = np.random.randint(args.future, raw_data.shape[1]-2*args.future)\n",
    "        loss = criterion(output[:, pos:pos+args.future],\n",
    "                         target[:, pos:pos+args.future])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.data[0] * inputt.size(0)\n",
    "        val_output_list.append(output[:,-args.future:]\n",
    "                               .data.squeeze(2).cpu().numpy())\n",
    "        \n",
    "        if i % args.log_every == 0:\n",
    "            print(\"   % Time: {:4.0f}s | Batch: {:4} | Loss: {:.4f}\"\n",
    "                  .format(time.time()-init_time, i+1, loss.data[0]))\n",
    "        \n",
    "    val_output_all = np.concatenate(val_output_list, axis=0)[inverse_p]\n",
    "    prediction = np.swapaxes(scaler.inverse_transform(\n",
    "            np.swapaxes(val_output_all, 0, 1)), 0, 1)\n",
    "    prediction = np.exp(prediction) - 1\n",
    "    \n",
    "    train_loss /= raw_data.shape[0]\n",
    "    val_loss = smape(prediction, raw_data[:, -args.future:])\n",
    "    \n",
    "    print(\"=\"*10)\n",
    "    print(\"   % Epoch: {} | Time: {:4.0f}s | \"\n",
    "          \"Train loss: {:.4f} | Val loss: {:.4f}\"\n",
    "          .format(epoch, time.time()-init_time, train_loss , val_loss))\n",
    "    print(\"=\"*10)\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-30T13:48:23.123865Z",
     "start_time": "2017-08-30T22:48:23.077287+09:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def forecast(scaled_data, scaler, model):\n",
    "    input_tensor = torch.from_numpy(scaled_data).unsqueeze(2)\n",
    "    target_tensor = torch.zeros(input_tensor.size(0))\n",
    "    dataset = torch.utils.data.TensorDataset(input_tensor, target_tensor)\n",
    "    data_loader = DataLoader(dataset, args.batch_size)\n",
    "    \n",
    "    output_list = []\n",
    "    for inputt, _ in data_loader:\n",
    "        if args.cuda:\n",
    "            inputt = inputt.cuda()\n",
    "        inputt = Variable(inputt)\n",
    "        output = model(inputt, args.future)\n",
    "        output_list.append(output.data.squeeze(2).cpu().numpy()\n",
    "                           [:, -args.future:])\n",
    "        \n",
    "    output_all = np.concatenate(output_list, axis=0)\n",
    "    prediction = np.swapaxes(scaler.inverse_transform(\n",
    "            np.swapaxes(output_all, 0, 1)), 0, 1)\n",
    "    prediction = np.exp(prediction) - 1\n",
    "    prediction[prediction < 0] = 0\n",
    "    prediction = np.around(prediction)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-30T13:48:23.172392Z",
     "start_time": "2017-08-30T22:48:23.126479+09:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def save_model(model, epoch, val_loss):\n",
    "    model_file = os.path.join(args.intermediate_path,\n",
    "                              \"model_seed{}_epoch{}_loss_{:.4f}.pth\"\n",
    "                              .format(args.seed, epoch, val_loss))\n",
    "    torch.save(model.state_dict(), os.path.join(model_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-30T13:48:27.112727Z",
     "start_time": "2017-08-30T22:48:23.174993+09:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "raw_data, scaled_data, scaler = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-30T13:48:29.444763Z",
     "start_time": "2017-08-30T22:48:27.115407+09:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model = DenseLSTMForecast(args.hidden_size)\n",
    "if args.cuda:\n",
    "    model.cuda()\n",
    "criterion = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-30T13:48:29.451793Z",
     "start_time": "2017-08-30T22:48:29.447306+09:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-30T14:06:24.488095Z",
     "start_time": "2017-08-30T22:48:29.454045+09:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> EPOCH 1\n",
      "   % Time:    3s | Batch:    1 | Loss: 0.7329\n",
      "   % Time:   25s | Batch:   11 | Loss: 0.4946\n",
      "   % Time:   47s | Batch:   21 | Loss: 0.4393\n",
      "   % Time:   70s | Batch:   31 | Loss: 0.4425\n",
      "   % Time:   92s | Batch:   41 | Loss: 0.4393\n",
      "   % Time:  115s | Batch:   51 | Loss: 0.4688\n",
      "   % Time:  137s | Batch:   61 | Loss: 0.4569\n",
      "   % Time:  159s | Batch:   71 | Loss: 0.4631\n",
      "   % Time:  182s | Batch:   81 | Loss: 0.4673\n",
      "   % Time:  204s | Batch:   91 | Loss: 0.4480\n",
      "   % Time:  226s | Batch:  101 | Loss: 0.4528\n",
      "   % Time:  248s | Batch:  111 | Loss: 0.4227\n",
      "   % Time:  271s | Batch:  121 | Loss: 0.4448\n",
      "   % Time:  293s | Batch:  131 | Loss: 0.4460\n",
      "   % Time:  315s | Batch:  141 | Loss: 0.4196\n",
      "   % Time:  337s | Batch:  151 | Loss: 0.4434\n",
      "   % Time:  360s | Batch:  161 | Loss: 0.4278\n",
      "   % Time:  382s | Batch:  171 | Loss: 0.4168\n",
      "   % Time:  404s | Batch:  181 | Loss: 0.4547\n",
      "   % Time:  426s | Batch:  191 | Loss: 0.3765\n",
      "   % Time:  449s | Batch:  201 | Loss: 0.4443\n",
      "   % Time:  471s | Batch:  211 | Loss: 0.4116\n",
      "   % Time:  493s | Batch:  221 | Loss: 0.4193\n",
      "   % Time:  515s | Batch:  231 | Loss: 0.3715\n",
      "   % Time:  537s | Batch:  241 | Loss: 0.4342\n",
      "   % Time:  560s | Batch:  251 | Loss: 0.4355\n",
      "   % Time:  582s | Batch:  261 | Loss: 0.4183\n",
      "   % Time:  604s | Batch:  271 | Loss: 0.4247\n",
      "   % Time:  626s | Batch:  281 | Loss: 0.4380\n",
      "   % Time:  649s | Batch:  291 | Loss: 0.3943\n",
      "   % Time:  671s | Batch:  301 | Loss: 0.4038\n",
      "   % Time:  693s | Batch:  311 | Loss: 0.3894\n",
      "   % Time:  715s | Batch:  321 | Loss: 0.4004\n",
      "   % Time:  738s | Batch:  331 | Loss: 0.3920\n",
      "   % Time:  760s | Batch:  341 | Loss: 0.3565\n",
      "   % Time:  782s | Batch:  351 | Loss: 0.3499\n",
      "   % Time:  805s | Batch:  361 | Loss: 0.3600\n",
      "   % Time:  827s | Batch:  371 | Loss: 0.4036\n",
      "   % Time:  849s | Batch:  381 | Loss: 0.4153\n",
      "   % Time:  871s | Batch:  391 | Loss: 0.4049\n",
      "   % Time:  893s | Batch:  401 | Loss: 0.3959\n",
      "   % Time:  915s | Batch:  411 | Loss: 0.4429\n",
      "   % Time:  937s | Batch:  421 | Loss: 0.4206\n",
      "   % Time:  959s | Batch:  431 | Loss: 0.4202\n",
      "   % Time:  982s | Batch:  441 | Loss: 0.4367\n",
      "   % Time: 1004s | Batch:  451 | Loss: 0.4150\n",
      "   % Time: 1026s | Batch:  461 | Loss: 0.4330\n",
      "   % Time: 1048s | Batch:  471 | Loss: 0.3729\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-960f7915ac5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         val_loss = train(raw_data, scaled_data, scaler,\n\u001b[0;32m----> 5\u001b[0;31m                          model, criterion, optimizer, epoch)\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-6516b4734178>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(raw_data, scaled_data, scaler, model, criterion, optimizer, epoch)\u001b[0m\n\u001b[1;32m     24\u001b[0m                          target[:, pos:pos+args.future])\n\u001b[1;32m     25\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0minputt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pydata/lib/python3.5/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \"\"\"\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pydata/lib/python3.5/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 98\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if args.train:\n",
    "    for epoch in range(1, args.n_epoch+1):\n",
    "        scheduler.step()\n",
    "        val_loss = train(raw_data, scaled_data, scaler,\n",
    "                         model, criterion, optimizer, epoch)\n",
    "        save_model(model, epoch, val_loss)\n",
    "else:\n",
    "    model_file = os.path.join(args.intermediate_path, args.model_name)\n",
    "    model.load_state_dict(torch.load(model_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-30T14:06:24.489172Z",
     "start_time": "2017-08-30T13:48:22.185Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "prediction = forecast(scaled_data, scaler, model)\n",
    "prediction_file = os.path.join(args.intermediate_path,\n",
    "                               'prediction_seed{}.pkl'.format(args.seed))\n",
    "\n",
    "with open(prediction_file, 'wb') as f:\n",
    "    pickle.dump(prediction, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pydata]",
   "language": "python",
   "name": "conda-env-pydata-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
