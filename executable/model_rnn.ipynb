{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-11T05:05:46.300794Z",
     "start_time": "2017-09-11T05:05:45.825680Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import numpy as np; np.seterr(invalid='ignore')\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-11T05:05:46.335050Z",
     "start_time": "2017-09-11T05:05:46.302119Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parser = {\n",
    "    'data_path': '../data/wttsf/',\n",
    "    'train_file': 'train_2.csv',\n",
    "    'key_file': 'key_2.csv',\n",
    "    'intermediate_path': '../intermediate/',\n",
    "    'train_len': 91,\n",
    "    'train_skip': 91,\n",
    "    'val_len': 74,\n",
    "    'offset': 793,\n",
    "    'batch_size': 256,\n",
    "    'hidden_size': 256,\n",
    "    'log_every': 10,\n",
    "    'read_from_file': False,\n",
    "    'train': True,\n",
    "    'model_name': 'model_20170910_epoch6_loss39.2927.pth',\n",
    "    'forecast': True,\n",
    "    'cuda': True,\n",
    "    'seed': 20170911,\n",
    "}\n",
    "args = argparse.Namespace(**parser)\n",
    "\n",
    "args.cuda = args.cuda and torch.cuda.is_available()\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "args.intermediate_path = os.path.join(args.intermediate_path, str(args.seed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-11T05:05:46.418892Z",
     "start_time": "2017-09-11T05:05:46.336657Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DenseLSTMForecast(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(DenseLSTMForecast, self).__init__()\n",
    "        self.lstm1 = nn.LSTMCell(1, hidden_size)\n",
    "        self.lstm2 = nn.LSTMCell(hidden_size+1, hidden_size)\n",
    "        self.lstm3 = nn.LSTMCell(2*hidden_size+1, hidden_size)\n",
    "        self.linear = nn.Linear(3*hidden_size+1, 1)\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    def forward(self, x, future=0):\n",
    "        o = []\n",
    "        tt = torch.cuda if args.cuda else torch\n",
    "        h1_t = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())\n",
    "        c1_t = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())\n",
    "        h2_t = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())\n",
    "        c2_t = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())\n",
    "        h3_t = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())\n",
    "        c3_t = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())\n",
    "        \n",
    "        for x_t in x.chunk(x.size(1), dim=1):\n",
    "            x_t = x_t.squeeze(dim=1)\n",
    "            h1_t, c1_t = self.lstm1(x_t, (h1_t, c1_t))\n",
    "            h1d_t = torch.cat([x_t, h1_t], dim=1)\n",
    "            h2_t, c2_t = self.lstm2(h1d_t, (h2_t, c2_t))\n",
    "            h2d_t = torch.cat([x_t, h1_t, h2_t], dim=1)\n",
    "            h3_t, c3_t = self.lstm3(h2d_t, (h3_t, c3_t))\n",
    "            h3d_t = torch.cat([x_t, h1_t, h2_t, h3_t], dim=1)\n",
    "            o_t = self.linear(h3d_t)\n",
    "            o.append(o_t)\n",
    "\n",
    "            \n",
    "        for i in range(future):\n",
    "            h1_t, c1_t = self.lstm1(o_t, (h1_t, c1_t))\n",
    "            h1d_t = torch.cat([o_t, h1_t], dim=1)\n",
    "            h2_t, c2_t = self.lstm2(h1d_t, (h2_t, c2_t))\n",
    "            h2d_t = torch.cat([o_t, h1_t, h2_t], dim=1)\n",
    "            h3_t, c3_t = self.lstm3(h2d_t, (h3_t, c3_t))\n",
    "            h3d_t = torch.cat([o_t, h1_t, h2_t, h3_t], dim=1)\n",
    "            o_t = self.linear(h3d_t)\n",
    "            o.append(o_t)\n",
    "\n",
    "        return torch.stack(o, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-11T05:05:46.485527Z",
     "start_time": "2017-09-11T05:05:46.420331Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def smape(y_pred, y_true):\n",
    "    y_pred = np.around(y_pred)\n",
    "    denominator = y_true + y_pred\n",
    "    diff = np.abs(y_true - y_pred) / denominator\n",
    "    diff[denominator == 0] = 0\n",
    "    return 200 * np.nanmean(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-11T05:05:46.553384Z",
     "start_time": "2017-09-11T05:05:46.487533Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    raw_data_file = os.path.join(args.intermediate_path, 'raw_data.pkl')\n",
    "    scaled_data_file = os.path.join(args.intermediate_path,\n",
    "                                    'scaled_data.pkl')\n",
    "    scaler_file = os.path.join(args.intermediate_path, 'scaler.pkl')\n",
    "    \n",
    "    if not args.read_from_file:\n",
    "        data_df = pd.read_csv(os.path.join(args.data_path, args.train_file),\n",
    "                              index_col='Page')\n",
    "        raw_data = data_df.values.copy()\n",
    "        data_df = data_df.fillna(method='ffill', axis=1).fillna(\n",
    "            method='bfill', axis=1)\n",
    "        data = np.nan_to_num(data_df.values.astype('float32'))\n",
    "        data = np.log1p(data)\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(np.swapaxes(data, 0, 1))\n",
    "        scaled_data = scaler.transform(np.swapaxes(data, 0, 1))\n",
    "        scaled_data = np.swapaxes(scaled_data, 0, 1)\n",
    "        \n",
    "        with open(raw_data_file, 'wb') as f:\n",
    "            pickle.dump(raw_data, f)\n",
    "        with open(scaled_data_file, 'wb') as f:\n",
    "            pickle.dump(scaled_data, f)\n",
    "        with open(scaler_file, 'wb') as f:\n",
    "            pickle.dump(scaler, f)\n",
    "    else:\n",
    "        with open(raw_data_file, 'rb') as f:\n",
    "            raw_data = pickle.load(f)\n",
    "        with open(scaled_data_file, 'rb') as f:\n",
    "            scaled_data = pickle.load(f)\n",
    "        with open(scaler_file, 'rb') as f:\n",
    "            scaler = pickle.load(f)\n",
    "    return raw_data, scaled_data, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-11T05:05:46.623964Z",
     "start_time": "2017-09-11T05:05:46.554701Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(raw_data, scaled_data, scaler, model, criterion, optimizer):\n",
    "    p = np.random.permutation(scaled_data.shape[0])\n",
    "    inverse_p = np.argsort(p)\n",
    "    \n",
    "    input_tensor = torch.from_numpy(\n",
    "        scaled_data[p, :(args.offset-1)]).unsqueeze(2)\n",
    "    target_tensor = torch.from_numpy(\n",
    "        scaled_data[p, 1:args.offset]).unsqueeze(2)\n",
    "    dataset = TensorDataset(input_tensor, target_tensor)\n",
    "    data_loader = DataLoader(dataset, args.batch_size)\n",
    "    \n",
    "    train_loss = 0\n",
    "    val_output_list = []\n",
    "    init_time = time.time()\n",
    "    for i, (inputt, target) in enumerate(data_loader):\n",
    "        if args.cuda:\n",
    "            inputt = inputt.cuda()\n",
    "            target = target.cuda()\n",
    "        inputt = Variable(inputt)\n",
    "        target = Variable(target)\n",
    "        \n",
    "#        output = model(inputt, future=args.val_len)\n",
    "        output = model(inputt)\n",
    "        pos = np.random.randint(args.train_skip,\n",
    "                                inputt.size(1)-args.train_len+1)\n",
    "        loss = criterion(output[:, pos:pos+args.train_len],\n",
    "                         target[:, pos:pos+args.train_len])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm(model.parameters(), 3, 'inf')\n",
    "        optimizer.step()\n",
    "        train_loss += loss.data[0] * inputt.size(0)\n",
    "#        val_output_list.append(output[:, -args.val_len:]\n",
    "#                               .data.squeeze(2).cpu().numpy())\n",
    "        \n",
    "        if i % args.log_every == 0:\n",
    "            print(\"   % Time: {:4.0f}s | Batch: {:4} | \"\n",
    "                  \"Train loss: {:.4f}\".format(\n",
    "                      time.time()-init_time, i+1, loss.data[0]))\n",
    "        \n",
    "#    val_output_all = np.concatenate(val_output_list, axis=0)[inverse_p]\n",
    "#    prediction = np.swapaxes(scaler.inverse_transform(\n",
    "#            np.swapaxes(val_output_all, 0, 1)), 0, 1)\n",
    "#    prediction = np.clip(np.exp(prediction)-1, 0, None)\n",
    "#    var_target = raw_data[:, args.offset:args.offset+args.val_len]\n",
    "    \n",
    "    train_loss /= scaled_data.shape[0]\n",
    "#    val_loss = smape(prediction, var_target)\n",
    "    val_loss = 0\n",
    "    print(\"=\"*10)\n",
    "    print(\"   % Epoch: {} | Time: {:4.0f}s | \"\n",
    "          \"Train loss: {:.4f} | Val loss: {:.4f}\"\n",
    "          .format(epoch, time.time()-init_time, train_loss, val_loss))\n",
    "    print(\"=\"*10)\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-11T05:05:46.705181Z",
     "start_time": "2017-09-11T05:05:46.625096Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forecast(raw_data, scaled_data, scaler, model):\n",
    "    input_tensor = torch.from_numpy(scaled_data[:,\n",
    "            :args.offset]).unsqueeze(2)\n",
    "    target_tensor = torch.zeros(input_tensor.size(0))\n",
    "    dataset = torch.utils.data.TensorDataset(input_tensor, target_tensor)\n",
    "    data_loader = DataLoader(dataset, 128)\n",
    "    \n",
    "    output_list = []\n",
    "    for i, (inputt, _) in enumerate(data_loader):\n",
    "        if args.cuda:\n",
    "            inputt = inputt.cuda()\n",
    "        inputt = Variable(inputt)\n",
    "        output = model(inputt, args.val_len)\n",
    "        output_list.append(output.data.squeeze(2).cpu().numpy()\n",
    "                           [:, -args.val_len:])\n",
    "        \n",
    "    output_all = np.concatenate(output_list, axis=0)\n",
    "    prediction = np.swapaxes(scaler.inverse_transform(\n",
    "            np.swapaxes(output_all, 0, 1)), 0, 1)\n",
    "\n",
    "    prediction = np.clip(np.exp(prediction) - 1, 0, None)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-11T05:05:46.774300Z",
     "start_time": "2017-09-11T05:05:46.706955Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_model(model, epoch, loss):\n",
    "    model_file = os.path.join(args.intermediate_path,\n",
    "                              \"model_{}_epoch{}_loss{:.4f}.pth\"\n",
    "                              .format(args.seed, epoch, loss))\n",
    "    torch.save(model.state_dict(), os.path.join(model_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-11T05:06:13.162179Z",
     "start_time": "2017-09-11T05:05:46.779009Z"
    }
   },
   "outputs": [],
   "source": [
    "raw_data, scaled_data, scaler = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-11T05:06:14.165480Z",
     "start_time": "2017-09-11T05:06:13.163520Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = DenseLSTMForecast(args.hidden_size)\n",
    "if args.cuda:\n",
    "    model.cuda()\n",
    "criterion = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-11T05:06:14.853302Z",
     "start_time": "2017-09-11T05:06:14.166765Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = optim.RMSprop(model.parameters(), lr=0.001)\n",
    "scheduler = MultiStepLR(optimizer, milestones=[2, 4, 6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-11T06:10:58.129192Z",
     "start_time": "2017-09-11T05:06:14.858153Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> EPOCH 1 with lr [0.001]\n",
      "   % Time:    2s | Batch:    1 | Train loss: 0.7122\n",
      "   % Time:   14s | Batch:   11 | Train loss: 0.5575\n",
      "   % Time:   25s | Batch:   21 | Train loss: 0.4695\n",
      "   % Time:   37s | Batch:   31 | Train loss: 0.4285\n",
      "   % Time:   48s | Batch:   41 | Train loss: 0.4239\n",
      "   % Time:   60s | Batch:   51 | Train loss: 0.4024\n",
      "   % Time:   71s | Batch:   61 | Train loss: 0.4192\n",
      "   % Time:   83s | Batch:   71 | Train loss: 0.4230\n",
      "   % Time:   94s | Batch:   81 | Train loss: 0.4306\n",
      "   % Time:  106s | Batch:   91 | Train loss: 0.3910\n",
      "   % Time:  118s | Batch:  101 | Train loss: 0.4088\n",
      "   % Time:  129s | Batch:  111 | Train loss: 0.4025\n",
      "   % Time:  141s | Batch:  121 | Train loss: 0.3983\n",
      "   % Time:  152s | Batch:  131 | Train loss: 0.4136\n",
      "   % Time:  164s | Batch:  141 | Train loss: 0.4246\n",
      "   % Time:  176s | Batch:  151 | Train loss: 0.4049\n",
      "   % Time:  187s | Batch:  161 | Train loss: 0.4102\n",
      "   % Time:  199s | Batch:  171 | Train loss: 0.4140\n",
      "   % Time:  210s | Batch:  181 | Train loss: 0.4525\n",
      "   % Time:  222s | Batch:  191 | Train loss: 0.4151\n",
      "   % Time:  233s | Batch:  201 | Train loss: 0.3921\n",
      "   % Time:  245s | Batch:  211 | Train loss: 0.4312\n",
      "   % Time:  257s | Batch:  221 | Train loss: 0.4108\n",
      "   % Time:  268s | Batch:  231 | Train loss: 0.4306\n",
      "   % Time:  280s | Batch:  241 | Train loss: 0.4217\n",
      "   % Time:  291s | Batch:  251 | Train loss: 0.3815\n",
      "   % Time:  303s | Batch:  261 | Train loss: 0.4109\n",
      "   % Time:  314s | Batch:  271 | Train loss: 0.3875\n",
      "   % Time:  326s | Batch:  281 | Train loss: 0.4194\n",
      "   % Time:  338s | Batch:  291 | Train loss: 0.4028\n",
      "   % Time:  349s | Batch:  301 | Train loss: 0.4379\n",
      "   % Time:  361s | Batch:  311 | Train loss: 0.4381\n",
      "   % Time:  372s | Batch:  321 | Train loss: 0.4208\n",
      "   % Time:  384s | Batch:  331 | Train loss: 0.4232\n",
      "   % Time:  395s | Batch:  341 | Train loss: 0.4212\n",
      "   % Time:  407s | Batch:  351 | Train loss: 0.3922\n",
      "   % Time:  419s | Batch:  361 | Train loss: 0.3796\n",
      "   % Time:  430s | Batch:  371 | Train loss: 0.4001\n",
      "   % Time:  442s | Batch:  381 | Train loss: 0.4078\n",
      "   % Time:  453s | Batch:  391 | Train loss: 0.4103\n",
      "   % Time:  465s | Batch:  401 | Train loss: 0.4203\n",
      "   % Time:  476s | Batch:  411 | Train loss: 0.4069\n",
      "   % Time:  488s | Batch:  421 | Train loss: 0.4127\n",
      "   % Time:  499s | Batch:  431 | Train loss: 0.4172\n",
      "   % Time:  511s | Batch:  441 | Train loss: 0.4505\n",
      "   % Time:  522s | Batch:  451 | Train loss: 0.4200\n",
      "   % Time:  534s | Batch:  461 | Train loss: 0.4104\n",
      "   % Time:  545s | Batch:  471 | Train loss: 0.4208\n",
      "   % Time:  557s | Batch:  481 | Train loss: 0.4066\n",
      "   % Time:  568s | Batch:  491 | Train loss: 0.4456\n",
      "   % Time:  580s | Batch:  501 | Train loss: 0.4138\n",
      "   % Time:  591s | Batch:  511 | Train loss: 0.4178\n",
      "   % Time:  603s | Batch:  521 | Train loss: 0.4135\n",
      "   % Time:  614s | Batch:  531 | Train loss: 0.3955\n",
      "   % Time:  626s | Batch:  541 | Train loss: 0.4270\n",
      "   % Time:  638s | Batch:  551 | Train loss: 0.4003\n",
      "   % Time:  649s | Batch:  561 | Train loss: 0.4030\n",
      "==========\n",
      "   % Epoch: 1 | Time:  656s | Train loss: 61207.8219 | Val loss: 0.0000\n",
      "==========\n",
      "=> EPOCH 2 with lr [0.001]\n",
      "   % Time:    1s | Batch:    1 | Train loss: 0.4091\n",
      "   % Time:   13s | Batch:   11 | Train loss: 0.4252\n",
      "   % Time:   24s | Batch:   21 | Train loss: 0.4155\n",
      "   % Time:   36s | Batch:   31 | Train loss: 0.4114\n",
      "   % Time:   47s | Batch:   41 | Train loss: 0.3919\n",
      "   % Time:   59s | Batch:   51 | Train loss: 0.4171\n",
      "   % Time:   70s | Batch:   61 | Train loss: 0.4068\n",
      "   % Time:   82s | Batch:   71 | Train loss: 0.3950\n",
      "   % Time:   93s | Batch:   81 | Train loss: 0.4034\n",
      "   % Time:  105s | Batch:   91 | Train loss: 0.4312\n",
      "   % Time:  116s | Batch:  101 | Train loss: 0.3792\n",
      "   % Time:  128s | Batch:  111 | Train loss: 0.3868\n",
      "   % Time:  139s | Batch:  121 | Train loss: 0.3941\n",
      "   % Time:  151s | Batch:  131 | Train loss: 0.3979\n",
      "   % Time:  162s | Batch:  141 | Train loss: 0.3700\n",
      "   % Time:  174s | Batch:  151 | Train loss: 0.4132\n",
      "   % Time:  185s | Batch:  161 | Train loss: 0.4026\n",
      "   % Time:  197s | Batch:  171 | Train loss: 0.4076\n",
      "   % Time:  209s | Batch:  181 | Train loss: 0.4095\n",
      "   % Time:  221s | Batch:  191 | Train loss: 0.3746\n",
      "   % Time:  232s | Batch:  201 | Train loss: 0.3790\n",
      "   % Time:  244s | Batch:  211 | Train loss: 0.3961\n",
      "   % Time:  256s | Batch:  221 | Train loss: 0.4136\n",
      "   % Time:  267s | Batch:  231 | Train loss: 0.4104\n",
      "   % Time:  279s | Batch:  241 | Train loss: 0.4031\n",
      "   % Time:  291s | Batch:  251 | Train loss: 0.3855\n",
      "   % Time:  302s | Batch:  261 | Train loss: 0.3993\n",
      "   % Time:  314s | Batch:  271 | Train loss: 0.3978\n",
      "   % Time:  326s | Batch:  281 | Train loss: 0.3972\n",
      "   % Time:  338s | Batch:  291 | Train loss: 0.3934\n",
      "   % Time:  350s | Batch:  301 | Train loss: 0.3965\n",
      "   % Time:  363s | Batch:  311 | Train loss: 0.3788\n",
      "   % Time:  375s | Batch:  321 | Train loss: 0.4205\n",
      "   % Time:  388s | Batch:  331 | Train loss: 0.3809\n",
      "   % Time:  400s | Batch:  341 | Train loss: 0.3857\n",
      "   % Time:  412s | Batch:  351 | Train loss: 0.3524\n",
      "   % Time:  425s | Batch:  361 | Train loss: 0.3753\n",
      "   % Time:  437s | Batch:  371 | Train loss: 0.3709\n",
      "   % Time:  449s | Batch:  381 | Train loss: 0.4148\n",
      "   % Time:  461s | Batch:  391 | Train loss: 0.3865\n",
      "   % Time:  473s | Batch:  401 | Train loss: 0.3839\n",
      "   % Time:  485s | Batch:  411 | Train loss: 0.3773\n",
      "   % Time:  497s | Batch:  421 | Train loss: 0.4097\n",
      "   % Time:  509s | Batch:  431 | Train loss: 0.3646\n",
      "   % Time:  521s | Batch:  441 | Train loss: 0.4089\n",
      "   % Time:  533s | Batch:  451 | Train loss: 0.4005\n",
      "   % Time:  545s | Batch:  461 | Train loss: 0.3875\n",
      "   % Time:  557s | Batch:  471 | Train loss: 0.3813\n",
      "   % Time:  569s | Batch:  481 | Train loss: 0.3940\n",
      "   % Time:  581s | Batch:  491 | Train loss: 0.4166\n",
      "   % Time:  593s | Batch:  501 | Train loss: 0.3645\n",
      "   % Time:  605s | Batch:  511 | Train loss: 0.4197\n",
      "   % Time:  617s | Batch:  521 | Train loss: 0.3837\n",
      "   % Time:  629s | Batch:  531 | Train loss: 0.3894\n",
      "   % Time:  641s | Batch:  541 | Train loss: 0.3677\n",
      "   % Time:  653s | Batch:  551 | Train loss: 0.4140\n",
      "   % Time:  665s | Batch:  561 | Train loss: 0.3874\n",
      "==========\n",
      "   % Epoch: 2 | Time:  672s | Train loss: 56982.6621 | Val loss: 0.0000\n",
      "==========\n",
      "=> EPOCH 3 with lr [0.0001]\n",
      "   % Time:    1s | Batch:    1 | Train loss: 0.4101\n",
      "   % Time:   13s | Batch:   11 | Train loss: 0.3643\n",
      "   % Time:   25s | Batch:   21 | Train loss: 0.3719\n",
      "   % Time:   36s | Batch:   31 | Train loss: 0.3870\n",
      "   % Time:   48s | Batch:   41 | Train loss: 0.3641\n",
      "   % Time:   60s | Batch:   51 | Train loss: 0.3925\n",
      "   % Time:   71s | Batch:   61 | Train loss: 0.3662\n",
      "   % Time:   83s | Batch:   71 | Train loss: 0.3888\n",
      "   % Time:   94s | Batch:   81 | Train loss: 0.3579\n",
      "   % Time:  106s | Batch:   91 | Train loss: 0.3902\n",
      "   % Time:  118s | Batch:  101 | Train loss: 0.3818\n",
      "   % Time:  129s | Batch:  111 | Train loss: 0.3700\n",
      "   % Time:  141s | Batch:  121 | Train loss: 0.3873\n",
      "   % Time:  152s | Batch:  131 | Train loss: 0.3751\n",
      "   % Time:  164s | Batch:  141 | Train loss: 0.4081\n",
      "   % Time:  175s | Batch:  151 | Train loss: 0.3904\n",
      "   % Time:  187s | Batch:  161 | Train loss: 0.3596\n",
      "   % Time:  198s | Batch:  171 | Train loss: 0.3681\n",
      "   % Time:  210s | Batch:  181 | Train loss: 0.3638\n",
      "   % Time:  221s | Batch:  191 | Train loss: 0.4096\n",
      "   % Time:  233s | Batch:  201 | Train loss: 0.3838\n",
      "   % Time:  245s | Batch:  211 | Train loss: 0.3881\n",
      "   % Time:  256s | Batch:  221 | Train loss: 0.3794\n",
      "   % Time:  268s | Batch:  231 | Train loss: 0.3535\n",
      "   % Time:  279s | Batch:  241 | Train loss: 0.3954\n",
      "   % Time:  291s | Batch:  251 | Train loss: 0.3923\n",
      "   % Time:  302s | Batch:  261 | Train loss: 0.3948\n",
      "   % Time:  314s | Batch:  271 | Train loss: 0.3862\n",
      "   % Time:  325s | Batch:  281 | Train loss: 0.3652\n",
      "   % Time:  337s | Batch:  291 | Train loss: 0.3674\n",
      "   % Time:  348s | Batch:  301 | Train loss: 0.3874\n",
      "   % Time:  359s | Batch:  311 | Train loss: 0.3926\n",
      "   % Time:  371s | Batch:  321 | Train loss: 0.3785\n",
      "   % Time:  382s | Batch:  331 | Train loss: 0.3888\n",
      "   % Time:  394s | Batch:  341 | Train loss: 0.3598\n",
      "   % Time:  405s | Batch:  351 | Train loss: 0.3721\n",
      "   % Time:  417s | Batch:  361 | Train loss: 0.3945\n",
      "   % Time:  428s | Batch:  371 | Train loss: 0.3672\n",
      "   % Time:  440s | Batch:  381 | Train loss: 0.3824\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   % Time:  451s | Batch:  391 | Train loss: 0.3619\n",
      "   % Time:  462s | Batch:  401 | Train loss: 0.3591\n",
      "   % Time:  473s | Batch:  411 | Train loss: 0.3942\n",
      "   % Time:  485s | Batch:  421 | Train loss: 0.3665\n",
      "   % Time:  496s | Batch:  431 | Train loss: 0.4043\n",
      "   % Time:  507s | Batch:  441 | Train loss: 0.3950\n",
      "   % Time:  518s | Batch:  451 | Train loss: 0.4138\n",
      "   % Time:  530s | Batch:  461 | Train loss: 0.3893\n",
      "   % Time:  541s | Batch:  471 | Train loss: 0.3685\n",
      "   % Time:  552s | Batch:  481 | Train loss: 0.3553\n",
      "   % Time:  563s | Batch:  491 | Train loss: 0.3767\n",
      "   % Time:  575s | Batch:  501 | Train loss: 0.3686\n",
      "   % Time:  586s | Batch:  511 | Train loss: 0.3877\n",
      "   % Time:  597s | Batch:  521 | Train loss: 0.3813\n",
      "   % Time:  608s | Batch:  531 | Train loss: 0.3773\n",
      "   % Time:  619s | Batch:  541 | Train loss: 0.3620\n",
      "   % Time:  630s | Batch:  551 | Train loss: 0.3522\n",
      "   % Time:  642s | Batch:  561 | Train loss: 0.3885\n",
      "==========\n",
      "   % Epoch: 3 | Time:  648s | Train loss: 54824.9193 | Val loss: 0.0000\n",
      "==========\n",
      "=> EPOCH 4 with lr [0.0001]\n",
      "   % Time:    1s | Batch:    1 | Train loss: 0.3545\n",
      "   % Time:   12s | Batch:   11 | Train loss: 0.3733\n",
      "   % Time:   23s | Batch:   21 | Train loss: 0.3866\n",
      "   % Time:   35s | Batch:   31 | Train loss: 0.3674\n",
      "   % Time:   46s | Batch:   41 | Train loss: 0.4016\n",
      "   % Time:   57s | Batch:   51 | Train loss: 0.3703\n",
      "   % Time:   68s | Batch:   61 | Train loss: 0.3776\n",
      "   % Time:   79s | Batch:   71 | Train loss: 0.3762\n",
      "   % Time:   90s | Batch:   81 | Train loss: 0.3795\n",
      "   % Time:  102s | Batch:   91 | Train loss: 0.3532\n",
      "   % Time:  113s | Batch:  101 | Train loss: 0.3798\n",
      "   % Time:  124s | Batch:  111 | Train loss: 0.3624\n",
      "   % Time:  136s | Batch:  121 | Train loss: 0.3970\n",
      "   % Time:  147s | Batch:  131 | Train loss: 0.3880\n",
      "   % Time:  159s | Batch:  141 | Train loss: 0.3952\n",
      "   % Time:  171s | Batch:  151 | Train loss: 0.3829\n",
      "   % Time:  183s | Batch:  161 | Train loss: 0.3696\n",
      "   % Time:  195s | Batch:  171 | Train loss: 0.3687\n",
      "   % Time:  206s | Batch:  181 | Train loss: 0.3699\n",
      "   % Time:  218s | Batch:  191 | Train loss: 0.3839\n",
      "   % Time:  230s | Batch:  201 | Train loss: 0.3674\n",
      "   % Time:  242s | Batch:  211 | Train loss: 0.3898\n",
      "   % Time:  253s | Batch:  221 | Train loss: 0.4029\n",
      "   % Time:  265s | Batch:  231 | Train loss: 0.3663\n",
      "   % Time:  277s | Batch:  241 | Train loss: 0.3847\n",
      "   % Time:  288s | Batch:  251 | Train loss: 0.3713\n",
      "   % Time:  300s | Batch:  261 | Train loss: 0.3785\n",
      "   % Time:  311s | Batch:  271 | Train loss: 0.3950\n",
      "   % Time:  323s | Batch:  281 | Train loss: 0.4105\n",
      "   % Time:  334s | Batch:  291 | Train loss: 0.3501\n",
      "   % Time:  346s | Batch:  301 | Train loss: 0.3668\n",
      "   % Time:  357s | Batch:  311 | Train loss: 0.3721\n",
      "   % Time:  369s | Batch:  321 | Train loss: 0.3756\n",
      "   % Time:  380s | Batch:  331 | Train loss: 0.3609\n",
      "   % Time:  392s | Batch:  341 | Train loss: 0.3899\n",
      "   % Time:  403s | Batch:  351 | Train loss: 0.3654\n",
      "   % Time:  415s | Batch:  361 | Train loss: 0.3839\n",
      "   % Time:  426s | Batch:  371 | Train loss: 0.3889\n",
      "   % Time:  437s | Batch:  381 | Train loss: 0.3842\n",
      "   % Time:  449s | Batch:  391 | Train loss: 0.3833\n",
      "   % Time:  460s | Batch:  401 | Train loss: 0.3713\n",
      "   % Time:  472s | Batch:  411 | Train loss: 0.3869\n",
      "   % Time:  484s | Batch:  421 | Train loss: 0.3697\n",
      "   % Time:  495s | Batch:  431 | Train loss: 0.3546\n",
      "   % Time:  507s | Batch:  441 | Train loss: 0.3443\n",
      "   % Time:  518s | Batch:  451 | Train loss: 0.3731\n",
      "   % Time:  530s | Batch:  461 | Train loss: 0.3803\n",
      "   % Time:  541s | Batch:  471 | Train loss: 0.3580\n",
      "   % Time:  552s | Batch:  481 | Train loss: 0.3705\n",
      "   % Time:  563s | Batch:  491 | Train loss: 0.3646\n",
      "   % Time:  574s | Batch:  501 | Train loss: 0.3738\n",
      "   % Time:  585s | Batch:  511 | Train loss: 0.3589\n",
      "   % Time:  596s | Batch:  521 | Train loss: 0.3764\n",
      "   % Time:  607s | Batch:  531 | Train loss: 0.3755\n",
      "   % Time:  618s | Batch:  541 | Train loss: 0.3772\n",
      "   % Time:  629s | Batch:  551 | Train loss: 0.3792\n",
      "   % Time:  640s | Batch:  561 | Train loss: 0.3749\n",
      "==========\n",
      "   % Epoch: 4 | Time:  647s | Train loss: 54441.5493 | Val loss: 0.0000\n",
      "==========\n",
      "=> EPOCH 5 with lr [1.0000000000000003e-05]\n",
      "   % Time:    1s | Batch:    1 | Train loss: 0.3687\n",
      "   % Time:   12s | Batch:   11 | Train loss: 0.4001\n",
      "   % Time:   23s | Batch:   21 | Train loss: 0.3997\n",
      "   % Time:   34s | Batch:   31 | Train loss: 0.4006\n",
      "   % Time:   45s | Batch:   41 | Train loss: 0.3556\n",
      "   % Time:   56s | Batch:   51 | Train loss: 0.3654\n",
      "   % Time:   67s | Batch:   61 | Train loss: 0.3772\n",
      "   % Time:   78s | Batch:   71 | Train loss: 0.3650\n",
      "   % Time:   89s | Batch:   81 | Train loss: 0.3978\n",
      "   % Time:  100s | Batch:   91 | Train loss: 0.3716\n",
      "   % Time:  111s | Batch:  101 | Train loss: 0.3631\n",
      "   % Time:  122s | Batch:  111 | Train loss: 0.3660\n",
      "   % Time:  133s | Batch:  121 | Train loss: 0.3712\n",
      "   % Time:  144s | Batch:  131 | Train loss: 0.3852\n",
      "   % Time:  155s | Batch:  141 | Train loss: 0.3705\n",
      "   % Time:  166s | Batch:  151 | Train loss: 0.3977\n",
      "   % Time:  177s | Batch:  161 | Train loss: 0.3821\n",
      "   % Time:  188s | Batch:  171 | Train loss: 0.3726\n",
      "   % Time:  199s | Batch:  181 | Train loss: 0.3699\n",
      "   % Time:  210s | Batch:  191 | Train loss: 0.3757\n",
      "   % Time:  221s | Batch:  201 | Train loss: 0.3734\n",
      "   % Time:  233s | Batch:  211 | Train loss: 0.3567\n",
      "   % Time:  244s | Batch:  221 | Train loss: 0.3683\n",
      "   % Time:  255s | Batch:  231 | Train loss: 0.3593\n",
      "   % Time:  266s | Batch:  241 | Train loss: 0.3779\n",
      "   % Time:  277s | Batch:  251 | Train loss: 0.3860\n",
      "   % Time:  288s | Batch:  261 | Train loss: 0.3654\n",
      "   % Time:  299s | Batch:  271 | Train loss: 0.3770\n",
      "   % Time:  310s | Batch:  281 | Train loss: 0.3566\n",
      "   % Time:  321s | Batch:  291 | Train loss: 0.3671\n",
      "   % Time:  332s | Batch:  301 | Train loss: 0.3764\n",
      "   % Time:  343s | Batch:  311 | Train loss: 0.3957\n",
      "   % Time:  354s | Batch:  321 | Train loss: 0.3871\n",
      "   % Time:  365s | Batch:  331 | Train loss: 0.3870\n",
      "   % Time:  376s | Batch:  341 | Train loss: 0.3549\n",
      "   % Time:  387s | Batch:  351 | Train loss: 0.3717\n",
      "   % Time:  398s | Batch:  361 | Train loss: 0.3968\n",
      "   % Time:  409s | Batch:  371 | Train loss: 0.3810\n",
      "   % Time:  420s | Batch:  381 | Train loss: 0.3879\n",
      "   % Time:  431s | Batch:  391 | Train loss: 0.3857\n",
      "   % Time:  442s | Batch:  401 | Train loss: 0.3952\n",
      "   % Time:  453s | Batch:  411 | Train loss: 0.3601\n",
      "   % Time:  464s | Batch:  421 | Train loss: 0.3762\n",
      "   % Time:  476s | Batch:  431 | Train loss: 0.4007\n",
      "   % Time:  487s | Batch:  441 | Train loss: 0.3784\n",
      "   % Time:  498s | Batch:  451 | Train loss: 0.3579\n",
      "   % Time:  509s | Batch:  461 | Train loss: 0.3907\n",
      "   % Time:  520s | Batch:  471 | Train loss: 0.3961\n",
      "   % Time:  531s | Batch:  481 | Train loss: 0.3798\n",
      "   % Time:  542s | Batch:  491 | Train loss: 0.3839\n",
      "   % Time:  553s | Batch:  501 | Train loss: 0.3835\n",
      "   % Time:  564s | Batch:  511 | Train loss: 0.3699\n",
      "   % Time:  575s | Batch:  521 | Train loss: 0.4038\n",
      "   % Time:  586s | Batch:  531 | Train loss: 0.3767\n",
      "   % Time:  597s | Batch:  541 | Train loss: 0.3683\n",
      "   % Time:  608s | Batch:  551 | Train loss: 0.3886\n",
      "   % Time:  619s | Batch:  561 | Train loss: 0.3782\n",
      "==========\n",
      "   % Epoch: 5 | Time:  626s | Train loss: 54324.9236 | Val loss: 0.0000\n",
      "==========\n",
      "=> EPOCH 6 with lr [1.0000000000000003e-05]\n",
      "   % Time:    1s | Batch:    1 | Train loss: 0.3694\n",
      "   % Time:   12s | Batch:   11 | Train loss: 0.3917\n",
      "   % Time:   23s | Batch:   21 | Train loss: 0.3703\n",
      "   % Time:   34s | Batch:   31 | Train loss: 0.3558\n",
      "   % Time:   45s | Batch:   41 | Train loss: 0.3834\n",
      "   % Time:   56s | Batch:   51 | Train loss: 0.3763\n",
      "   % Time:   67s | Batch:   61 | Train loss: 0.3593\n",
      "   % Time:   78s | Batch:   71 | Train loss: 0.3804\n",
      "   % Time:   89s | Batch:   81 | Train loss: 0.3679\n",
      "   % Time:  100s | Batch:   91 | Train loss: 0.3676\n",
      "   % Time:  111s | Batch:  101 | Train loss: 0.3761\n",
      "   % Time:  122s | Batch:  111 | Train loss: 0.3732\n",
      "   % Time:  133s | Batch:  121 | Train loss: 0.3785\n",
      "   % Time:  144s | Batch:  131 | Train loss: 0.3631\n",
      "   % Time:  155s | Batch:  141 | Train loss: 0.3605\n",
      "   % Time:  166s | Batch:  151 | Train loss: 0.3693\n",
      "   % Time:  177s | Batch:  161 | Train loss: 0.3638\n",
      "   % Time:  188s | Batch:  171 | Train loss: 0.3718\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   % Time:  199s | Batch:  181 | Train loss: 0.3591\n",
      "   % Time:  210s | Batch:  191 | Train loss: 0.3927\n",
      "   % Time:  221s | Batch:  201 | Train loss: 0.3748\n",
      "   % Time:  232s | Batch:  211 | Train loss: 0.3818\n",
      "   % Time:  243s | Batch:  221 | Train loss: 0.3438\n",
      "   % Time:  254s | Batch:  231 | Train loss: 0.3867\n",
      "   % Time:  265s | Batch:  241 | Train loss: 0.3784\n",
      "   % Time:  276s | Batch:  251 | Train loss: 0.3767\n",
      "   % Time:  287s | Batch:  261 | Train loss: 0.3743\n",
      "   % Time:  298s | Batch:  271 | Train loss: 0.3963\n",
      "   % Time:  309s | Batch:  281 | Train loss: 0.3700\n",
      "   % Time:  320s | Batch:  291 | Train loss: 0.3791\n",
      "   % Time:  331s | Batch:  301 | Train loss: 0.3592\n",
      "   % Time:  342s | Batch:  311 | Train loss: 0.3740\n",
      "   % Time:  353s | Batch:  321 | Train loss: 0.3298\n",
      "   % Time:  364s | Batch:  331 | Train loss: 0.3848\n",
      "   % Time:  375s | Batch:  341 | Train loss: 0.3650\n",
      "   % Time:  386s | Batch:  351 | Train loss: 0.3860\n",
      "   % Time:  397s | Batch:  361 | Train loss: 0.3497\n",
      "   % Time:  408s | Batch:  371 | Train loss: 0.3508\n",
      "   % Time:  419s | Batch:  381 | Train loss: 0.4043\n",
      "   % Time:  430s | Batch:  391 | Train loss: 0.3913\n",
      "   % Time:  441s | Batch:  401 | Train loss: 0.3529\n",
      "   % Time:  452s | Batch:  411 | Train loss: 0.3675\n",
      "   % Time:  463s | Batch:  421 | Train loss: 0.3711\n",
      "   % Time:  474s | Batch:  431 | Train loss: 0.3745\n",
      "   % Time:  485s | Batch:  441 | Train loss: 0.3517\n",
      "   % Time:  496s | Batch:  451 | Train loss: 0.3532\n",
      "   % Time:  507s | Batch:  461 | Train loss: 0.3656\n",
      "   % Time:  518s | Batch:  471 | Train loss: 0.3661\n",
      "   % Time:  529s | Batch:  481 | Train loss: 0.3612\n",
      "   % Time:  540s | Batch:  491 | Train loss: 0.4064\n",
      "   % Time:  551s | Batch:  501 | Train loss: 0.3588\n",
      "   % Time:  562s | Batch:  511 | Train loss: 0.4009\n",
      "   % Time:  573s | Batch:  521 | Train loss: 0.3778\n",
      "   % Time:  584s | Batch:  531 | Train loss: 0.3807\n",
      "   % Time:  595s | Batch:  541 | Train loss: 0.3625\n",
      "   % Time:  606s | Batch:  551 | Train loss: 0.3603\n",
      "   % Time:  617s | Batch:  561 | Train loss: 0.3749\n",
      "==========\n",
      "   % Epoch: 6 | Time:  624s | Train loss: 54196.4207 | Val loss: 0.0000\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "if args.train:\n",
    "    for epoch in range(1, 7):\n",
    "        scheduler.step()\n",
    "        print(\"=> EPOCH {} with lr {}\".format(epoch, scheduler.get_lr()))\n",
    "        val_loss = train(raw_data, scaled_data, scaler,\n",
    "                         model, criterion, optimizer)\n",
    "        save_model(model, epoch, val_loss)\n",
    "else:\n",
    "    model_file = os.path.join(args.intermediate_path, args.model_name)\n",
    "    model.load_state_dict(torch.load(model_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-11T06:19:32.982329Z",
     "start_time": "2017-09-11T06:10:58.130313Z"
    }
   },
   "outputs": [],
   "source": [
    "if args.forecast:\n",
    "    prediction = forecast(raw_data, scaled_data, scaler, model)\n",
    "#    print(\"SMAPE: {}\".format(smape(prediction, raw_data[:,\n",
    "#        args.offset:args.offset+args.val_len])))\n",
    "    with open(os.path.join(args.intermediate_path,\n",
    "                           \"pred_rnn.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(prediction, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pydata)",
   "language": "python",
   "name": "pydata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
