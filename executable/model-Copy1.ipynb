{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-03T07:56:06.202276Z",
     "start_time": "2017-09-03T07:56:05.727642Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import numpy as np; np.seterr(invalid='ignore')\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-03T07:56:06.215803Z",
     "start_time": "2017-09-03T07:56:06.203592Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parser = {\n",
    "    'data_path': '../data/wttsf/',\n",
    "    'train_file': 'train_1.csv',\n",
    "    'intermediate_path': '../intermediate/',\n",
    "    'n_epoch': 3,\n",
    "    'future': 73,\n",
    "    'batch_size': 64,\n",
    "    'hidden_size': 128,\n",
    "    'log_every': 10,\n",
    "    'read_from_file': False,\n",
    "    'train': True,\n",
    "    'model_name': '',\n",
    "    'cuda': True,\n",
    "    'seed': 20170903,\n",
    "}\n",
    "args = argparse.Namespace(**parser)\n",
    "\n",
    "args.cuda = args.cuda and torch.cuda.is_available()\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "args.intermediate_path = os.path.join(args.intermediate_path, str(args.seed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-03T07:56:06.303590Z",
     "start_time": "2017-09-03T07:56:06.216955Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DenseLSTMForecast(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(DenseLSTMForecast, self).__init__()\n",
    "        self.lstm1 = nn.LSTMCell(15, hidden_size, bias=False)\n",
    "        self.lstm2 = nn.LSTMCell(hidden_size+15, hidden_size, bias=False)\n",
    "        self.lstm3 = nn.LSTMCell(2*hidden_size+15, hidden_size, bias=False)\n",
    "        self.linear = nn.Linear(3*hidden_size+15, 1)\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    def forward(self, x, feature, future=1):\n",
    "        o = []\n",
    "        tt = torch.cuda if args.cuda else torch\n",
    "        h1_t = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())\n",
    "        c1_t = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())\n",
    "        h2_t = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())\n",
    "        c2_t = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())\n",
    "        h3_t = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())\n",
    "        c3_t = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())\n",
    "        \n",
    "        for x_t in x.chunk(x.size(1), dim=1):\n",
    "            x_t = x_t.squeeze(dim=1)\n",
    "            xd_t = torch.cat([x_t, feature], dim=1)\n",
    "            h1_t, c1_t = self.lstm1(xd_t, (h1_t, c1_t))\n",
    "            h1d_t = torch.cat([xd_t, h1_t], dim=1)\n",
    "            h2_t, c2_t = self.lstm2(h1d_t, (h2_t, c2_t))\n",
    "            h2d_t = torch.cat([xd_t, h1_t, h2_t], dim=1)\n",
    "            h3_t, c3_t = self.lstm3(h2d_t, (h3_t, c3_t))\n",
    "            h3d_t = torch.cat([xd_t, h1_t, h2_t, h3_t], dim=1)\n",
    "            o_t = self.linear(h3d_t)\n",
    "            o.append(o_t)\n",
    "            \n",
    "        for i in range(future-1):\n",
    "            od_t = torch.cat([o_t, feature], dim=1)\n",
    "            h1_t, c1_t = self.lstm1(od_t, (h1_t, c1_t))\n",
    "            h1d_t = torch.cat([od_t, h1_t], dim=1)\n",
    "            h2_t, c2_t = self.lstm2(h1d_t, (h2_t, c2_t))\n",
    "            h2d_t = torch.cat([od_t, h1_t, h2_t], dim=1)\n",
    "            h3_t, c3_t = self.lstm3(h2d_t, (h3_t, c3_t))\n",
    "            h3d_t = torch.cat([od_t, h1_t, h2_t, h3_t], dim=1)\n",
    "            o_t = self.linear(h3d_t)\n",
    "            o.append(o_t)\n",
    "\n",
    "        return torch.stack(o, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-03T07:56:06.371778Z",
     "start_time": "2017-09-03T07:56:06.304778Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def smape(y_pred, y_true):\n",
    "    y_pred = np.around(np.clip(np.exp(y_pred)-1, 0, None))\n",
    "    y_true = np.around(np.exp(y_true) - 1)\n",
    "    raw_smape = np.abs(y_true - y_pred) / (np.abs(y_true) + np.abs(y_pred))\n",
    "    kaggle_smape = np.nan_to_num(raw_smape)\n",
    "    return np.mean(kaggle_smape) * 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-03T07:56:06.426946Z",
     "start_time": "2017-09-03T07:56:06.373061Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data():\n",
    "#     raw_data_file = os.path.join(args.intermediate_path,\n",
    "#                                  'raw_data.pkl')\n",
    "    scaled_data_file = os.path.join(args.intermediate_path,\n",
    "                                    'scaled_data.pkl')\n",
    "    scaler_file = os.path.join(args.intermediate_path, 'scaler.pkl')\n",
    "    features_file = os.path.join(args.intermediate_path, 'features.pkl')\n",
    "    \n",
    "    if not args.read_from_file:\n",
    "        data_df = pd.read_csv(os.path.join(args.data_path, args.train_file),\n",
    "                              index_col='Page')\n",
    "        data_df[\"agent\"] = data_df.index.str.rsplit('_').str.get(-1)\n",
    "        data_df[\"access\"] = data_df.index.str.rsplit('_').str.get(-2)\n",
    "        data_df[\"project\"] = data_df.index.str.rsplit('_').str.get(-3)\n",
    "        features = pd.get_dummies(data_df[[\"agent\", \"access\", \"project\"]],\n",
    "            columns=[\"agent\", \"access\", \"project\"]).values.astype('float32')\n",
    "        raw_data = np.nan_to_num(\n",
    "            data_df.iloc[:,:-3].values.astype('float32'))\n",
    "        data = np.log1p(raw_data)\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(np.swapaxes(data[:, :-args.future], 0, 1))\n",
    "        scaled_data = scaler.transform(np.swapaxes(data, 0, 1))\n",
    "        scaled_data = np.swapaxes(scaled_data, 0, 1)\n",
    "        \n",
    "#         with open(raw_data_file, 'wb') as f:\n",
    "#             pickle.dump(raw_data, f)\n",
    "        with open(scaled_data_file, 'wb') as f:\n",
    "            pickle.dump(scaled_data, f)\n",
    "        with open(scaler_file, 'wb') as f:\n",
    "            pickle.dump(scaler, f)\n",
    "        with open(features_file, 'wb') as f:\n",
    "            pickle.dump(features, f)\n",
    "    else:\n",
    "#         with open(raw_data_file, 'rb') as f:\n",
    "#             raw_data = pickle.load(f)\n",
    "        with open(scaled_data_file, 'rb') as f:\n",
    "            scaled_data = pickle.load(f)\n",
    "        with open(scaler_file, 'rb') as f:\n",
    "            scaler = pickle.load(f)\n",
    "        with open(features_file, 'rb') as f:\n",
    "            features = pickle.load(f)\n",
    "    return scaled_data, scaler, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-03T07:56:06.547384Z",
     "start_time": "2017-09-03T07:56:06.428093Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(scaled_data, scaler, features, model, criterion, optimizer):\n",
    "    p = np.random.permutation(scaled_data.shape[0])\n",
    "    inverse_p = np.argsort(p)\n",
    "\n",
    "    input_tensor = torch.from_numpy(scaled_data[p, :-1]).unsqueeze(2)\n",
    "    target_tensor = torch.from_numpy(scaled_data[p, 1:]).unsqueeze(2)\n",
    "    features_tensor = torch.from_numpy(features[p, :])\n",
    "    dataset = TensorDataset(input_tensor, target_tensor)\n",
    "    data_loader = DataLoader(dataset, args.batch_size)\n",
    "    \n",
    "    train_loss = 0\n",
    "    val_output_list = []\n",
    "    init_time = time.time()\n",
    "    for i, (inputt, target) in enumerate(data_loader):\n",
    "        feature = features_tensor[i*args.batch_size:(i*args.batch_size\n",
    "                                                     +inputt.size(0))]\n",
    "        if args.cuda:\n",
    "            inputt = inputt.cuda()\n",
    "            target = target.cuda()\n",
    "            feature = feature.cuda()\n",
    "        inputt = Variable(inputt)\n",
    "        target = Variable(target)\n",
    "        feature = Variable(feature)\n",
    "        \n",
    "        output = model(inputt, feature)\n",
    "        pos = np.random.randint(args.future,\n",
    "                                output.size(1)-2*args.future+1)\n",
    "#        pos_val = np.random.randint(args.future,\n",
    "#                                    output.size(1)-args.future+1)\n",
    "        loss = criterion(output[:, pos:pos+args.future],\n",
    "                         target[:, pos:pos+args.future])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.data[0] * inputt.size(0)\n",
    "        \n",
    "        val_output_list.append(output[:, -args.future:]\n",
    "                               .data.squeeze(2).cpu().numpy())\n",
    "\n",
    "        if i % args.log_every == 0:\n",
    "\n",
    "            print(\"   % Time: {:4.0f}s | Batch: {:4} | \"\n",
    "                  \"Train loss: {:.4f}\".format(\n",
    "                      time.time()-init_time, i+1, loss.data[0]))\n",
    "        \n",
    "    val_output_all = np.concatenate(val_output_list, axis=0)[inverse_p]\n",
    "    prediction = np.swapaxes(scaler.inverse_transform(\n",
    "            np.swapaxes(val_output_all, 0, 1)), 0, 1)\n",
    "    \n",
    "    var_target = np.swapaxes(scaler.inverse_transform(\n",
    "            np.swapaxes(scaled_data[:, -args.future:], 0, 1)), 0, 1)\n",
    "    train_loss /= scaled_data.shape[0]\n",
    "    val_loss = smape(prediction, var_target)\n",
    "    \n",
    "    print(\"=\"*10)\n",
    "    print(\"   % Epoch: {} | Time: {:4.0f}s | \"\n",
    "          \"Train loss: {:.4f} | Val loss: {:.4f}\"\n",
    "          .format(epoch, time.time()-init_time, train_loss , val_loss))\n",
    "    print(\"=\"*10)\n",
    "    return val_loss\n",
    "\n",
    "#             val_output = output[\n",
    "#                 :, pos_val:pos_val+args.future].data.squeeze(2).cpu().numpy()\n",
    "#             val_target = target[\n",
    "#                 :, pos_val:pos_val+args.future].data.squeeze(2).cpu().numpy()\n",
    "#             val_scale = scaler.scale_[p][\n",
    "#                 i*args.batch_size:i*args.batch_size+inputt.size(0)]\n",
    "#             val_mean = scaler.mean_[p][\n",
    "#                 i*args.batch_size:i*args.batch_size+inputt.size(0)]\n",
    "#             raw_val_output = (val_output.T * val_scale + val_mean).T\n",
    "#             raw_val_target = (val_target.T * val_scale + val_mean).T\n",
    "#             val_loss = smape(raw_val_output, raw_val_target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-03T07:56:06.615621Z",
     "start_time": "2017-09-03T07:56:06.548440Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forecast(scaled_data, scaler, features, model):\n",
    "    input_tensor = torch.from_numpy(scaled_data).unsqueeze(2)\n",
    "    target_tensor = torch.zeros(input_tensor.size(0))\n",
    "    features_tensor = torch.from_numpy(features)\n",
    "    dataset = torch.utils.data.TensorDataset(input_tensor, target_tensor)\n",
    "    data_loader = DataLoader(dataset, args.batch_size)\n",
    "    \n",
    "    output_list = []\n",
    "    for i, (inputt, _) in enumerate(data_loader):\n",
    "        feature = features_tensor[i*args.batch_size:(i*args.batch_size\n",
    "                                                     +inputt.size(0))]\n",
    "        if args.cuda:\n",
    "            inputt = inputt.cuda()\n",
    "            feature = feature.cuda()\n",
    "        inputt = Variable(inputt)\n",
    "        feature = Variable(feature)\n",
    "        output = model(inputt, feature, args.future)\n",
    "        output_list.append(output.data.squeeze(2).cpu().numpy()\n",
    "                           [:, -args.future:])\n",
    "        \n",
    "    output_all = np.concatenate(output_list, axis=0)\n",
    "    prediction = np.swapaxes(scaler.inverse_transform(\n",
    "            np.swapaxes(output_all, 0, 1)), 0, 1)\n",
    "    prediction = np.around(np.clip(np.exp(prediction) - 1, 0, None))\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-03T07:56:06.682498Z",
     "start_time": "2017-09-03T07:56:06.617268Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_model(model, epoch, loss):\n",
    "    model_file = os.path.join(args.intermediate_path,\n",
    "                              \"model_seed{}_epoch{}_loss_{:.4f}.pth\"\n",
    "                              .format(args.seed, epoch, loss))\n",
    "    torch.save(model.state_dict(), os.path.join(model_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-03T07:56:17.323267Z",
     "start_time": "2017-09-03T07:56:06.684069Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scaled_data, scaler, features = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-03T07:56:18.512906Z",
     "start_time": "2017-09-03T07:56:17.324740Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = DenseLSTMForecast(args.hidden_size)\n",
    "if args.cuda:\n",
    "    model.cuda()\n",
    "criterion = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-03T07:56:19.032258Z",
     "start_time": "2017-09-03T07:56:18.514221Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "scheduler = MultiStepLR(optimizer, milestones=[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-03T09:20:50.201123Z",
     "start_time": "2017-09-03T07:56:19.033413Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> EPOCH 1\n",
      "   % Time:    1s | Batch:    1 | Train loss: 0.7365\n",
      "   % Time:   10s | Batch:   11 | Train loss: 0.4983\n",
      "   % Time:   19s | Batch:   21 | Train loss: 0.4348\n",
      "   % Time:   29s | Batch:   31 | Train loss: 0.4087\n",
      "   % Time:   39s | Batch:   41 | Train loss: 0.4330\n",
      "   % Time:   48s | Batch:   51 | Train loss: 0.4578\n",
      "   % Time:   57s | Batch:   61 | Train loss: 0.4657\n",
      "   % Time:   67s | Batch:   71 | Train loss: 0.4188\n",
      "   % Time:   77s | Batch:   81 | Train loss: 0.4300\n",
      "   % Time:   86s | Batch:   91 | Train loss: 0.4873\n",
      "   % Time:   95s | Batch:  101 | Train loss: 0.4488\n",
      "   % Time:  105s | Batch:  111 | Train loss: 0.4561\n",
      "   % Time:  114s | Batch:  121 | Train loss: 0.4403\n",
      "   % Time:  125s | Batch:  131 | Train loss: 0.4372\n",
      "   % Time:  134s | Batch:  141 | Train loss: 0.4042\n",
      "   % Time:  143s | Batch:  151 | Train loss: 0.4476\n",
      "   % Time:  153s | Batch:  161 | Train loss: 0.4459\n",
      "   % Time:  162s | Batch:  171 | Train loss: 0.4061\n",
      "   % Time:  172s | Batch:  181 | Train loss: 0.3853\n",
      "   % Time:  181s | Batch:  191 | Train loss: 0.4161\n",
      "   % Time:  191s | Batch:  201 | Train loss: 0.4227\n",
      "   % Time:  200s | Batch:  211 | Train loss: 0.4065\n",
      "   % Time:  210s | Batch:  221 | Train loss: 0.4208\n",
      "   % Time:  219s | Batch:  231 | Train loss: 0.4468\n",
      "   % Time:  228s | Batch:  241 | Train loss: 0.3983\n",
      "   % Time:  238s | Batch:  251 | Train loss: 0.4005\n",
      "   % Time:  247s | Batch:  261 | Train loss: 0.4320\n",
      "   % Time:  256s | Batch:  271 | Train loss: 0.4302\n",
      "   % Time:  266s | Batch:  281 | Train loss: 0.4189\n",
      "   % Time:  275s | Batch:  291 | Train loss: 0.3918\n",
      "   % Time:  284s | Batch:  301 | Train loss: 0.4262\n",
      "   % Time:  294s | Batch:  311 | Train loss: 0.3925\n",
      "   % Time:  303s | Batch:  321 | Train loss: 0.4018\n",
      "   % Time:  313s | Batch:  331 | Train loss: 0.3819\n",
      "   % Time:  322s | Batch:  341 | Train loss: 0.3994\n",
      "   % Time:  331s | Batch:  351 | Train loss: 0.3787\n",
      "   % Time:  341s | Batch:  361 | Train loss: 0.3990\n",
      "   % Time:  351s | Batch:  371 | Train loss: 0.4126\n",
      "   % Time:  360s | Batch:  381 | Train loss: 0.3672\n",
      "   % Time:  369s | Batch:  391 | Train loss: 0.4304\n",
      "   % Time:  378s | Batch:  401 | Train loss: 0.4334\n",
      "   % Time:  388s | Batch:  411 | Train loss: 0.3927\n",
      "   % Time:  397s | Batch:  421 | Train loss: 0.4017\n",
      "   % Time:  406s | Batch:  431 | Train loss: 0.4226\n",
      "   % Time:  416s | Batch:  441 | Train loss: 0.3918\n",
      "   % Time:  425s | Batch:  451 | Train loss: 0.4222\n",
      "   % Time:  435s | Batch:  461 | Train loss: 0.3869\n",
      "   % Time:  444s | Batch:  471 | Train loss: 0.4078\n",
      "   % Time:  453s | Batch:  481 | Train loss: 0.3397\n",
      "   % Time:  462s | Batch:  491 | Train loss: 0.4248\n",
      "   % Time:  472s | Batch:  501 | Train loss: 0.4342\n",
      "   % Time:  481s | Batch:  511 | Train loss: 0.4108\n",
      "   % Time:  491s | Batch:  521 | Train loss: 0.4057\n",
      "   % Time:  500s | Batch:  531 | Train loss: 0.3936\n",
      "   % Time:  510s | Batch:  541 | Train loss: 0.4112\n",
      "   % Time:  519s | Batch:  551 | Train loss: 0.4212\n",
      "   % Time:  529s | Batch:  561 | Train loss: 0.4641\n",
      "   % Time:  538s | Batch:  571 | Train loss: 0.4070\n",
      "   % Time:  548s | Batch:  581 | Train loss: 0.3712\n",
      "   % Time:  557s | Batch:  591 | Train loss: 0.4011\n",
      "   % Time:  567s | Batch:  601 | Train loss: 0.4200\n",
      "   % Time:  576s | Batch:  611 | Train loss: 0.4341\n",
      "   % Time:  585s | Batch:  621 | Train loss: 0.3660\n",
      "   % Time:  595s | Batch:  631 | Train loss: 0.4560\n",
      "   % Time:  604s | Batch:  641 | Train loss: 0.4044\n",
      "   % Time:  613s | Batch:  651 | Train loss: 0.3843\n",
      "   % Time:  623s | Batch:  661 | Train loss: 0.4123\n",
      "   % Time:  632s | Batch:  671 | Train loss: 0.4308\n",
      "   % Time:  642s | Batch:  681 | Train loss: 0.3883\n",
      "   % Time:  651s | Batch:  691 | Train loss: 0.3896\n",
      "   % Time:  661s | Batch:  701 | Train loss: 0.3652\n",
      "   % Time:  670s | Batch:  711 | Train loss: 0.4313\n",
      "   % Time:  680s | Batch:  721 | Train loss: 0.4545\n",
      "   % Time:  689s | Batch:  731 | Train loss: 0.3905\n",
      "   % Time:  699s | Batch:  741 | Train loss: 0.4063\n",
      "   % Time:  708s | Batch:  751 | Train loss: 0.4494\n",
      "   % Time:  718s | Batch:  761 | Train loss: 0.3787\n",
      "   % Time:  727s | Batch:  771 | Train loss: 0.4131\n",
      "   % Time:  736s | Batch:  781 | Train loss: 0.3769\n",
      "   % Time:  746s | Batch:  791 | Train loss: 0.3998\n",
      "   % Time:  755s | Batch:  801 | Train loss: 0.3965\n",
      "   % Time:  764s | Batch:  811 | Train loss: 0.4349\n",
      "   % Time:  773s | Batch:  821 | Train loss: 0.3418\n",
      "   % Time:  782s | Batch:  831 | Train loss: 0.4063\n",
      "   % Time:  792s | Batch:  841 | Train loss: 0.3689\n",
      "   % Time:  802s | Batch:  851 | Train loss: 0.3498\n",
      "   % Time:  811s | Batch:  861 | Train loss: 0.3911\n",
      "   % Time:  821s | Batch:  871 | Train loss: 0.4135\n",
      "   % Time:  830s | Batch:  881 | Train loss: 0.4162\n",
      "   % Time:  840s | Batch:  891 | Train loss: 0.3204\n",
      "   % Time:  849s | Batch:  901 | Train loss: 0.3670\n",
      "   % Time:  859s | Batch:  911 | Train loss: 0.3776\n",
      "   % Time:  868s | Batch:  921 | Train loss: 0.4240\n",
      "   % Time:  877s | Batch:  931 | Train loss: 0.3725\n",
      "   % Time:  887s | Batch:  941 | Train loss: 0.3617\n",
      "   % Time:  895s | Batch:  951 | Train loss: 0.3991\n",
      "   % Time:  903s | Batch:  961 | Train loss: 0.3584\n",
      "   % Time:  910s | Batch:  971 | Train loss: 0.4203\n",
      "   % Time:  918s | Batch:  981 | Train loss: 0.4178\n",
      "   % Time:  926s | Batch:  991 | Train loss: 0.3685\n",
      "   % Time:  933s | Batch: 1001 | Train loss: 0.4173\n",
      "   % Time:  941s | Batch: 1011 | Train loss: 0.4097\n",
      "   % Time:  948s | Batch: 1021 | Train loss: 0.4021\n",
      "   % Time:  956s | Batch: 1031 | Train loss: 0.4283\n",
      "   % Time:  963s | Batch: 1041 | Train loss: 0.3818\n",
      "   % Time:  971s | Batch: 1051 | Train loss: 0.4028\n",
      "   % Time:  979s | Batch: 1061 | Train loss: 0.4251\n",
      "   % Time:  986s | Batch: 1071 | Train loss: 0.3872\n",
      "   % Time:  994s | Batch: 1081 | Train loss: 0.4520\n",
      "   % Time: 1001s | Batch: 1091 | Train loss: 0.3884\n",
      "   % Time: 1009s | Batch: 1101 | Train loss: 0.4403\n",
      "   % Time: 1016s | Batch: 1111 | Train loss: 0.4086\n",
      "   % Time: 1024s | Batch: 1121 | Train loss: 0.3924\n",
      "   % Time: 1031s | Batch: 1131 | Train loss: 0.3815\n",
      "   % Time: 1039s | Batch: 1141 | Train loss: 0.4048\n",
      "   % Time: 1046s | Batch: 1151 | Train loss: 0.3540\n",
      "   % Time: 1054s | Batch: 1161 | Train loss: 0.3759\n",
      "   % Time: 1061s | Batch: 1171 | Train loss: 0.3971\n",
      "   % Time: 1069s | Batch: 1181 | Train loss: 0.3659\n",
      "   % Time: 1076s | Batch: 1191 | Train loss: 0.3983\n",
      "   % Time: 1084s | Batch: 1201 | Train loss: 0.4035\n",
      "   % Time: 1091s | Batch: 1211 | Train loss: 0.3974\n",
      "   % Time: 1099s | Batch: 1221 | Train loss: 0.4088\n",
      "   % Time: 1106s | Batch: 1231 | Train loss: 0.4506\n",
      "   % Time: 1114s | Batch: 1241 | Train loss: 0.4445\n",
      "   % Time: 1121s | Batch: 1251 | Train loss: 0.4232\n",
      "   % Time: 1129s | Batch: 1261 | Train loss: 0.4153\n",
      "   % Time: 1136s | Batch: 1271 | Train loss: 0.4300\n",
      "   % Time: 1144s | Batch: 1281 | Train loss: 0.4308\n",
      "   % Time: 1151s | Batch: 1291 | Train loss: 0.3745\n",
      "   % Time: 1158s | Batch: 1301 | Train loss: 0.4154\n",
      "   % Time: 1166s | Batch: 1311 | Train loss: 0.4166\n",
      "   % Time: 1173s | Batch: 1321 | Train loss: 0.3629\n",
      "   % Time: 1181s | Batch: 1331 | Train loss: 0.3758\n",
      "   % Time: 1188s | Batch: 1341 | Train loss: 0.3999\n",
      "   % Time: 1196s | Batch: 1351 | Train loss: 0.4368\n",
      "   % Time: 1203s | Batch: 1361 | Train loss: 0.4373\n",
      "   % Time: 1211s | Batch: 1371 | Train loss: 0.3870\n",
      "   % Time: 1218s | Batch: 1381 | Train loss: 0.3545\n",
      "   % Time: 1226s | Batch: 1391 | Train loss: 0.3804\n",
      "   % Time: 1233s | Batch: 1401 | Train loss: 0.3835\n",
      "   % Time: 1240s | Batch: 1411 | Train loss: 0.3652\n",
      "   % Time: 1248s | Batch: 1421 | Train loss: 0.3862\n",
      "   % Time: 1255s | Batch: 1431 | Train loss: 0.3841\n",
      "   % Time: 1263s | Batch: 1441 | Train loss: 0.3390\n",
      "   % Time: 1270s | Batch: 1451 | Train loss: 0.4506\n",
      "   % Time: 1278s | Batch: 1461 | Train loss: 0.4355\n",
      "   % Time: 1285s | Batch: 1471 | Train loss: 0.4060\n",
      "   % Time: 1293s | Batch: 1481 | Train loss: 0.3733\n",
      "   % Time: 1300s | Batch: 1491 | Train loss: 0.3948\n",
      "   % Time: 1308s | Batch: 1501 | Train loss: 0.3614\n",
      "   % Time: 1315s | Batch: 1511 | Train loss: 0.3748\n",
      "   % Time: 1323s | Batch: 1521 | Train loss: 0.3463\n",
      "   % Time: 1330s | Batch: 1531 | Train loss: 0.4204\n",
      "   % Time: 1338s | Batch: 1541 | Train loss: 0.4208\n",
      "   % Time: 1345s | Batch: 1551 | Train loss: 0.4029\n",
      "   % Time: 1352s | Batch: 1561 | Train loss: 0.4145\n",
      "   % Time: 1360s | Batch: 1571 | Train loss: 0.3714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   % Time: 1367s | Batch: 1581 | Train loss: 0.3800\n",
      "   % Time: 1375s | Batch: 1591 | Train loss: 0.3610\n",
      "   % Time: 1383s | Batch: 1601 | Train loss: 0.3804\n",
      "   % Time: 1390s | Batch: 1611 | Train loss: 0.3785\n",
      "   % Time: 1397s | Batch: 1621 | Train loss: 0.4012\n",
      "   % Time: 1405s | Batch: 1631 | Train loss: 0.3787\n",
      "   % Time: 1412s | Batch: 1641 | Train loss: 0.3699\n",
      "   % Time: 1420s | Batch: 1651 | Train loss: 0.4594\n",
      "   % Time: 1427s | Batch: 1661 | Train loss: 0.3835\n",
      "   % Time: 1435s | Batch: 1671 | Train loss: 0.4014\n",
      "   % Time: 1442s | Batch: 1681 | Train loss: 0.4029\n",
      "   % Time: 1450s | Batch: 1691 | Train loss: 0.4172\n",
      "   % Time: 1457s | Batch: 1701 | Train loss: 0.3888\n",
      "   % Time: 1465s | Batch: 1711 | Train loss: 0.4061\n",
      "   % Time: 1472s | Batch: 1721 | Train loss: 0.4182\n",
      "   % Time: 1480s | Batch: 1731 | Train loss: 0.3828\n",
      "   % Time: 1487s | Batch: 1741 | Train loss: 0.3538\n",
      "   % Time: 1495s | Batch: 1751 | Train loss: 0.3734\n",
      "   % Time: 1502s | Batch: 1761 | Train loss: 0.4713\n",
      "   % Time: 1510s | Batch: 1771 | Train loss: 0.4096\n",
      "   % Time: 1517s | Batch: 1781 | Train loss: 0.4136\n",
      "   % Time: 1525s | Batch: 1791 | Train loss: 0.3758\n",
      "   % Time: 1532s | Batch: 1801 | Train loss: 0.4494\n",
      "   % Time: 1540s | Batch: 1811 | Train loss: 0.3841\n",
      "   % Time: 1547s | Batch: 1821 | Train loss: 0.3566\n",
      "   % Time: 1555s | Batch: 1831 | Train loss: 0.4096\n",
      "   % Time: 1562s | Batch: 1841 | Train loss: 0.3826\n",
      "   % Time: 1570s | Batch: 1851 | Train loss: 0.3794\n",
      "   % Time: 1577s | Batch: 1861 | Train loss: 0.3519\n",
      "   % Time: 1585s | Batch: 1871 | Train loss: 0.3833\n",
      "   % Time: 1592s | Batch: 1881 | Train loss: 0.3983\n",
      "   % Time: 1599s | Batch: 1891 | Train loss: 0.3777\n",
      "   % Time: 1607s | Batch: 1901 | Train loss: 0.3588\n",
      "   % Time: 1614s | Batch: 1911 | Train loss: 0.3392\n",
      "   % Time: 1622s | Batch: 1921 | Train loss: 0.4021\n",
      "   % Time: 1629s | Batch: 1931 | Train loss: 0.3752\n",
      "   % Time: 1637s | Batch: 1941 | Train loss: 0.4054\n",
      "   % Time: 1644s | Batch: 1951 | Train loss: 0.4177\n",
      "   % Time: 1652s | Batch: 1961 | Train loss: 0.3488\n",
      "   % Time: 1659s | Batch: 1971 | Train loss: 0.4345\n",
      "   % Time: 1667s | Batch: 1981 | Train loss: 0.3913\n",
      "   % Time: 1674s | Batch: 1991 | Train loss: 0.4086\n",
      "   % Time: 1682s | Batch: 2001 | Train loss: 0.3379\n",
      "   % Time: 1689s | Batch: 2011 | Train loss: 0.3875\n",
      "   % Time: 1697s | Batch: 2021 | Train loss: 0.3934\n",
      "   % Time: 1704s | Batch: 2031 | Train loss: 0.3837\n",
      "   % Time: 1711s | Batch: 2041 | Train loss: 0.4258\n",
      "   % Time: 1719s | Batch: 2051 | Train loss: 0.4056\n",
      "   % Time: 1726s | Batch: 2061 | Train loss: 0.3553\n",
      "   % Time: 1733s | Batch: 2071 | Train loss: 0.4190\n",
      "   % Time: 1741s | Batch: 2081 | Train loss: 0.3965\n",
      "   % Time: 1748s | Batch: 2091 | Train loss: 0.3675\n",
      "   % Time: 1756s | Batch: 2101 | Train loss: 0.4399\n",
      "   % Time: 1763s | Batch: 2111 | Train loss: 0.3793\n",
      "   % Time: 1770s | Batch: 2121 | Train loss: 0.4052\n",
      "   % Time: 1778s | Batch: 2131 | Train loss: 0.3676\n",
      "   % Time: 1785s | Batch: 2141 | Train loss: 0.3844\n",
      "   % Time: 1793s | Batch: 2151 | Train loss: 0.3846\n",
      "   % Time: 1800s | Batch: 2161 | Train loss: 0.3860\n",
      "   % Time: 1807s | Batch: 2171 | Train loss: 0.4261\n",
      "   % Time: 1815s | Batch: 2181 | Train loss: 0.4125\n",
      "   % Time: 1822s | Batch: 2191 | Train loss: 0.3886\n",
      "   % Time: 1830s | Batch: 2201 | Train loss: 0.3784\n",
      "   % Time: 1837s | Batch: 2211 | Train loss: 0.4018\n",
      "   % Time: 1844s | Batch: 2221 | Train loss: 0.3806\n",
      "   % Time: 1852s | Batch: 2231 | Train loss: 0.4024\n",
      "   % Time: 1859s | Batch: 2241 | Train loss: 0.3579\n",
      "   % Time: 1867s | Batch: 2251 | Train loss: 0.4119\n",
      "   % Time: 1874s | Batch: 2261 | Train loss: 0.3551\n",
      "==========\n",
      "   % Epoch: 1 | Time: 1879s | Train loss: 0.4016 | Val loss: 30.3821\n",
      "==========\n",
      "=> EPOCH 2\n",
      "   % Time:    1s | Batch:    1 | Train loss: 0.2946\n",
      "   % Time:    8s | Batch:   11 | Train loss: 0.4019\n",
      "   % Time:   15s | Batch:   21 | Train loss: 0.3739\n",
      "   % Time:   23s | Batch:   31 | Train loss: 0.3578\n",
      "   % Time:   30s | Batch:   41 | Train loss: 0.4254\n",
      "   % Time:   37s | Batch:   51 | Train loss: 0.4273\n",
      "   % Time:   45s | Batch:   61 | Train loss: 0.4015\n",
      "   % Time:   52s | Batch:   71 | Train loss: 0.4162\n",
      "   % Time:   59s | Batch:   81 | Train loss: 0.3852\n",
      "   % Time:   67s | Batch:   91 | Train loss: 0.3681\n",
      "   % Time:   74s | Batch:  101 | Train loss: 0.3562\n",
      "   % Time:   81s | Batch:  111 | Train loss: 0.4612\n",
      "   % Time:   89s | Batch:  121 | Train loss: 0.4050\n",
      "   % Time:   96s | Batch:  131 | Train loss: 0.3580\n",
      "   % Time:  103s | Batch:  141 | Train loss: 0.4003\n",
      "   % Time:  110s | Batch:  151 | Train loss: 0.4130\n",
      "   % Time:  118s | Batch:  161 | Train loss: 0.4014\n",
      "   % Time:  125s | Batch:  171 | Train loss: 0.3831\n",
      "   % Time:  132s | Batch:  181 | Train loss: 0.3729\n",
      "   % Time:  140s | Batch:  191 | Train loss: 0.3865\n",
      "   % Time:  147s | Batch:  201 | Train loss: 0.4516\n",
      "   % Time:  154s | Batch:  211 | Train loss: 0.3631\n",
      "   % Time:  161s | Batch:  221 | Train loss: 0.4076\n",
      "   % Time:  169s | Batch:  231 | Train loss: 0.3448\n",
      "   % Time:  176s | Batch:  241 | Train loss: 0.4028\n",
      "   % Time:  183s | Batch:  251 | Train loss: 0.3638\n",
      "   % Time:  191s | Batch:  261 | Train loss: 0.4267\n",
      "   % Time:  198s | Batch:  271 | Train loss: 0.3817\n",
      "   % Time:  205s | Batch:  281 | Train loss: 0.3845\n",
      "   % Time:  213s | Batch:  291 | Train loss: 0.3681\n",
      "   % Time:  220s | Batch:  301 | Train loss: 0.3913\n",
      "   % Time:  227s | Batch:  311 | Train loss: 0.3745\n",
      "   % Time:  235s | Batch:  321 | Train loss: 0.3588\n",
      "   % Time:  242s | Batch:  331 | Train loss: 0.3846\n",
      "   % Time:  249s | Batch:  341 | Train loss: 0.3878\n",
      "   % Time:  257s | Batch:  351 | Train loss: 0.3491\n",
      "   % Time:  264s | Batch:  361 | Train loss: 0.4155\n",
      "   % Time:  271s | Batch:  371 | Train loss: 0.4152\n",
      "   % Time:  278s | Batch:  381 | Train loss: 0.3645\n",
      "   % Time:  286s | Batch:  391 | Train loss: 0.3857\n",
      "   % Time:  293s | Batch:  401 | Train loss: 0.3833\n",
      "   % Time:  300s | Batch:  411 | Train loss: 0.3839\n",
      "   % Time:  308s | Batch:  421 | Train loss: 0.3567\n",
      "   % Time:  315s | Batch:  431 | Train loss: 0.3885\n",
      "   % Time:  322s | Batch:  441 | Train loss: 0.3664\n",
      "   % Time:  330s | Batch:  451 | Train loss: 0.3721\n",
      "   % Time:  337s | Batch:  461 | Train loss: 0.3607\n",
      "   % Time:  344s | Batch:  471 | Train loss: 0.3582\n",
      "   % Time:  352s | Batch:  481 | Train loss: 0.3290\n",
      "   % Time:  359s | Batch:  491 | Train loss: 0.3789\n",
      "   % Time:  366s | Batch:  501 | Train loss: 0.3771\n",
      "   % Time:  373s | Batch:  511 | Train loss: 0.3672\n",
      "   % Time:  381s | Batch:  521 | Train loss: 0.4006\n",
      "   % Time:  388s | Batch:  531 | Train loss: 0.3518\n",
      "   % Time:  395s | Batch:  541 | Train loss: 0.3931\n",
      "   % Time:  403s | Batch:  551 | Train loss: 0.3123\n",
      "   % Time:  410s | Batch:  561 | Train loss: 0.3872\n",
      "   % Time:  417s | Batch:  571 | Train loss: 0.3351\n",
      "   % Time:  424s | Batch:  581 | Train loss: 0.4270\n",
      "   % Time:  432s | Batch:  591 | Train loss: 0.3670\n",
      "   % Time:  439s | Batch:  601 | Train loss: 0.3793\n",
      "   % Time:  446s | Batch:  611 | Train loss: 0.3600\n",
      "   % Time:  454s | Batch:  621 | Train loss: 0.3340\n",
      "   % Time:  461s | Batch:  631 | Train loss: 0.3643\n",
      "   % Time:  468s | Batch:  641 | Train loss: 0.4294\n",
      "   % Time:  475s | Batch:  651 | Train loss: 0.3775\n",
      "   % Time:  483s | Batch:  661 | Train loss: 0.4018\n",
      "   % Time:  490s | Batch:  671 | Train loss: 0.3829\n",
      "   % Time:  497s | Batch:  681 | Train loss: 0.4418\n",
      "   % Time:  504s | Batch:  691 | Train loss: 0.3634\n",
      "   % Time:  512s | Batch:  701 | Train loss: 0.3635\n",
      "   % Time:  519s | Batch:  711 | Train loss: 0.4169\n",
      "   % Time:  526s | Batch:  721 | Train loss: 0.4122\n",
      "   % Time:  534s | Batch:  731 | Train loss: 0.3975\n",
      "   % Time:  541s | Batch:  741 | Train loss: 0.3530\n",
      "   % Time:  548s | Batch:  751 | Train loss: 0.3749\n",
      "   % Time:  555s | Batch:  761 | Train loss: 0.3688\n",
      "   % Time:  563s | Batch:  771 | Train loss: 0.4342\n",
      "   % Time:  570s | Batch:  781 | Train loss: 0.4347\n",
      "   % Time:  577s | Batch:  791 | Train loss: 0.4013\n",
      "   % Time:  585s | Batch:  801 | Train loss: 0.4090\n",
      "   % Time:  593s | Batch:  811 | Train loss: 0.3818\n",
      "   % Time:  602s | Batch:  821 | Train loss: 0.4001\n",
      "   % Time:  609s | Batch:  831 | Train loss: 0.3902\n",
      "   % Time:  617s | Batch:  841 | Train loss: 0.3846\n",
      "   % Time:  624s | Batch:  851 | Train loss: 0.3910\n",
      "   % Time:  631s | Batch:  861 | Train loss: 0.4047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   % Time:  638s | Batch:  871 | Train loss: 0.3855\n",
      "   % Time:  646s | Batch:  881 | Train loss: 0.4241\n",
      "   % Time:  653s | Batch:  891 | Train loss: 0.3402\n",
      "   % Time:  660s | Batch:  901 | Train loss: 0.3807\n",
      "   % Time:  667s | Batch:  911 | Train loss: 0.4179\n",
      "   % Time:  675s | Batch:  921 | Train loss: 0.3713\n",
      "   % Time:  682s | Batch:  931 | Train loss: 0.3670\n",
      "   % Time:  689s | Batch:  941 | Train loss: 0.3918\n",
      "   % Time:  697s | Batch:  951 | Train loss: 0.3742\n",
      "   % Time:  704s | Batch:  961 | Train loss: 0.3800\n",
      "   % Time:  711s | Batch:  971 | Train loss: 0.3955\n",
      "   % Time:  718s | Batch:  981 | Train loss: 0.3737\n",
      "   % Time:  725s | Batch:  991 | Train loss: 0.3643\n",
      "   % Time:  732s | Batch: 1001 | Train loss: 0.3373\n",
      "   % Time:  739s | Batch: 1011 | Train loss: 0.3692\n",
      "   % Time:  746s | Batch: 1021 | Train loss: 0.3971\n",
      "   % Time:  753s | Batch: 1031 | Train loss: 0.4073\n",
      "   % Time:  760s | Batch: 1041 | Train loss: 0.3858\n",
      "   % Time:  767s | Batch: 1051 | Train loss: 0.3522\n",
      "   % Time:  774s | Batch: 1061 | Train loss: 0.3430\n",
      "   % Time:  781s | Batch: 1071 | Train loss: 0.3753\n",
      "   % Time:  788s | Batch: 1081 | Train loss: 0.3872\n",
      "   % Time:  795s | Batch: 1091 | Train loss: 0.3840\n",
      "   % Time:  801s | Batch: 1101 | Train loss: 0.3606\n",
      "   % Time:  808s | Batch: 1111 | Train loss: 0.3725\n",
      "   % Time:  815s | Batch: 1121 | Train loss: 0.4011\n",
      "   % Time:  822s | Batch: 1131 | Train loss: 0.4223\n",
      "   % Time:  829s | Batch: 1141 | Train loss: 0.3600\n",
      "   % Time:  836s | Batch: 1151 | Train loss: 0.3879\n",
      "   % Time:  843s | Batch: 1161 | Train loss: 0.4200\n",
      "   % Time:  850s | Batch: 1171 | Train loss: 0.4233\n",
      "   % Time:  857s | Batch: 1181 | Train loss: 0.3783\n",
      "   % Time:  864s | Batch: 1191 | Train loss: 0.3476\n",
      "   % Time:  871s | Batch: 1201 | Train loss: 0.4535\n",
      "   % Time:  878s | Batch: 1211 | Train loss: 0.3986\n",
      "   % Time:  885s | Batch: 1221 | Train loss: 0.4071\n",
      "   % Time:  892s | Batch: 1231 | Train loss: 0.3513\n",
      "   % Time:  899s | Batch: 1241 | Train loss: 0.3750\n",
      "   % Time:  906s | Batch: 1251 | Train loss: 0.3856\n",
      "   % Time:  912s | Batch: 1261 | Train loss: 0.3995\n",
      "   % Time:  919s | Batch: 1271 | Train loss: 0.3328\n",
      "   % Time:  926s | Batch: 1281 | Train loss: 0.3531\n",
      "   % Time:  933s | Batch: 1291 | Train loss: 0.3329\n",
      "   % Time:  940s | Batch: 1301 | Train loss: 0.3923\n",
      "   % Time:  947s | Batch: 1311 | Train loss: 0.3920\n",
      "   % Time:  954s | Batch: 1321 | Train loss: 0.3502\n",
      "   % Time:  961s | Batch: 1331 | Train loss: 0.3967\n",
      "   % Time:  968s | Batch: 1341 | Train loss: 0.3580\n",
      "   % Time:  975s | Batch: 1351 | Train loss: 0.3995\n",
      "   % Time:  982s | Batch: 1361 | Train loss: 0.3491\n",
      "   % Time:  989s | Batch: 1371 | Train loss: 0.4120\n",
      "   % Time:  996s | Batch: 1381 | Train loss: 0.3417\n",
      "   % Time: 1003s | Batch: 1391 | Train loss: 0.3871\n",
      "   % Time: 1010s | Batch: 1401 | Train loss: 0.3810\n",
      "   % Time: 1017s | Batch: 1411 | Train loss: 0.4292\n",
      "   % Time: 1024s | Batch: 1421 | Train loss: 0.3997\n",
      "   % Time: 1031s | Batch: 1431 | Train loss: 0.3861\n",
      "   % Time: 1038s | Batch: 1441 | Train loss: 0.4007\n",
      "   % Time: 1045s | Batch: 1451 | Train loss: 0.3885\n",
      "   % Time: 1052s | Batch: 1461 | Train loss: 0.3794\n",
      "   % Time: 1059s | Batch: 1471 | Train loss: 0.4073\n",
      "   % Time: 1066s | Batch: 1481 | Train loss: 0.4446\n",
      "   % Time: 1073s | Batch: 1491 | Train loss: 0.3744\n",
      "   % Time: 1080s | Batch: 1501 | Train loss: 0.3585\n",
      "   % Time: 1087s | Batch: 1511 | Train loss: 0.3744\n",
      "   % Time: 1094s | Batch: 1521 | Train loss: 0.3968\n",
      "   % Time: 1101s | Batch: 1531 | Train loss: 0.3928\n",
      "   % Time: 1108s | Batch: 1541 | Train loss: 0.4009\n",
      "   % Time: 1114s | Batch: 1551 | Train loss: 0.4022\n",
      "   % Time: 1121s | Batch: 1561 | Train loss: 0.3289\n",
      "   % Time: 1128s | Batch: 1571 | Train loss: 0.4001\n",
      "   % Time: 1135s | Batch: 1581 | Train loss: 0.3962\n",
      "   % Time: 1142s | Batch: 1591 | Train loss: 0.4068\n",
      "   % Time: 1149s | Batch: 1601 | Train loss: 0.3961\n",
      "   % Time: 1156s | Batch: 1611 | Train loss: 0.3661\n",
      "   % Time: 1163s | Batch: 1621 | Train loss: 0.3792\n",
      "   % Time: 1170s | Batch: 1631 | Train loss: 0.3526\n",
      "   % Time: 1177s | Batch: 1641 | Train loss: 0.3598\n",
      "   % Time: 1184s | Batch: 1651 | Train loss: 0.3851\n",
      "   % Time: 1191s | Batch: 1661 | Train loss: 0.4002\n",
      "   % Time: 1198s | Batch: 1671 | Train loss: 0.3712\n",
      "   % Time: 1205s | Batch: 1681 | Train loss: 0.3979\n",
      "   % Time: 1211s | Batch: 1691 | Train loss: 0.3818\n",
      "   % Time: 1218s | Batch: 1701 | Train loss: 0.4608\n",
      "   % Time: 1225s | Batch: 1711 | Train loss: 0.4047\n",
      "   % Time: 1232s | Batch: 1721 | Train loss: 0.3658\n",
      "   % Time: 1239s | Batch: 1731 | Train loss: 0.3669\n",
      "   % Time: 1246s | Batch: 1741 | Train loss: 0.3630\n",
      "   % Time: 1253s | Batch: 1751 | Train loss: 0.3660\n",
      "   % Time: 1260s | Batch: 1761 | Train loss: 0.3434\n",
      "   % Time: 1267s | Batch: 1771 | Train loss: 0.3774\n",
      "   % Time: 1274s | Batch: 1781 | Train loss: 0.4034\n",
      "   % Time: 1281s | Batch: 1791 | Train loss: 0.3825\n",
      "   % Time: 1288s | Batch: 1801 | Train loss: 0.3541\n",
      "   % Time: 1295s | Batch: 1811 | Train loss: 0.4167\n",
      "   % Time: 1302s | Batch: 1821 | Train loss: 0.3630\n",
      "   % Time: 1309s | Batch: 1831 | Train loss: 0.4048\n",
      "   % Time: 1316s | Batch: 1841 | Train loss: 0.3520\n",
      "   % Time: 1323s | Batch: 1851 | Train loss: 0.3622\n",
      "   % Time: 1330s | Batch: 1861 | Train loss: 0.3589\n",
      "   % Time: 1337s | Batch: 1871 | Train loss: 0.3905\n",
      "   % Time: 1344s | Batch: 1881 | Train loss: 0.3960\n",
      "   % Time: 1351s | Batch: 1891 | Train loss: 0.3382\n",
      "   % Time: 1358s | Batch: 1901 | Train loss: 0.3913\n",
      "   % Time: 1364s | Batch: 1911 | Train loss: 0.3642\n",
      "   % Time: 1371s | Batch: 1921 | Train loss: 0.3702\n",
      "   % Time: 1378s | Batch: 1931 | Train loss: 0.4625\n",
      "   % Time: 1385s | Batch: 1941 | Train loss: 0.3783\n",
      "   % Time: 1392s | Batch: 1951 | Train loss: 0.3743\n",
      "   % Time: 1399s | Batch: 1961 | Train loss: 0.3754\n",
      "   % Time: 1406s | Batch: 1971 | Train loss: 0.4250\n",
      "   % Time: 1413s | Batch: 1981 | Train loss: 0.3223\n",
      "   % Time: 1420s | Batch: 1991 | Train loss: 0.3733\n",
      "   % Time: 1427s | Batch: 2001 | Train loss: 0.3988\n",
      "   % Time: 1434s | Batch: 2011 | Train loss: 0.3530\n",
      "   % Time: 1441s | Batch: 2021 | Train loss: 0.3968\n",
      "   % Time: 1448s | Batch: 2031 | Train loss: 0.4243\n",
      "   % Time: 1455s | Batch: 2041 | Train loss: 0.4063\n",
      "   % Time: 1462s | Batch: 2051 | Train loss: 0.3675\n",
      "   % Time: 1469s | Batch: 2061 | Train loss: 0.3571\n",
      "   % Time: 1476s | Batch: 2071 | Train loss: 0.3975\n",
      "   % Time: 1482s | Batch: 2081 | Train loss: 0.3343\n",
      "   % Time: 1489s | Batch: 2091 | Train loss: 0.3787\n",
      "   % Time: 1496s | Batch: 2101 | Train loss: 0.3388\n",
      "   % Time: 1503s | Batch: 2111 | Train loss: 0.3718\n",
      "   % Time: 1510s | Batch: 2121 | Train loss: 0.3679\n",
      "   % Time: 1517s | Batch: 2131 | Train loss: 0.4012\n",
      "   % Time: 1524s | Batch: 2141 | Train loss: 0.3758\n",
      "   % Time: 1531s | Batch: 2151 | Train loss: 0.3662\n",
      "   % Time: 1538s | Batch: 2161 | Train loss: 0.3856\n",
      "   % Time: 1545s | Batch: 2171 | Train loss: 0.3400\n",
      "   % Time: 1552s | Batch: 2181 | Train loss: 0.3872\n",
      "   % Time: 1559s | Batch: 2191 | Train loss: 0.3515\n",
      "   % Time: 1566s | Batch: 2201 | Train loss: 0.3791\n",
      "   % Time: 1573s | Batch: 2211 | Train loss: 0.3466\n",
      "   % Time: 1580s | Batch: 2221 | Train loss: 0.3704\n",
      "   % Time: 1587s | Batch: 2231 | Train loss: 0.3691\n",
      "   % Time: 1594s | Batch: 2241 | Train loss: 0.3788\n",
      "   % Time: 1601s | Batch: 2251 | Train loss: 0.3817\n",
      "   % Time: 1608s | Batch: 2261 | Train loss: 0.3597\n",
      "==========\n",
      "   % Epoch: 2 | Time: 1612s | Train loss: 0.3862 | Val loss: 29.1305\n",
      "==========\n",
      "=> EPOCH 3\n",
      "   % Time:    1s | Batch:    1 | Train loss: 0.4145\n",
      "   % Time:    8s | Batch:   11 | Train loss: 0.4052\n",
      "   % Time:   14s | Batch:   21 | Train loss: 0.3880\n",
      "   % Time:   21s | Batch:   31 | Train loss: 0.3595\n",
      "   % Time:   28s | Batch:   41 | Train loss: 0.4106\n",
      "   % Time:   35s | Batch:   51 | Train loss: 0.3935\n",
      "   % Time:   42s | Batch:   61 | Train loss: 0.3505\n",
      "   % Time:   49s | Batch:   71 | Train loss: 0.3622\n",
      "   % Time:   56s | Batch:   81 | Train loss: 0.3742\n",
      "   % Time:   63s | Batch:   91 | Train loss: 0.3292\n",
      "   % Time:   70s | Batch:  101 | Train loss: 0.3909\n",
      "   % Time:   77s | Batch:  111 | Train loss: 0.3477\n",
      "   % Time:   84s | Batch:  121 | Train loss: 0.4080\n",
      "   % Time:   91s | Batch:  131 | Train loss: 0.3397\n",
      "   % Time:   98s | Batch:  141 | Train loss: 0.3737\n",
      "   % Time:  105s | Batch:  151 | Train loss: 0.3499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   % Time:  112s | Batch:  161 | Train loss: 0.4009\n",
      "   % Time:  119s | Batch:  171 | Train loss: 0.3378\n",
      "   % Time:  126s | Batch:  181 | Train loss: 0.3817\n",
      "   % Time:  133s | Batch:  191 | Train loss: 0.3618\n",
      "   % Time:  139s | Batch:  201 | Train loss: 0.3581\n",
      "   % Time:  146s | Batch:  211 | Train loss: 0.3557\n",
      "   % Time:  153s | Batch:  221 | Train loss: 0.3771\n",
      "   % Time:  160s | Batch:  231 | Train loss: 0.3754\n",
      "   % Time:  167s | Batch:  241 | Train loss: 0.3881\n",
      "   % Time:  174s | Batch:  251 | Train loss: 0.3618\n",
      "   % Time:  181s | Batch:  261 | Train loss: 0.3970\n",
      "   % Time:  188s | Batch:  271 | Train loss: 0.3983\n",
      "   % Time:  195s | Batch:  281 | Train loss: 0.3637\n",
      "   % Time:  202s | Batch:  291 | Train loss: 0.3754\n",
      "   % Time:  209s | Batch:  301 | Train loss: 0.3644\n",
      "   % Time:  216s | Batch:  311 | Train loss: 0.4070\n",
      "   % Time:  222s | Batch:  321 | Train loss: 0.3551\n",
      "   % Time:  229s | Batch:  331 | Train loss: 0.4038\n",
      "   % Time:  236s | Batch:  341 | Train loss: 0.3563\n",
      "   % Time:  243s | Batch:  351 | Train loss: 0.3599\n",
      "   % Time:  250s | Batch:  361 | Train loss: 0.3788\n",
      "   % Time:  257s | Batch:  371 | Train loss: 0.3960\n",
      "   % Time:  264s | Batch:  381 | Train loss: 0.4318\n",
      "   % Time:  271s | Batch:  391 | Train loss: 0.3859\n",
      "   % Time:  278s | Batch:  401 | Train loss: 0.3675\n",
      "   % Time:  285s | Batch:  411 | Train loss: 0.3765\n",
      "   % Time:  292s | Batch:  421 | Train loss: 0.4125\n",
      "   % Time:  299s | Batch:  431 | Train loss: 0.3789\n",
      "   % Time:  306s | Batch:  441 | Train loss: 0.3750\n",
      "   % Time:  313s | Batch:  451 | Train loss: 0.4118\n",
      "   % Time:  320s | Batch:  461 | Train loss: 0.3891\n",
      "   % Time:  326s | Batch:  471 | Train loss: 0.3748\n",
      "   % Time:  333s | Batch:  481 | Train loss: 0.3865\n",
      "   % Time:  340s | Batch:  491 | Train loss: 0.4006\n",
      "   % Time:  347s | Batch:  501 | Train loss: 0.3715\n",
      "   % Time:  354s | Batch:  511 | Train loss: 0.3488\n",
      "   % Time:  361s | Batch:  521 | Train loss: 0.3719\n",
      "   % Time:  368s | Batch:  531 | Train loss: 0.4198\n",
      "   % Time:  375s | Batch:  541 | Train loss: 0.4182\n",
      "   % Time:  382s | Batch:  551 | Train loss: 0.3747\n",
      "   % Time:  389s | Batch:  561 | Train loss: 0.3695\n",
      "   % Time:  396s | Batch:  571 | Train loss: 0.3719\n",
      "   % Time:  403s | Batch:  581 | Train loss: 0.4182\n",
      "   % Time:  410s | Batch:  591 | Train loss: 0.3757\n",
      "   % Time:  417s | Batch:  601 | Train loss: 0.2851\n",
      "   % Time:  424s | Batch:  611 | Train loss: 0.3613\n",
      "   % Time:  431s | Batch:  621 | Train loss: 0.3510\n",
      "   % Time:  438s | Batch:  631 | Train loss: 0.3794\n",
      "   % Time:  445s | Batch:  641 | Train loss: 0.4079\n",
      "   % Time:  452s | Batch:  651 | Train loss: 0.3420\n",
      "   % Time:  459s | Batch:  661 | Train loss: 0.3572\n",
      "   % Time:  466s | Batch:  671 | Train loss: 0.3658\n",
      "   % Time:  473s | Batch:  681 | Train loss: 0.3988\n",
      "   % Time:  480s | Batch:  691 | Train loss: 0.3464\n",
      "   % Time:  487s | Batch:  701 | Train loss: 0.3564\n",
      "   % Time:  494s | Batch:  711 | Train loss: 0.3439\n",
      "   % Time:  501s | Batch:  721 | Train loss: 0.3638\n",
      "   % Time:  507s | Batch:  731 | Train loss: 0.3348\n",
      "   % Time:  515s | Batch:  741 | Train loss: 0.3313\n",
      "   % Time:  521s | Batch:  751 | Train loss: 0.3735\n",
      "   % Time:  528s | Batch:  761 | Train loss: 0.3877\n",
      "   % Time:  535s | Batch:  771 | Train loss: 0.3871\n",
      "   % Time:  542s | Batch:  781 | Train loss: 0.3417\n",
      "   % Time:  549s | Batch:  791 | Train loss: 0.3771\n",
      "   % Time:  556s | Batch:  801 | Train loss: 0.3582\n",
      "   % Time:  563s | Batch:  811 | Train loss: 0.3902\n",
      "   % Time:  570s | Batch:  821 | Train loss: 0.3327\n",
      "   % Time:  577s | Batch:  831 | Train loss: 0.4183\n",
      "   % Time:  584s | Batch:  841 | Train loss: 0.3638\n",
      "   % Time:  591s | Batch:  851 | Train loss: 0.3975\n",
      "   % Time:  598s | Batch:  861 | Train loss: 0.3291\n",
      "   % Time:  605s | Batch:  871 | Train loss: 0.4128\n",
      "   % Time:  612s | Batch:  881 | Train loss: 0.4001\n",
      "   % Time:  619s | Batch:  891 | Train loss: 0.3758\n",
      "   % Time:  626s | Batch:  901 | Train loss: 0.3160\n",
      "   % Time:  632s | Batch:  911 | Train loss: 0.3818\n",
      "   % Time:  639s | Batch:  921 | Train loss: 0.3919\n",
      "   % Time:  646s | Batch:  931 | Train loss: 0.3748\n",
      "   % Time:  653s | Batch:  941 | Train loss: 0.3874\n",
      "   % Time:  660s | Batch:  951 | Train loss: 0.3612\n",
      "   % Time:  667s | Batch:  961 | Train loss: 0.3581\n",
      "   % Time:  674s | Batch:  971 | Train loss: 0.3556\n",
      "   % Time:  681s | Batch:  981 | Train loss: 0.3870\n",
      "   % Time:  688s | Batch:  991 | Train loss: 0.3588\n",
      "   % Time:  695s | Batch: 1001 | Train loss: 0.4061\n",
      "   % Time:  702s | Batch: 1011 | Train loss: 0.4108\n",
      "   % Time:  709s | Batch: 1021 | Train loss: 0.3554\n",
      "   % Time:  716s | Batch: 1031 | Train loss: 0.4270\n",
      "   % Time:  723s | Batch: 1041 | Train loss: 0.3642\n",
      "   % Time:  730s | Batch: 1051 | Train loss: 0.4176\n",
      "   % Time:  737s | Batch: 1061 | Train loss: 0.3489\n",
      "   % Time:  744s | Batch: 1071 | Train loss: 0.4131\n",
      "   % Time:  751s | Batch: 1081 | Train loss: 0.3508\n",
      "   % Time:  758s | Batch: 1091 | Train loss: 0.3866\n",
      "   % Time:  765s | Batch: 1101 | Train loss: 0.3775\n",
      "   % Time:  772s | Batch: 1111 | Train loss: 0.3999\n",
      "   % Time:  779s | Batch: 1121 | Train loss: 0.3861\n",
      "   % Time:  786s | Batch: 1131 | Train loss: 0.4027\n",
      "   % Time:  792s | Batch: 1141 | Train loss: 0.3866\n",
      "   % Time:  799s | Batch: 1151 | Train loss: 0.4057\n",
      "   % Time:  806s | Batch: 1161 | Train loss: 0.4249\n",
      "   % Time:  813s | Batch: 1171 | Train loss: 0.3922\n",
      "   % Time:  820s | Batch: 1181 | Train loss: 0.3618\n",
      "   % Time:  827s | Batch: 1191 | Train loss: 0.3638\n",
      "   % Time:  834s | Batch: 1201 | Train loss: 0.3663\n",
      "   % Time:  841s | Batch: 1211 | Train loss: 0.4159\n",
      "   % Time:  848s | Batch: 1221 | Train loss: 0.3789\n",
      "   % Time:  855s | Batch: 1231 | Train loss: 0.3998\n",
      "   % Time:  862s | Batch: 1241 | Train loss: 0.3938\n",
      "   % Time:  869s | Batch: 1251 | Train loss: 0.3928\n",
      "   % Time:  876s | Batch: 1261 | Train loss: 0.3882\n",
      "   % Time:  883s | Batch: 1271 | Train loss: 0.3773\n",
      "   % Time:  890s | Batch: 1281 | Train loss: 0.4186\n",
      "   % Time:  897s | Batch: 1291 | Train loss: 0.4181\n",
      "   % Time:  904s | Batch: 1301 | Train loss: 0.3597\n",
      "   % Time:  911s | Batch: 1311 | Train loss: 0.3379\n",
      "   % Time:  918s | Batch: 1321 | Train loss: 0.3916\n",
      "   % Time:  925s | Batch: 1331 | Train loss: 0.3794\n",
      "   % Time:  931s | Batch: 1341 | Train loss: 0.3629\n",
      "   % Time:  938s | Batch: 1351 | Train loss: 0.4072\n",
      "   % Time:  945s | Batch: 1361 | Train loss: 0.4108\n",
      "   % Time:  952s | Batch: 1371 | Train loss: 0.4166\n",
      "   % Time:  959s | Batch: 1381 | Train loss: 0.4282\n",
      "   % Time:  966s | Batch: 1391 | Train loss: 0.3891\n",
      "   % Time:  973s | Batch: 1401 | Train loss: 0.3723\n",
      "   % Time:  980s | Batch: 1411 | Train loss: 0.3194\n",
      "   % Time:  987s | Batch: 1421 | Train loss: 0.3942\n",
      "   % Time:  994s | Batch: 1431 | Train loss: 0.4033\n",
      "   % Time: 1001s | Batch: 1441 | Train loss: 0.3542\n",
      "   % Time: 1008s | Batch: 1451 | Train loss: 0.3683\n",
      "   % Time: 1015s | Batch: 1461 | Train loss: 0.3673\n",
      "   % Time: 1022s | Batch: 1471 | Train loss: 0.3841\n",
      "   % Time: 1029s | Batch: 1481 | Train loss: 0.3775\n",
      "   % Time: 1036s | Batch: 1491 | Train loss: 0.3855\n",
      "   % Time: 1043s | Batch: 1501 | Train loss: 0.3778\n",
      "   % Time: 1050s | Batch: 1511 | Train loss: 0.3672\n",
      "   % Time: 1057s | Batch: 1521 | Train loss: 0.4175\n",
      "   % Time: 1064s | Batch: 1531 | Train loss: 0.3918\n",
      "   % Time: 1071s | Batch: 1541 | Train loss: 0.3949\n",
      "   % Time: 1078s | Batch: 1551 | Train loss: 0.3898\n",
      "   % Time: 1085s | Batch: 1561 | Train loss: 0.4102\n",
      "   % Time: 1092s | Batch: 1571 | Train loss: 0.4021\n",
      "   % Time: 1099s | Batch: 1581 | Train loss: 0.4322\n",
      "   % Time: 1105s | Batch: 1591 | Train loss: 0.3997\n",
      "   % Time: 1112s | Batch: 1601 | Train loss: 0.3892\n",
      "   % Time: 1119s | Batch: 1611 | Train loss: 0.4171\n",
      "   % Time: 1126s | Batch: 1621 | Train loss: 0.3712\n",
      "   % Time: 1133s | Batch: 1631 | Train loss: 0.3052\n",
      "   % Time: 1140s | Batch: 1641 | Train loss: 0.3933\n",
      "   % Time: 1147s | Batch: 1651 | Train loss: 0.4011\n",
      "   % Time: 1154s | Batch: 1661 | Train loss: 0.3946\n",
      "   % Time: 1161s | Batch: 1671 | Train loss: 0.3700\n",
      "   % Time: 1168s | Batch: 1681 | Train loss: 0.3791\n",
      "   % Time: 1175s | Batch: 1691 | Train loss: 0.4142\n",
      "   % Time: 1182s | Batch: 1701 | Train loss: 0.4506\n",
      "   % Time: 1189s | Batch: 1711 | Train loss: 0.3554\n",
      "   % Time: 1196s | Batch: 1721 | Train loss: 0.3747\n",
      "   % Time: 1203s | Batch: 1731 | Train loss: 0.4383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   % Time: 1209s | Batch: 1741 | Train loss: 0.4009\n",
      "   % Time: 1216s | Batch: 1751 | Train loss: 0.3886\n",
      "   % Time: 1223s | Batch: 1761 | Train loss: 0.3946\n",
      "   % Time: 1230s | Batch: 1771 | Train loss: 0.3401\n",
      "   % Time: 1237s | Batch: 1781 | Train loss: 0.3732\n",
      "   % Time: 1244s | Batch: 1791 | Train loss: 0.3608\n",
      "   % Time: 1251s | Batch: 1801 | Train loss: 0.4054\n",
      "   % Time: 1258s | Batch: 1811 | Train loss: 0.3544\n",
      "   % Time: 1265s | Batch: 1821 | Train loss: 0.4597\n",
      "   % Time: 1272s | Batch: 1831 | Train loss: 0.4039\n",
      "   % Time: 1279s | Batch: 1841 | Train loss: 0.4299\n",
      "   % Time: 1286s | Batch: 1851 | Train loss: 0.3508\n",
      "   % Time: 1293s | Batch: 1861 | Train loss: 0.4037\n",
      "   % Time: 1300s | Batch: 1871 | Train loss: 0.3345\n",
      "   % Time: 1307s | Batch: 1881 | Train loss: 0.3725\n",
      "   % Time: 1314s | Batch: 1891 | Train loss: 0.3677\n",
      "   % Time: 1321s | Batch: 1901 | Train loss: 0.4089\n",
      "   % Time: 1328s | Batch: 1911 | Train loss: 0.3560\n",
      "   % Time: 1335s | Batch: 1921 | Train loss: 0.3430\n",
      "   % Time: 1342s | Batch: 1931 | Train loss: 0.3919\n",
      "   % Time: 1349s | Batch: 1941 | Train loss: 0.3527\n",
      "   % Time: 1356s | Batch: 1951 | Train loss: 0.3642\n",
      "   % Time: 1363s | Batch: 1961 | Train loss: 0.3825\n",
      "   % Time: 1370s | Batch: 1971 | Train loss: 0.3881\n",
      "   % Time: 1377s | Batch: 1981 | Train loss: 0.3809\n",
      "   % Time: 1384s | Batch: 1991 | Train loss: 0.3980\n",
      "   % Time: 1390s | Batch: 2001 | Train loss: 0.4008\n",
      "   % Time: 1397s | Batch: 2011 | Train loss: 0.3606\n",
      "   % Time: 1404s | Batch: 2021 | Train loss: 0.3609\n",
      "   % Time: 1412s | Batch: 2031 | Train loss: 0.3714\n",
      "   % Time: 1418s | Batch: 2041 | Train loss: 0.4256\n",
      "   % Time: 1425s | Batch: 2051 | Train loss: 0.4002\n",
      "   % Time: 1432s | Batch: 2061 | Train loss: 0.3270\n",
      "   % Time: 1439s | Batch: 2071 | Train loss: 0.3605\n",
      "   % Time: 1446s | Batch: 2081 | Train loss: 0.3772\n",
      "   % Time: 1453s | Batch: 2091 | Train loss: 0.4091\n",
      "   % Time: 1460s | Batch: 2101 | Train loss: 0.3888\n",
      "   % Time: 1467s | Batch: 2111 | Train loss: 0.3610\n",
      "   % Time: 1474s | Batch: 2121 | Train loss: 0.3703\n",
      "   % Time: 1481s | Batch: 2131 | Train loss: 0.3858\n",
      "   % Time: 1488s | Batch: 2141 | Train loss: 0.3880\n",
      "   % Time: 1495s | Batch: 2151 | Train loss: 0.3436\n",
      "   % Time: 1502s | Batch: 2161 | Train loss: 0.4011\n",
      "   % Time: 1509s | Batch: 2171 | Train loss: 0.3973\n",
      "   % Time: 1516s | Batch: 2181 | Train loss: 0.3724\n",
      "   % Time: 1523s | Batch: 2191 | Train loss: 0.3746\n",
      "   % Time: 1530s | Batch: 2201 | Train loss: 0.3577\n",
      "   % Time: 1537s | Batch: 2211 | Train loss: 0.4037\n",
      "   % Time: 1544s | Batch: 2221 | Train loss: 0.3766\n",
      "   % Time: 1551s | Batch: 2231 | Train loss: 0.3731\n",
      "   % Time: 1558s | Batch: 2241 | Train loss: 0.3720\n",
      "   % Time: 1565s | Batch: 2251 | Train loss: 0.3691\n",
      "   % Time: 1572s | Batch: 2261 | Train loss: 0.4208\n",
      "==========\n",
      "   % Epoch: 3 | Time: 1576s | Train loss: 0.3792 | Val loss: 28.7299\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "if args.train:\n",
    "    for epoch in range(1, args.n_epoch+1):\n",
    "        print(\"=> EPOCH {}\".format(epoch))\n",
    "        scheduler.step()\n",
    "        val_loss = train(scaled_data, scaler, features,\n",
    "                         model, criterion, optimizer)\n",
    "#        scheduler.step(val_loss)\n",
    "#        save_model(model, epoch, val_loss)\n",
    "else:\n",
    "    model_file = os.path.join(args.intermediate_path, args.model_name)\n",
    "    model.load_state_dict(torch.load(model_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-03T09:33:28.329676Z",
     "start_time": "2017-09-03T09:20:50.207507Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction = forecast(scaled_data, scaler, features, model)\n",
    "prediction_file = os.path.join(args.intermediate_path,\n",
    "                               'prediction_seed{}.pkl'.format(args.seed))\n",
    "\n",
    "with open(prediction_file, 'wb') as f:\n",
    "    pickle.dump(prediction, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
