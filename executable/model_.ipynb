{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-03T13:13:59.226904Z",
     "start_time": "2017-09-03T13:13:58.786685Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import numpy as np; np.seterr(invalid='ignore')\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-03T13:13:59.275873Z",
     "start_time": "2017-09-03T13:13:59.228128Z"
    }
   },
   "outputs": [],
   "source": [
    "parser = {\n",
    "    'data_path': '../data/wttsf/',\n",
    "    'train_file': 'train_1.csv',\n",
    "    'intermediate_path': '../intermediate/',\n",
    "    'n_epoch': 3,\n",
    "    'future': 73,\n",
    "    'batch_size': 64,\n",
    "    'hidden_size': 256,\n",
    "    'log_every': 10,\n",
    "    'read_from_file': False,\n",
    "    'train': True,\n",
    "    'model_name': '',\n",
    "    'cuda': True,\n",
    "    'seed': 20170903,\n",
    "}\n",
    "args = argparse.Namespace(**parser)\n",
    "\n",
    "args.cuda = args.cuda and torch.cuda.is_available()\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "args.intermediate_path = os.path.join(args.intermediate_path, str(args.seed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-03T13:13:59.362614Z",
     "start_time": "2017-09-03T13:13:59.276827Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DenseLSTMForecast(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(DenseLSTMForecast, self).__init__()\n",
    "        self.lstm1 = nn.LSTMCell(15, hidden_size)\n",
    "        self.lstm2 = nn.LSTMCell(hidden_size+15, hidden_size)\n",
    "        self.lstm3 = nn.LSTMCell(2*hidden_size+15, hidden_size)\n",
    "        self.linear = nn.Linear(3*hidden_size+15, 1)\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    def forward(self, x, feature, future=1):\n",
    "        o = []\n",
    "        tt = torch.cuda if args.cuda else torch\n",
    "        h1_t = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())\n",
    "        c1_t = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())\n",
    "        h2_t = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())\n",
    "        c2_t = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())\n",
    "        h3_t = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())\n",
    "        c3_t = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())\n",
    "        \n",
    "        for x_t in x.chunk(x.size(1), dim=1):\n",
    "            x_t = x_t.squeeze(dim=1)\n",
    "            xd_t = torch.cat([x_t, feature], dim=1)\n",
    "            h1_t, c1_t = self.lstm1(xd_t, (h1_t, c1_t))\n",
    "            h1d_t = torch.cat([xd_t, h1_t], dim=1)\n",
    "            h2_t, c2_t = self.lstm2(h1d_t, (h2_t, c2_t))\n",
    "            h2d_t = torch.cat([xd_t, h1_t, h2_t], dim=1)\n",
    "            h3_t, c3_t = self.lstm3(h2d_t, (h3_t, c3_t))\n",
    "            h3d_t = torch.cat([xd_t, h1_t, h2_t, h3_t], dim=1)\n",
    "            o_t = self.linear(h3d_t)\n",
    "            o.append(o_t)\n",
    "            \n",
    "        for i in range(future-1):\n",
    "            od_t = torch.cat([o_t, feature], dim=1)\n",
    "            h1_t, c1_t = self.lstm1(od_t, (h1_t, c1_t))\n",
    "            h1d_t = torch.cat([od_t, h1_t], dim=1)\n",
    "            h2_t, c2_t = self.lstm2(h1d_t, (h2_t, c2_t))\n",
    "            h2d_t = torch.cat([od_t, h1_t, h2_t], dim=1)\n",
    "            h3_t, c3_t = self.lstm3(h2d_t, (h3_t, c3_t))\n",
    "            h3d_t = torch.cat([od_t, h1_t, h2_t, h3_t], dim=1)\n",
    "            o_t = self.linear(h3d_t)\n",
    "            o.append(o_t)\n",
    "\n",
    "        return torch.stack(o, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-03T13:13:59.426758Z",
     "start_time": "2017-09-03T13:13:59.363674Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def smape(y_pred, y_true):\n",
    "    y_pred = np.around(np.clip(np.exp(y_pred)-1, 0, None))\n",
    "    y_true = np.around(np.exp(y_true) - 1)\n",
    "    raw_smape = np.abs(y_true - y_pred) / (np.abs(y_true) + np.abs(y_pred))\n",
    "    kaggle_smape = np.nan_to_num(raw_smape)\n",
    "    return np.mean(kaggle_smape) * 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-03T13:13:59.490437Z",
     "start_time": "2017-09-03T13:13:59.428554Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    scaled_data_file = os.path.join(args.intermediate_path,\n",
    "                                    'scaled_data.pkl')\n",
    "    scaler_file = os.path.join(args.intermediate_path, 'scaler.pkl')\n",
    "    features_file = os.path.join(args.intermediate_path, 'features.pkl')\n",
    "    \n",
    "    if not args.read_from_file:\n",
    "        data_df = pd.read_csv(os.path.join(args.data_path, args.train_file),\n",
    "                              index_col='Page')\n",
    "        data_df[\"agent\"] = data_df.index.str.rsplit('_').str.get(-1)\n",
    "        data_df[\"access\"] = data_df.index.str.rsplit('_').str.get(-2)\n",
    "        data_df[\"project\"] = data_df.index.str.rsplit('_').str.get(-3)\n",
    "        features = pd.get_dummies(data_df[[\"agent\", \"access\", \"project\"]],\n",
    "            columns=[\"agent\", \"access\", \"project\"]).values.astype('float32')\n",
    "        raw_data = np.nan_to_num(\n",
    "            data_df.iloc[:,:-3].values.astype('float32'))\n",
    "        data = np.log1p(raw_data)\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(np.swapaxes(data[:, :-args.future], 0, 1))\n",
    "        scaled_data = scaler.transform(np.swapaxes(data, 0, 1))\n",
    "        scaled_data = np.swapaxes(scaled_data, 0, 1)\n",
    "        \n",
    "        with open(scaled_data_file, 'wb') as f:\n",
    "            pickle.dump(scaled_data, f)\n",
    "        with open(scaler_file, 'wb') as f:\n",
    "            pickle.dump(scaler, f)\n",
    "        with open(features_file, 'wb') as f:\n",
    "            pickle.dump(features, f)\n",
    "    else:\n",
    "        with open(scaled_data_file, 'rb') as f:\n",
    "            scaled_data = pickle.load(f)\n",
    "        with open(scaler_file, 'rb') as f:\n",
    "            scaler = pickle.load(f)\n",
    "        with open(features_file, 'rb') as f:\n",
    "            features = pickle.load(f)\n",
    "    return scaled_data, scaler, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-03T13:13:59.572592Z",
     "start_time": "2017-09-03T13:13:59.491803Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(scaled_data, scaler, features, model, criterion, optimizer):\n",
    "    p = np.random.permutation(scaled_data.shape[0])\n",
    "    inverse_p = np.argsort(p)\n",
    "\n",
    "    input_tensor = torch.from_numpy(scaled_data[p, :-1]).unsqueeze(2)\n",
    "    target_tensor = torch.from_numpy(scaled_data[p, 1:]).unsqueeze(2)\n",
    "    features_tensor = torch.from_numpy(features[p, :])\n",
    "    dataset = TensorDataset(input_tensor, target_tensor)\n",
    "    data_loader = DataLoader(dataset, args.batch_size)\n",
    "    \n",
    "    train_loss = 0\n",
    "    val_output_list = []\n",
    "    init_time = time.time()\n",
    "    for i, (inputt, target) in enumerate(data_loader):\n",
    "        feature = features_tensor[i*args.batch_size:(i*args.batch_size\n",
    "                                                     +inputt.size(0))]\n",
    "        if args.cuda:\n",
    "            inputt = inputt.cuda()\n",
    "            target = target.cuda()\n",
    "            feature = feature.cuda()\n",
    "        inputt = Variable(inputt)\n",
    "        target = Variable(target)\n",
    "        feature = Variable(feature)\n",
    "        \n",
    "        output = model(inputt, feature)\n",
    "        pos = np.random.randint(args.future,\n",
    "                                output.size(1)-2*args.future+1)\n",
    "        loss = criterion(output[:, pos:pos+args.future],\n",
    "                         target[:, pos:pos+args.future])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.data[0] * inputt.size(0)\n",
    "        val_output_list.append(output[:, -args.future:]\n",
    "                               .data.squeeze(2).cpu().numpy())\n",
    "\n",
    "        if i % args.log_every == 0:\n",
    "            print(\"   % Time: {:4.0f}s | Batch: {:4} | \"\n",
    "                  \"Train loss: {:.4f}\".format(\n",
    "                      time.time()-init_time, i+1, loss.data[0]))\n",
    "        \n",
    "    val_output_all = np.concatenate(val_output_list, axis=0)[inverse_p]\n",
    "    prediction = np.swapaxes(scaler.inverse_transform(\n",
    "            np.swapaxes(val_output_all, 0, 1)), 0, 1)\n",
    "    \n",
    "    var_target = np.swapaxes(scaler.inverse_transform(\n",
    "            np.swapaxes(scaled_data[:, -args.future:], 0, 1)), 0, 1)\n",
    "    train_loss /= scaled_data.shape[0]\n",
    "    val_loss = smape(prediction, var_target)\n",
    "    \n",
    "    print(\"=\"*10)\n",
    "    print(\"   % Epoch: {} | Time: {:4.0f}s | \"\n",
    "          \"Train loss: {:.4f} | Val loss: {:.4f}\"\n",
    "          .format(epoch, time.time()-init_time, train_loss , val_loss))\n",
    "    print(\"=\"*10)\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-03T13:13:59.629574Z",
     "start_time": "2017-09-03T13:13:59.573715Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forecast(scaled_data, scaler, features, model):\n",
    "    input_tensor = torch.from_numpy(scaled_data).unsqueeze(2)\n",
    "    target_tensor = torch.zeros(input_tensor.size(0))\n",
    "    features_tensor = torch.from_numpy(features)\n",
    "    dataset = torch.utils.data.TensorDataset(input_tensor, target_tensor)\n",
    "    data_loader = DataLoader(dataset, args.batch_size)\n",
    "    \n",
    "    output_list = []\n",
    "    for i, (inputt, _) in enumerate(data_loader):\n",
    "        feature = features_tensor[i*args.batch_size:(i*args.batch_size\n",
    "                                                     +inputt.size(0))]\n",
    "        if args.cuda:\n",
    "            inputt = inputt.cuda()\n",
    "            feature = feature.cuda()\n",
    "        inputt = Variable(inputt)\n",
    "        feature = Variable(feature)\n",
    "        output = model(inputt, feature, args.future)\n",
    "        output_list.append(output.data.squeeze(2).cpu().numpy()\n",
    "                           [:, -args.future:])\n",
    "        \n",
    "    output_all = np.concatenate(output_list, axis=0)\n",
    "    prediction = np.swapaxes(scaler.inverse_transform(\n",
    "            np.swapaxes(output_all, 0, 1)), 0, 1)\n",
    "    prediction = np.around(np.clip(np.exp(prediction) - 1, 0, None))\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-03T13:13:59.689844Z",
     "start_time": "2017-09-03T13:13:59.632035Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_model(model, epoch, loss):\n",
    "    model_file = os.path.join(args.intermediate_path,\n",
    "                              \"model_seed{}_epoch{}_loss_{:.4f}.pth\"\n",
    "                              .format(args.seed, epoch, loss))\n",
    "    torch.save(model.state_dict(), os.path.join(model_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-03T13:14:11.519413Z",
     "start_time": "2017-09-03T13:13:59.694783Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scaled_data, scaler, features = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-03T13:14:12.710944Z",
     "start_time": "2017-09-03T13:14:11.520796Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = DenseLSTMForecast(args.hidden_size)\n",
    "if args.cuda:\n",
    "    model.cuda()\n",
    "criterion = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-09-03T13:13:58.956Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "scheduler = MultiStepLR(optimizer, milestones=[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-09-03T13:13:58.959Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> EPOCH 1\n",
      "   % Time:    1s | Batch:    1 | Train loss: 0.6661\n",
      "   % Time:    9s | Batch:   11 | Train loss: 0.4750\n",
      "   % Time:   17s | Batch:   21 | Train loss: 0.4477\n",
      "   % Time:   25s | Batch:   31 | Train loss: 0.4532\n",
      "   % Time:   32s | Batch:   41 | Train loss: 0.4209\n",
      "   % Time:   40s | Batch:   51 | Train loss: 0.4335\n",
      "   % Time:   48s | Batch:   61 | Train loss: 0.4341\n",
      "   % Time:   56s | Batch:   71 | Train loss: 0.4560\n",
      "   % Time:   64s | Batch:   81 | Train loss: 0.4376\n",
      "   % Time:   71s | Batch:   91 | Train loss: 0.4215\n",
      "   % Time:   79s | Batch:  101 | Train loss: 0.4257\n",
      "   % Time:   87s | Batch:  111 | Train loss: 0.4364\n",
      "   % Time:   95s | Batch:  121 | Train loss: 0.4359\n",
      "   % Time:  103s | Batch:  131 | Train loss: 0.3663\n",
      "   % Time:  110s | Batch:  141 | Train loss: 0.4097\n",
      "   % Time:  118s | Batch:  151 | Train loss: 0.3885\n",
      "   % Time:  126s | Batch:  161 | Train loss: 0.4422\n",
      "   % Time:  134s | Batch:  171 | Train loss: 0.4931\n",
      "   % Time:  142s | Batch:  181 | Train loss: 0.4117\n",
      "   % Time:  149s | Batch:  191 | Train loss: 0.4325\n",
      "   % Time:  157s | Batch:  201 | Train loss: 0.3650\n",
      "   % Time:  165s | Batch:  211 | Train loss: 0.3885\n",
      "   % Time:  173s | Batch:  221 | Train loss: 0.4592\n",
      "   % Time:  180s | Batch:  231 | Train loss: 0.3793\n",
      "   % Time:  188s | Batch:  241 | Train loss: 0.4277\n",
      "   % Time:  196s | Batch:  251 | Train loss: 0.4049\n",
      "   % Time:  204s | Batch:  261 | Train loss: 0.4368\n",
      "   % Time:  211s | Batch:  271 | Train loss: 0.4023\n",
      "   % Time:  219s | Batch:  281 | Train loss: 0.4720\n",
      "   % Time:  227s | Batch:  291 | Train loss: 0.4173\n",
      "   % Time:  235s | Batch:  301 | Train loss: 0.3555\n",
      "   % Time:  242s | Batch:  311 | Train loss: 0.4500\n",
      "   % Time:  250s | Batch:  321 | Train loss: 0.4022\n",
      "   % Time:  258s | Batch:  331 | Train loss: 0.4190\n",
      "   % Time:  266s | Batch:  341 | Train loss: 0.4635\n",
      "   % Time:  273s | Batch:  351 | Train loss: 0.3696\n",
      "   % Time:  281s | Batch:  361 | Train loss: 0.4725\n",
      "   % Time:  289s | Batch:  371 | Train loss: 0.3845\n",
      "   % Time:  297s | Batch:  381 | Train loss: 0.4147\n",
      "   % Time:  304s | Batch:  391 | Train loss: 0.4049\n",
      "   % Time:  312s | Batch:  401 | Train loss: 0.4449\n",
      "   % Time:  320s | Batch:  411 | Train loss: 0.3923\n",
      "   % Time:  328s | Batch:  421 | Train loss: 0.3914\n",
      "   % Time:  335s | Batch:  431 | Train loss: 0.4067\n",
      "   % Time:  343s | Batch:  441 | Train loss: 0.4022\n",
      "   % Time:  351s | Batch:  451 | Train loss: 0.3812\n",
      "   % Time:  359s | Batch:  461 | Train loss: 0.3883\n",
      "   % Time:  366s | Batch:  471 | Train loss: 0.3748\n",
      "   % Time:  374s | Batch:  481 | Train loss: 0.3754\n",
      "   % Time:  382s | Batch:  491 | Train loss: 0.4487\n",
      "   % Time:  390s | Batch:  501 | Train loss: 0.3660\n",
      "   % Time:  397s | Batch:  511 | Train loss: 0.4287\n",
      "   % Time:  405s | Batch:  521 | Train loss: 0.3916\n",
      "   % Time:  413s | Batch:  531 | Train loss: 0.3356\n",
      "   % Time:  421s | Batch:  541 | Train loss: 0.3858\n",
      "   % Time:  428s | Batch:  551 | Train loss: 0.4157\n",
      "   % Time:  436s | Batch:  561 | Train loss: 0.4610\n",
      "   % Time:  444s | Batch:  571 | Train loss: 0.3928\n",
      "   % Time:  452s | Batch:  581 | Train loss: 0.3463\n",
      "   % Time:  459s | Batch:  591 | Train loss: 0.4094\n",
      "   % Time:  467s | Batch:  601 | Train loss: 0.4217\n",
      "   % Time:  475s | Batch:  611 | Train loss: 0.4450\n",
      "   % Time:  483s | Batch:  621 | Train loss: 0.4133\n",
      "   % Time:  490s | Batch:  631 | Train loss: 0.4225\n",
      "   % Time:  498s | Batch:  641 | Train loss: 0.4206\n",
      "   % Time:  506s | Batch:  651 | Train loss: 0.4225\n",
      "   % Time:  514s | Batch:  661 | Train loss: 0.3893\n",
      "   % Time:  521s | Batch:  671 | Train loss: 0.3914\n",
      "   % Time:  529s | Batch:  681 | Train loss: 0.3889\n",
      "   % Time:  537s | Batch:  691 | Train loss: 0.3994\n",
      "   % Time:  545s | Batch:  701 | Train loss: 0.4026\n",
      "   % Time:  553s | Batch:  711 | Train loss: 0.3814\n",
      "   % Time:  560s | Batch:  721 | Train loss: 0.3404\n",
      "   % Time:  568s | Batch:  731 | Train loss: 0.3835\n",
      "   % Time:  576s | Batch:  741 | Train loss: 0.3695\n",
      "   % Time:  584s | Batch:  751 | Train loss: 0.3954\n",
      "   % Time:  591s | Batch:  761 | Train loss: 0.4208\n",
      "   % Time:  599s | Batch:  771 | Train loss: 0.4020\n",
      "   % Time:  607s | Batch:  781 | Train loss: 0.3561\n",
      "   % Time:  615s | Batch:  791 | Train loss: 0.4110\n",
      "   % Time:  622s | Batch:  801 | Train loss: 0.3528\n",
      "   % Time:  630s | Batch:  811 | Train loss: 0.3614\n",
      "   % Time:  638s | Batch:  821 | Train loss: 0.4211\n",
      "   % Time:  646s | Batch:  831 | Train loss: 0.3813\n",
      "   % Time:  654s | Batch:  841 | Train loss: 0.4126\n",
      "   % Time:  661s | Batch:  851 | Train loss: 0.4224\n",
      "   % Time:  669s | Batch:  861 | Train loss: 0.4005\n",
      "   % Time:  677s | Batch:  871 | Train loss: 0.4095\n",
      "   % Time:  685s | Batch:  881 | Train loss: 0.3821\n",
      "   % Time:  692s | Batch:  891 | Train loss: 0.3826\n",
      "   % Time:  700s | Batch:  901 | Train loss: 0.3896\n",
      "   % Time:  708s | Batch:  911 | Train loss: 0.3823\n",
      "   % Time:  716s | Batch:  921 | Train loss: 0.4021\n",
      "   % Time:  723s | Batch:  931 | Train loss: 0.4024\n",
      "   % Time:  731s | Batch:  941 | Train loss: 0.3745\n",
      "   % Time:  739s | Batch:  951 | Train loss: 0.4272\n",
      "   % Time:  747s | Batch:  961 | Train loss: 0.3323\n",
      "   % Time:  755s | Batch:  971 | Train loss: 0.4004\n",
      "   % Time:  763s | Batch:  981 | Train loss: 0.3925\n",
      "   % Time:  770s | Batch:  991 | Train loss: 0.4213\n",
      "   % Time:  778s | Batch: 1001 | Train loss: 0.3721\n",
      "   % Time:  786s | Batch: 1011 | Train loss: 0.3813\n",
      "   % Time:  794s | Batch: 1021 | Train loss: 0.3460\n",
      "   % Time:  801s | Batch: 1031 | Train loss: 0.3486\n",
      "   % Time:  809s | Batch: 1041 | Train loss: 0.3766\n",
      "   % Time:  817s | Batch: 1051 | Train loss: 0.3595\n",
      "   % Time:  825s | Batch: 1061 | Train loss: 0.3966\n",
      "   % Time:  833s | Batch: 1071 | Train loss: 0.3729\n",
      "   % Time:  841s | Batch: 1081 | Train loss: 0.4029\n",
      "   % Time:  848s | Batch: 1091 | Train loss: 0.4101\n",
      "   % Time:  856s | Batch: 1101 | Train loss: 0.3756\n",
      "   % Time:  864s | Batch: 1111 | Train loss: 0.4077\n",
      "   % Time:  872s | Batch: 1121 | Train loss: 0.3818\n",
      "   % Time:  880s | Batch: 1131 | Train loss: 0.4035\n",
      "   % Time:  887s | Batch: 1141 | Train loss: 0.3405\n",
      "   % Time:  895s | Batch: 1151 | Train loss: 0.4489\n",
      "   % Time:  903s | Batch: 1161 | Train loss: 0.3813\n",
      "   % Time:  911s | Batch: 1171 | Train loss: 0.3925\n",
      "   % Time:  919s | Batch: 1181 | Train loss: 0.3791\n",
      "   % Time:  926s | Batch: 1191 | Train loss: 0.4259\n",
      "   % Time:  934s | Batch: 1201 | Train loss: 0.3864\n",
      "   % Time:  942s | Batch: 1211 | Train loss: 0.3798\n",
      "   % Time:  950s | Batch: 1221 | Train loss: 0.3854\n",
      "   % Time:  958s | Batch: 1231 | Train loss: 0.3901\n",
      "   % Time:  966s | Batch: 1241 | Train loss: 0.3802\n",
      "   % Time:  973s | Batch: 1251 | Train loss: 0.4084\n",
      "   % Time:  981s | Batch: 1261 | Train loss: 0.3977\n",
      "   % Time:  989s | Batch: 1271 | Train loss: 0.3817\n",
      "   % Time:  997s | Batch: 1281 | Train loss: 0.3997\n",
      "   % Time: 1005s | Batch: 1291 | Train loss: 0.4249\n",
      "   % Time: 1013s | Batch: 1301 | Train loss: 0.4491\n",
      "   % Time: 1020s | Batch: 1311 | Train loss: 0.4214\n",
      "   % Time: 1028s | Batch: 1321 | Train loss: 0.4319\n",
      "   % Time: 1036s | Batch: 1331 | Train loss: 0.4152\n",
      "   % Time: 1044s | Batch: 1341 | Train loss: 0.4054\n",
      "   % Time: 1052s | Batch: 1351 | Train loss: 0.3707\n",
      "   % Time: 1060s | Batch: 1361 | Train loss: 0.3710\n",
      "   % Time: 1067s | Batch: 1371 | Train loss: 0.3694\n",
      "   % Time: 1075s | Batch: 1381 | Train loss: 0.4364\n",
      "   % Time: 1083s | Batch: 1391 | Train loss: 0.4122\n",
      "   % Time: 1091s | Batch: 1401 | Train loss: 0.4010\n",
      "   % Time: 1098s | Batch: 1411 | Train loss: 0.4172\n",
      "   % Time: 1106s | Batch: 1421 | Train loss: 0.3903\n",
      "   % Time: 1114s | Batch: 1431 | Train loss: 0.3842\n",
      "   % Time: 1122s | Batch: 1441 | Train loss: 0.4270\n",
      "   % Time: 1129s | Batch: 1451 | Train loss: 0.3831\n",
      "   % Time: 1137s | Batch: 1461 | Train loss: 0.3511\n",
      "   % Time: 1145s | Batch: 1471 | Train loss: 0.3842\n",
      "   % Time: 1153s | Batch: 1481 | Train loss: 0.3755\n",
      "   % Time: 1161s | Batch: 1491 | Train loss: 0.4105\n",
      "   % Time: 1168s | Batch: 1501 | Train loss: 0.3470\n",
      "   % Time: 1176s | Batch: 1511 | Train loss: 0.3855\n",
      "   % Time: 1184s | Batch: 1521 | Train loss: 0.4116\n",
      "   % Time: 1192s | Batch: 1531 | Train loss: 0.4067\n",
      "   % Time: 1199s | Batch: 1541 | Train loss: 0.3878\n",
      "   % Time: 1207s | Batch: 1551 | Train loss: 0.4144\n",
      "   % Time: 1215s | Batch: 1561 | Train loss: 0.4323\n",
      "   % Time: 1223s | Batch: 1571 | Train loss: 0.3671\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   % Time: 1231s | Batch: 1581 | Train loss: 0.3892\n",
      "   % Time: 1238s | Batch: 1591 | Train loss: 0.3885\n",
      "   % Time: 1246s | Batch: 1601 | Train loss: 0.4246\n",
      "   % Time: 1254s | Batch: 1611 | Train loss: 0.3516\n",
      "   % Time: 1262s | Batch: 1621 | Train loss: 0.3586\n",
      "   % Time: 1270s | Batch: 1631 | Train loss: 0.3643\n",
      "   % Time: 1278s | Batch: 1641 | Train loss: 0.3496\n",
      "   % Time: 1286s | Batch: 1651 | Train loss: 0.3979\n",
      "   % Time: 1293s | Batch: 1661 | Train loss: 0.3914\n",
      "   % Time: 1301s | Batch: 1671 | Train loss: 0.3768\n",
      "   % Time: 1309s | Batch: 1681 | Train loss: 0.3821\n",
      "   % Time: 1317s | Batch: 1691 | Train loss: 0.4067\n",
      "   % Time: 1325s | Batch: 1701 | Train loss: 0.4200\n",
      "   % Time: 1333s | Batch: 1711 | Train loss: 0.3575\n",
      "   % Time: 1341s | Batch: 1721 | Train loss: 0.3861\n",
      "   % Time: 1349s | Batch: 1731 | Train loss: 0.3698\n",
      "   % Time: 1356s | Batch: 1741 | Train loss: 0.4216\n",
      "   % Time: 1364s | Batch: 1751 | Train loss: 0.4430\n",
      "   % Time: 1372s | Batch: 1761 | Train loss: 0.3778\n",
      "   % Time: 1380s | Batch: 1771 | Train loss: 0.3743\n",
      "   % Time: 1388s | Batch: 1781 | Train loss: 0.3990\n",
      "   % Time: 1396s | Batch: 1791 | Train loss: 0.3898\n",
      "   % Time: 1404s | Batch: 1801 | Train loss: 0.4155\n",
      "   % Time: 1411s | Batch: 1811 | Train loss: 0.4573\n",
      "   % Time: 1419s | Batch: 1821 | Train loss: 0.3282\n",
      "   % Time: 1427s | Batch: 1831 | Train loss: 0.3493\n",
      "   % Time: 1435s | Batch: 1841 | Train loss: 0.4039\n",
      "   % Time: 1443s | Batch: 1851 | Train loss: 0.3898\n",
      "   % Time: 1450s | Batch: 1861 | Train loss: 0.3555\n",
      "   % Time: 1458s | Batch: 1871 | Train loss: 0.3520\n",
      "   % Time: 1466s | Batch: 1881 | Train loss: 0.3824\n",
      "   % Time: 1474s | Batch: 1891 | Train loss: 0.4056\n",
      "   % Time: 1482s | Batch: 1901 | Train loss: 0.3652\n",
      "   % Time: 1490s | Batch: 1911 | Train loss: 0.3773\n",
      "   % Time: 1497s | Batch: 1921 | Train loss: 0.3459\n",
      "   % Time: 1505s | Batch: 1931 | Train loss: 0.3949\n",
      "   % Time: 1513s | Batch: 1941 | Train loss: 0.4223\n",
      "   % Time: 1521s | Batch: 1951 | Train loss: 0.3476\n",
      "   % Time: 1528s | Batch: 1961 | Train loss: 0.3345\n",
      "   % Time: 1536s | Batch: 1971 | Train loss: 0.3526\n",
      "   % Time: 1544s | Batch: 1981 | Train loss: 0.3853\n",
      "   % Time: 1552s | Batch: 1991 | Train loss: 0.3313\n",
      "   % Time: 1560s | Batch: 2001 | Train loss: 0.3702\n",
      "   % Time: 1568s | Batch: 2011 | Train loss: 0.3793\n",
      "   % Time: 1576s | Batch: 2021 | Train loss: 0.3826\n",
      "   % Time: 1583s | Batch: 2031 | Train loss: 0.3740\n",
      "   % Time: 1591s | Batch: 2041 | Train loss: 0.3855\n",
      "   % Time: 1599s | Batch: 2051 | Train loss: 0.3756\n",
      "   % Time: 1607s | Batch: 2061 | Train loss: 0.3877\n",
      "   % Time: 1615s | Batch: 2071 | Train loss: 0.4051\n",
      "   % Time: 1623s | Batch: 2081 | Train loss: 0.4014\n",
      "   % Time: 1630s | Batch: 2091 | Train loss: 0.3787\n",
      "   % Time: 1638s | Batch: 2101 | Train loss: 0.4144\n",
      "   % Time: 1646s | Batch: 2111 | Train loss: 0.3666\n",
      "   % Time: 1654s | Batch: 2121 | Train loss: 0.3775\n",
      "   % Time: 1661s | Batch: 2131 | Train loss: 0.3768\n",
      "   % Time: 1669s | Batch: 2141 | Train loss: 0.3506\n",
      "   % Time: 1677s | Batch: 2151 | Train loss: 0.3483\n",
      "   % Time: 1685s | Batch: 2161 | Train loss: 0.3674\n",
      "   % Time: 1693s | Batch: 2171 | Train loss: 0.3774\n",
      "   % Time: 1700s | Batch: 2181 | Train loss: 0.4287\n",
      "   % Time: 1708s | Batch: 2191 | Train loss: 0.3646\n",
      "   % Time: 1716s | Batch: 2201 | Train loss: 0.3969\n",
      "   % Time: 1724s | Batch: 2211 | Train loss: 0.3737\n",
      "   % Time: 1732s | Batch: 2221 | Train loss: 0.3860\n",
      "   % Time: 1739s | Batch: 2231 | Train loss: 0.4117\n",
      "   % Time: 1747s | Batch: 2241 | Train loss: 0.4236\n",
      "   % Time: 1755s | Batch: 2251 | Train loss: 0.3362\n",
      "   % Time: 1763s | Batch: 2261 | Train loss: 0.3661\n",
      "==========\n",
      "   % Epoch: 1 | Time: 1768s | Train loss: 0.3981 | Val loss: 30.0702\n",
      "==========\n",
      "=> EPOCH 2\n",
      "   % Time:    1s | Batch:    1 | Train loss: 0.4322\n",
      "   % Time:    8s | Batch:   11 | Train loss: 0.3509\n",
      "   % Time:   16s | Batch:   21 | Train loss: 0.4021\n",
      "   % Time:   24s | Batch:   31 | Train loss: 0.4174\n",
      "   % Time:   32s | Batch:   41 | Train loss: 0.3861\n",
      "   % Time:   39s | Batch:   51 | Train loss: 0.3872\n",
      "   % Time:   47s | Batch:   61 | Train loss: 0.3831\n",
      "   % Time:   55s | Batch:   71 | Train loss: 0.3844\n",
      "   % Time:   63s | Batch:   81 | Train loss: 0.3767\n",
      "   % Time:   71s | Batch:   91 | Train loss: 0.4062\n",
      "   % Time:   79s | Batch:  101 | Train loss: 0.3394\n",
      "   % Time:   86s | Batch:  111 | Train loss: 0.4170\n",
      "   % Time:   94s | Batch:  121 | Train loss: 0.3935\n",
      "   % Time:  102s | Batch:  131 | Train loss: 0.4161\n",
      "   % Time:  110s | Batch:  141 | Train loss: 0.3893\n",
      "   % Time:  117s | Batch:  151 | Train loss: 0.3544\n",
      "   % Time:  125s | Batch:  161 | Train loss: 0.4168\n",
      "   % Time:  133s | Batch:  171 | Train loss: 0.4440\n",
      "   % Time:  141s | Batch:  181 | Train loss: 0.3929\n",
      "   % Time:  148s | Batch:  191 | Train loss: 0.3948\n",
      "   % Time:  156s | Batch:  201 | Train loss: 0.3746\n",
      "   % Time:  164s | Batch:  211 | Train loss: 0.3810\n",
      "   % Time:  172s | Batch:  221 | Train loss: 0.3760\n",
      "   % Time:  179s | Batch:  231 | Train loss: 0.3618\n",
      "   % Time:  187s | Batch:  241 | Train loss: 0.3773\n",
      "   % Time:  195s | Batch:  251 | Train loss: 0.3806\n",
      "   % Time:  203s | Batch:  261 | Train loss: 0.4001\n",
      "   % Time:  211s | Batch:  271 | Train loss: 0.3485\n",
      "   % Time:  218s | Batch:  281 | Train loss: 0.3850\n",
      "   % Time:  226s | Batch:  291 | Train loss: 0.3731\n",
      "   % Time:  234s | Batch:  301 | Train loss: 0.3908\n",
      "   % Time:  242s | Batch:  311 | Train loss: 0.3770\n",
      "   % Time:  249s | Batch:  321 | Train loss: 0.3637\n",
      "   % Time:  257s | Batch:  331 | Train loss: 0.3963\n"
     ]
    }
   ],
   "source": [
    "if args.train:\n",
    "    for epoch in range(1, args.n_epoch+1):\n",
    "        print(\"=> EPOCH {}\".format(epoch))\n",
    "        scheduler.step()\n",
    "        val_loss = train(scaled_data, scaler, features,\n",
    "                         model, criterion, optimizer)\n",
    "else:\n",
    "    model_file = os.path.join(args.intermediate_path, args.model_name)\n",
    "    model.load_state_dict(torch.load(model_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-09-03T13:13:58.971Z"
    }
   },
   "outputs": [],
   "source": [
    "prediction = forecast(scaled_data, scaler, features, model)\n",
    "prediction_file = os.path.join(args.intermediate_path,\n",
    "                               'prediction_seed{}.pkl'.format(args.seed))\n",
    "\n",
    "with open(prediction_file, 'wb') as f:\n",
    "    pickle.dump(prediction, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
